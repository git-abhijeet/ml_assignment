{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f803647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for creating diagrams and mathematical examples\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
    "import seaborn as sns\n",
    "from math import log2\n",
    "\n",
    "# Set up matplotlib for better visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“š Decision Tree Theory - Part 1\")\n",
    "print(\"================================\")\n",
    "print(\"Libraries imported successfully for theoretical explanations and examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81255655",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of a Decision Tree. What kind of problems is it best suited for?\n",
    "\n",
    "## Answer:\n",
    "\n",
    "### **Concept of Decision Tree:**\n",
    "\n",
    "A **Decision Tree** is a tree-like model used for both classification and regression tasks that makes decisions by splitting the data based on feature values. It mimics human decision-making by asking a series of questions about the features and following different paths based on the answers until reaching a final decision.\n",
    "\n",
    "### **How it Works:**\n",
    "1. **Starting Point**: Begin with the entire dataset at the root\n",
    "2. **Splitting**: Choose the best feature and threshold to split the data\n",
    "3. **Branching**: Create branches for different values/ranges of the chosen feature\n",
    "4. **Recursion**: Repeat the process for each subset until stopping criteria are met\n",
    "5. **Prediction**: Follow the path from root to leaf based on new data's feature values\n",
    "\n",
    "### **Tree Structure:**\n",
    "- **Root Node**: Top node containing all data\n",
    "- **Internal Nodes**: Decision points that split data based on features\n",
    "- **Branches**: Connections representing possible feature values\n",
    "- **Leaf Nodes**: Terminal nodes containing final predictions/classifications\n",
    "\n",
    "### **Problems Best Suited For:**\n",
    "\n",
    "#### **1. Classification Problems:**\n",
    "- **Binary Classification**: Email spam detection, medical diagnosis\n",
    "- **Multi-class Classification**: Image recognition, customer segmentation\n",
    "- **Examples**: \n",
    "  - Determining loan approval (Approved/Rejected)\n",
    "  - Medical diagnosis (Disease A/B/C/Healthy)\n",
    "  - Customer churn prediction (Will churn/Won't churn)\n",
    "\n",
    "#### **2. Regression Problems:**\n",
    "- **Continuous Target Prediction**: House price prediction, stock prices\n",
    "- **Examples**:\n",
    "  - Predicting sales revenue based on marketing spend\n",
    "  - Estimating delivery time based on distance and traffic\n",
    "\n",
    "#### **3. Specific Problem Characteristics Where Decision Trees Excel:**\n",
    "\n",
    "**âœ… Categorical Features:**\n",
    "- Naturally handles categorical variables without encoding\n",
    "- Works well with mixed data types (categorical + numerical)\n",
    "\n",
    "**âœ… Non-linear Relationships:**\n",
    "- Captures complex, non-linear patterns in data\n",
    "- No assumptions about feature distributions\n",
    "\n",
    "**âœ… Feature Interactions:**\n",
    "- Automatically captures interactions between features\n",
    "- Can model complex decision boundaries\n",
    "\n",
    "**âœ… Interpretability Requirements:**\n",
    "- Provides clear, explainable decision rules\n",
    "- Easy to understand for non-technical stakeholders\n",
    "\n",
    "**âœ… Missing Data Handling:**\n",
    "- Can handle missing values through surrogate splits\n",
    "- Robust to incomplete data\n",
    "\n",
    "#### **4. Real-World Applications:**\n",
    "\n",
    "**Healthcare:**\n",
    "- Symptom-based diagnosis systems\n",
    "- Treatment recommendation engines\n",
    "- Risk assessment for medical procedures\n",
    "\n",
    "**Finance:**\n",
    "- Credit scoring and loan approval\n",
    "- Fraud detection systems\n",
    "- Investment decision support\n",
    "\n",
    "**Marketing:**\n",
    "- Customer segmentation\n",
    "- Targeted advertising campaigns\n",
    "- Recommendation systems\n",
    "\n",
    "**Business Operations:**\n",
    "- Supply chain optimization\n",
    "- Quality control processes\n",
    "- Human resource decisions\n",
    "\n",
    "#### **5. Advantages for Specific Problem Types:**\n",
    "\n",
    "**When Feature Interpretability is Crucial:**\n",
    "- Regulatory compliance (banking, healthcare)\n",
    "- Scientific research where understanding relationships matters\n",
    "- Business decisions requiring justification\n",
    "\n",
    "**When Data has Complex Patterns:**\n",
    "- Non-linear relationships between features and target\n",
    "- Multiple feature interactions\n",
    "- Hierarchical decision-making processes\n",
    "\n",
    "**When Minimal Data Preprocessing is Desired:**\n",
    "- Mixed data types (numerical and categorical)\n",
    "- Missing values present\n",
    "- No need for feature scaling or normalization\n",
    "\n",
    "### **Limitations to Consider:**\n",
    "\n",
    "âŒ **Overfitting**: Can create overly complex trees that don't generalize well\n",
    "âŒ **Instability**: Small changes in data can result in very different trees  \n",
    "âŒ **Bias**: Favors features with more levels when using certain splitting criteria\n",
    "âŒ **Linear Relationships**: May not efficiently capture simple linear patterns\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Decision Trees are particularly well-suited for problems requiring **interpretable models** with **mixed data types**, **complex non-linear relationships**, and scenarios where **understanding the decision process** is as important as prediction accuracy. They excel in domains like healthcare, finance, and business where stakeholders need to understand and trust the model's reasoning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dce4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of a Decision Tree concept\n",
    "print(\"ðŸŒ³ Decision Tree Concept Visualization\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a visual example of a decision tree for loan approval\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Left subplot: Tree structure diagram\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Draw the tree structure\n",
    "# Root node\n",
    "root = FancyBboxPatch((4, 8), 2, 1, boxstyle=\"round,pad=0.1\", \n",
    "                     facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "ax1.add_patch(root)\n",
    "ax1.text(5, 8.5, 'Income > 50K?', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Left branch (No)\n",
    "left_internal = FancyBboxPatch((1, 5.5), 2, 1, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor='lightgreen', edgecolor='black', linewidth=1)\n",
    "ax1.add_patch(left_internal)\n",
    "ax1.text(2, 6, 'Age > 25?', ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Right branch (Yes)\n",
    "right_internal = FancyBboxPatch((7, 5.5), 2, 1, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='lightgreen', edgecolor='black', linewidth=1)\n",
    "ax1.add_patch(right_internal)\n",
    "ax1.text(8, 6, 'Credit Score\\n> 700?', ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Leaf nodes\n",
    "leaf1 = FancyBboxPatch((0, 3), 1.5, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightcoral', edgecolor='black', linewidth=1)\n",
    "ax1.add_patch(leaf1)\n",
    "ax1.text(0.75, 3.4, 'Reject', ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "leaf2 = FancyBboxPatch((2.5, 3), 1.5, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightcoral', edgecolor='black', linewidth=1)\n",
    "ax1.add_patch(leaf2)\n",
    "ax1.text(3.25, 3.4, 'Approve', ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "leaf3 = FancyBboxPatch((6, 3), 1.5, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightcoral', edgecolor='black', linewidth=1)\n",
    "ax1.add_patch(leaf3)\n",
    "ax1.text(6.75, 3.4, 'Review', ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "leaf4 = FancyBboxPatch((8.5, 3), 1.5, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightcoral', edgecolor='black', linewidth=1)\n",
    "ax1.add_patch(leaf4)\n",
    "ax1.text(9.25, 3.4, 'Approve', ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Draw connections\n",
    "# Root to internal nodes\n",
    "ax1.plot([4.5, 2.5], [8, 6.5], 'k-', linewidth=2)\n",
    "ax1.plot([5.5, 7.5], [8, 6.5], 'k-', linewidth=2)\n",
    "ax1.text(3.2, 7.3, 'No', fontweight='bold', color='red')\n",
    "ax1.text(6.8, 7.3, 'Yes', fontweight='bold', color='green')\n",
    "\n",
    "# Internal nodes to leaves\n",
    "ax1.plot([1.5, 0.75], [5.5, 3.8], 'k-', linewidth=1)\n",
    "ax1.plot([2.5, 3.25], [5.5, 3.8], 'k-', linewidth=1)\n",
    "ax1.plot([7.5, 6.75], [5.5, 3.8], 'k-', linewidth=1)\n",
    "ax1.plot([8.5, 9.25], [5.5, 3.8], 'k-', linewidth=1)\n",
    "\n",
    "ax1.text(1, 4.5, 'No', fontweight='bold', color='red', fontsize=8)\n",
    "ax1.text(3, 4.5, 'Yes', fontweight='bold', color='green', fontsize=8)\n",
    "ax1.text(7, 4.5, 'No', fontweight='bold', color='red', fontsize=8)\n",
    "ax1.text(9, 4.5, 'Yes', fontweight='bold', color='green', fontsize=8)\n",
    "\n",
    "ax1.set_title('Decision Tree Structure\\n(Loan Approval Example)', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor='lightblue', edgecolor='black', label='Root Node'),\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor='lightgreen', edgecolor='black', label='Internal Node'),\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor='lightcoral', edgecolor='black', label='Leaf Node')\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 0.2))\n",
    "\n",
    "# Right subplot: Problem types suited for Decision Trees\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "\n",
    "# Create categories\n",
    "categories = [\n",
    "    ('Classification\\nProblems', 8.5, ['Email Spam Detection', 'Medical Diagnosis', 'Customer Segmentation']),\n",
    "    ('Regression\\nProblems', 6.5, ['House Price Prediction', 'Sales Forecasting', 'Risk Assessment']),\n",
    "    ('Business\\nApplications', 4.5, ['Loan Approval', 'Marketing Campaigns', 'Quality Control']),\n",
    "    ('Data\\nCharacteristics', 2.5, ['Mixed Data Types', 'Non-linear Patterns', 'Feature Interactions'])\n",
    "]\n",
    "\n",
    "colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightpink']\n",
    "\n",
    "for i, (category, y_pos, examples) in enumerate(categories):\n",
    "    # Category box\n",
    "    cat_box = FancyBboxPatch((1, y_pos-0.4), 2.5, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                            facecolor=colors[i], edgecolor='black', linewidth=2)\n",
    "    ax2.add_patch(cat_box)\n",
    "    ax2.text(2.25, y_pos, category, ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Examples\n",
    "    for j, example in enumerate(examples):\n",
    "        ex_box = FancyBboxPatch((4.5, y_pos-0.2+j*0.4-0.4), 4.5, 0.3, boxstyle=\"round,pad=0.05\",\n",
    "                               facecolor='white', edgecolor='gray', linewidth=1)\n",
    "        ax2.add_patch(ex_box)\n",
    "        ax2.text(6.75, y_pos-j*0.4+0.2-0.4, example, ha='center', va='center', fontsize=9)\n",
    "        \n",
    "        # Arrow\n",
    "        ax2.annotate('', xy=(4.4, y_pos-j*0.4+0.2-0.4), xytext=(3.6, y_pos),\n",
    "                    arrowprops=dict(arrowstyle='->', color='black', lw=1))\n",
    "\n",
    "ax2.set_title('Problems Best Suited for Decision Trees', fontsize=12, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example decision path\n",
    "print(\"\\nðŸ“ Example Decision Path:\")\n",
    "print(\"=\"*40)\n",
    "print(\"For a loan applicant with:\")\n",
    "print(\"â€¢ Income: $60,000 (> $50K) âœ“\")\n",
    "print(\"â€¢ Credit Score: 750 (> 700) âœ“\")\n",
    "print(\"â€¢ Decision Path: Root â†’ Income > 50K? (Yes) â†’ Credit Score > 700? (Yes) â†’ APPROVE\")\n",
    "print(\"\\nThis demonstrates how Decision Trees make transparent, rule-based decisions!\")\n",
    "\n",
    "print(\"\\nâœ… Decision Tree concept visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a587385",
   "metadata": {},
   "source": [
    "# Q2. Define the following terms with examples: Root Node, Leaf Node, Internal Node, Branch\n",
    "\n",
    "## Answer:\n",
    "\n",
    "Understanding the anatomy of a Decision Tree is crucial for comprehending how the algorithm works. Each component plays a specific role in the decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9fe4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed visualization of Decision Tree components\n",
    "print(\"ðŸŒ³ DECISION TREE COMPONENTS - DEFINITIONS AND EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a comprehensive diagram showing all components\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Decision Tree Components - Definitions and Examples', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Example 1: Weather Decision Tree\n",
    "ax1 = axes[0, 0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Root Node\n",
    "root = FancyBboxPatch((3.5, 8), 3, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                     facecolor='gold', edgecolor='red', linewidth=3)\n",
    "ax1.add_patch(root)\n",
    "ax1.text(5, 8.6, 'Weather = ?', ha='center', va='center', fontweight='bold', fontsize=12)\n",
    "ax1.text(5, 9.5, 'ROOT NODE', ha='center', va='center', fontweight='bold', \n",
    "         fontsize=10, color='red', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='red'))\n",
    "\n",
    "# Internal Nodes\n",
    "internal1 = FancyBboxPatch((1, 5.5), 2.5, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
    "ax1.add_patch(internal1)\n",
    "ax1.text(2.25, 6, 'Temperature\\n> 20Â°C?', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "internal2 = FancyBboxPatch((6.5, 5.5), 2.5, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
    "ax1.add_patch(internal2)\n",
    "ax1.text(7.75, 6, 'Wind Speed\\n> 15 mph?', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax1.text(1, 7.2, 'INTERNAL NODES', ha='left', va='center', fontweight='bold', \n",
    "         fontsize=10, color='blue', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='blue'))\n",
    "\n",
    "# Leaf Nodes\n",
    "leaf1 = FancyBboxPatch((0, 2.5), 1.8, 1, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightgreen', edgecolor='green', linewidth=2)\n",
    "ax1.add_patch(leaf1)\n",
    "ax1.text(0.9, 3, 'Stay\\nIndoors', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "leaf2 = FancyBboxPatch((2.3, 2.5), 1.8, 1, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightgreen', edgecolor='green', linewidth=2)\n",
    "ax1.add_patch(leaf2)\n",
    "ax1.text(3.2, 3, 'Go for\\nWalk', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "leaf3 = FancyBboxPatch((5.7, 2.5), 1.8, 1, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightgreen', edgecolor='green', linewidth=2)\n",
    "ax1.add_patch(leaf3)\n",
    "ax1.text(6.6, 3, 'Indoor\\nActivity', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "leaf4 = FancyBboxPatch((8.2, 2.5), 1.8, 1, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='lightgreen', edgecolor='green', linewidth=2)\n",
    "ax1.add_patch(leaf4)\n",
    "ax1.text(9.1, 3, 'Outdoor\\nSports', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax1.text(8.5, 4.2, 'LEAF NODES', ha='right', va='center', fontweight='bold', \n",
    "         fontsize=10, color='green', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='green'))\n",
    "\n",
    "# Branches\n",
    "ax1.plot([4.2, 2.8], [8, 6.5], 'k-', linewidth=3, color='purple')\n",
    "ax1.plot([5.8, 7.2], [8, 6.5], 'k-', linewidth=3, color='purple')\n",
    "ax1.plot([1.7, 0.9], [5.5, 3.5], 'k-', linewidth=2, color='purple')\n",
    "ax1.plot([2.8, 3.2], [5.5, 3.5], 'k-', linewidth=2, color='purple')\n",
    "ax1.plot([7.2, 6.6], [5.5, 3.5], 'k-', linewidth=2, color='purple')\n",
    "ax1.plot([8.3, 9.1], [5.5, 3.5], 'k-', linewidth=2, color='purple')\n",
    "\n",
    "# Branch labels\n",
    "ax1.text(3.2, 7.3, 'Sunny', fontweight='bold', color='orange', fontsize=10,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='white', edgecolor='purple'))\n",
    "ax1.text(6.8, 7.3, 'Rainy', fontweight='bold', color='orange', fontsize=10,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='white', edgecolor='purple'))\n",
    "\n",
    "ax1.text(0.8, 4.5, 'No', fontweight='bold', color='orange', fontsize=9)\n",
    "ax1.text(3.5, 4.5, 'Yes', fontweight='bold', color='orange', fontsize=9)\n",
    "ax1.text(6.2, 4.5, 'No', fontweight='bold', color='orange', fontsize=9)\n",
    "ax1.text(9.5, 4.5, 'Yes', fontweight='bold', color='orange', fontsize=9)\n",
    "\n",
    "ax1.text(5, 1, 'BRANCHES', ha='center', va='center', fontweight='bold', \n",
    "         fontsize=12, color='purple', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', edgecolor='purple'))\n",
    "\n",
    "ax1.set_title('Weather Activity Decision Tree', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Example 2: Component Definitions Table\n",
    "ax2 = axes[0, 1]\n",
    "ax2.axis('off')\n",
    "\n",
    "definitions = [\n",
    "    (\"ROOT NODE\", \"gold\", \"red\", [\n",
    "        \"â€¢ The topmost node of the tree\",\n",
    "        \"â€¢ Contains the entire dataset initially\", \n",
    "        \"â€¢ First decision point in the tree\",\n",
    "        \"â€¢ Has no parent node\",\n",
    "        \"â€¢ Example: 'Weather = ?' in our tree\"\n",
    "    ]),\n",
    "    (\"INTERNAL NODE\", \"lightblue\", \"blue\", [\n",
    "        \"â€¢ Intermediate decision points\",\n",
    "        \"â€¢ Test a specific feature/attribute\",\n",
    "        \"â€¢ Have both parent and child nodes\",\n",
    "        \"â€¢ Split data into subsets\",\n",
    "        \"â€¢ Example: 'Temperature > 20Â°C?'\"\n",
    "    ]),\n",
    "    (\"LEAF NODE\", \"lightgreen\", \"green\", [\n",
    "        \"â€¢ Terminal nodes (end points)\",\n",
    "        \"â€¢ Contain final predictions/decisions\",\n",
    "        \"â€¢ Have parent but no child nodes\",\n",
    "        \"â€¢ No further splitting occurs\",\n",
    "        \"â€¢ Example: 'Go for Walk', 'Stay Indoors'\"\n",
    "    ]),\n",
    "    (\"BRANCH\", \"white\", \"purple\", [\n",
    "        \"â€¢ Connections between nodes\",\n",
    "        \"â€¢ Represent possible outcomes\",\n",
    "        \"â€¢ Show the path of decisions\",\n",
    "        \"â€¢ Labeled with conditions/values\",\n",
    "        \"â€¢ Example: 'Sunny', 'Rainy', 'Yes', 'No'\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "y_start = 9.5\n",
    "for i, (title, bg_color, border_color, points) in enumerate(definitions):\n",
    "    y_pos = y_start - i * 2.3\n",
    "    \n",
    "    # Title box\n",
    "    title_box = FancyBboxPatch((0.5, y_pos-0.3), 8, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                              facecolor=bg_color, edgecolor=border_color, linewidth=2)\n",
    "    ax2.add_patch(title_box)\n",
    "    ax2.text(4.5, y_pos, title, ha='center', va='center', fontweight='bold', \n",
    "             fontsize=12, color=border_color)\n",
    "    \n",
    "    # Definition points\n",
    "    for j, point in enumerate(points):\n",
    "        ax2.text(0.7, y_pos - 0.8 - j*0.3, point, ha='left', va='center', fontsize=10)\n",
    "\n",
    "ax2.set_xlim(0, 9)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.set_title('Component Definitions', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Example 3: Real-world example - Medical Diagnosis\n",
    "ax3 = axes[1, 0]\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 10)\n",
    "ax3.set_aspect('equal')\n",
    "\n",
    "# Medical diagnosis tree\n",
    "ax3.text(5, 9.5, 'Medical Diagnosis Example', ha='center', va='center', \n",
    "         fontweight='bold', fontsize=12)\n",
    "\n",
    "# Root\n",
    "med_root = FancyBboxPatch((3.5, 8), 3, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='gold', edgecolor='red', linewidth=2)\n",
    "ax3.add_patch(med_root)\n",
    "ax3.text(5, 8.4, 'Fever > 38Â°C?', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Internal nodes\n",
    "med_int1 = FancyBboxPatch((1.5, 6), 2, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
    "ax3.add_patch(med_int1)\n",
    "ax3.text(2.5, 6.4, 'Cough?', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "med_int2 = FancyBboxPatch((6.5, 6), 2, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
    "ax3.add_patch(med_int2)\n",
    "ax3.text(7.5, 6.4, 'Headache?', ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Leaf nodes\n",
    "diagnoses = [\n",
    "    (0.5, 4, 'Common\\nCold'),\n",
    "    (2.5, 4, 'Flu'),\n",
    "    (6, 4, 'Monitor\\nSymptoms'),\n",
    "    (8.5, 4, 'Possible\\nMigraine')\n",
    "]\n",
    "\n",
    "for x, y, diagnosis in diagnoses:\n",
    "    leaf = FancyBboxPatch((x-0.6, y-0.4), 1.2, 0.8, boxstyle=\"round,pad=0.1\",\n",
    "                         facecolor='lightgreen', edgecolor='green', linewidth=2)\n",
    "    ax3.add_patch(leaf)\n",
    "    ax3.text(x, y, diagnosis, ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Connections\n",
    "ax3.plot([4.2, 3], [8, 6.8], 'k-', linewidth=2)\n",
    "ax3.plot([5.8, 7], [8, 6.8], 'k-', linewidth=2)\n",
    "ax3.plot([2, 1.1], [6, 4.8], 'k-', linewidth=2)\n",
    "ax3.plot([3, 3.1], [6, 4.8], 'k-', linewidth=2)\n",
    "ax3.plot([7, 6.6], [6, 4.8], 'k-', linewidth=2)\n",
    "ax3.plot([8, 9.1], [6, 4.8], 'k-', linewidth=2)\n",
    "\n",
    "# Labels\n",
    "ax3.text(3.4, 7.5, 'Yes', fontweight='bold', color='red', fontsize=9)\n",
    "ax3.text(6.6, 7.5, 'No', fontweight='bold', color='blue', fontsize=9)\n",
    "ax3.text(1.3, 5.3, 'Yes', fontweight='bold', color='red', fontsize=8)\n",
    "ax3.text(3.3, 5.3, 'No', fontweight='bold', color='blue', fontsize=8)\n",
    "ax3.text(6.5, 5.3, 'No', fontweight='bold', color='blue', fontsize=8)\n",
    "ax3.text(8.8, 5.3, 'Yes', fontweight='bold', color='red', fontsize=8)\n",
    "\n",
    "ax3.axis('off')\n",
    "\n",
    "# Example 4: Tree Traversal Example\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "ax4.text(0.5, 9.5, 'Tree Traversal Example', ha='center', va='center', \n",
    "         fontweight='bold', fontsize=12)\n",
    "\n",
    "traversal_text = \"\"\"\n",
    "EXAMPLE PATIENT:\n",
    "â€¢ Fever: 39Â°C (> 38Â°C) â†’ Yes\n",
    "â€¢ Cough: Present â†’ Yes\n",
    "â€¢ Decision Path: Root â†’ Fever? (Yes) â†’ Cough? (Yes) â†’ DIAGNOSIS: Flu\n",
    "\n",
    "TREE COMPONENT IDENTIFICATION:\n",
    "\n",
    "ROOT NODE:\n",
    "â””â”€â”€ \"Fever > 38Â°C?\" \n",
    "    â”œâ”€â”€ Contains all patient data initially\n",
    "    â””â”€â”€ First decision point\n",
    "\n",
    "INTERNAL NODES:\n",
    "â”œâ”€â”€ \"Cough?\" (left branch from root)\n",
    "â””â”€â”€ \"Headache?\" (right branch from root)\n",
    "    â”œâ”€â”€ Test specific symptoms\n",
    "    â””â”€â”€ Split patients into subgroups\n",
    "\n",
    "LEAF NODES:\n",
    "â”œâ”€â”€ \"Common Cold\" (final diagnosis)\n",
    "â”œâ”€â”€ \"Flu\" (final diagnosis)  \n",
    "â”œâ”€â”€ \"Monitor Symptoms\" (final recommendation)\n",
    "â””â”€â”€ \"Possible Migraine\" (final diagnosis)\n",
    "    â””â”€â”€ No further splitting, contain final decisions\n",
    "\n",
    "BRANCHES:\n",
    "â”œâ”€â”€ \"Yes\"/\"No\" connections between nodes\n",
    "â”œâ”€â”€ Show possible paths through the tree\n",
    "â””â”€â”€ Labeled with decision outcomes\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0, 8.5, traversal_text, ha='left', va='top', fontsize=9, \n",
    "         fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightyellow'))\n",
    "\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nðŸ“Š COMPONENT SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "components_data = {\n",
    "    'Component': ['Root Node', 'Internal Node', 'Leaf Node', 'Branch'],\n",
    "    'Definition': [\n",
    "        'Topmost node containing entire dataset',\n",
    "        'Intermediate decision points testing features', \n",
    "        'Terminal nodes with final predictions',\n",
    "        'Connections showing decision paths'\n",
    "    ],\n",
    "    'Characteristics': [\n",
    "        'No parent, has children, first split',\n",
    "        'Has parent and children, feature tests',\n",
    "        'Has parent, no children, final output',\n",
    "        'Labeled connections between nodes'\n",
    "    ],\n",
    "    'Example': [\n",
    "        '\"Weather = ?\" in weather tree',\n",
    "        '\"Temperature > 20Â°C?\" splitting on temperature',\n",
    "        '\"Go for Walk\" as final decision',\n",
    "        '\"Sunny\", \"Rainy\", \"Yes\", \"No\" labels'\n",
    "    ]\n",
    "}\n",
    "\n",
    "components_df = pd.DataFrame(components_data)\n",
    "print(components_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ… Decision Tree components visualization and definitions completed!\")\n",
    "print(f\"Each component plays a crucial role in the tree's decision-making process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eeae45",
   "metadata": {},
   "source": [
    "# Q3. Explain the concept of Entropy in Decision Trees with mathematical examples\n",
    "\n",
    "**Entropy** is a fundamental concept in decision trees that measures the **impurity** or **randomness** in a dataset. It helps determine the best feature to split on at each node by quantifying how mixed the target classes are.\n",
    "\n",
    "## ðŸŽ¯ **Key Concepts:**\n",
    "\n",
    "### **What is Entropy?**\n",
    "- **Entropy** measures the disorder or uncertainty in a set of data\n",
    "- **Lower entropy** = more pure/homogeneous data (better)\n",
    "- **Higher entropy** = more mixed/heterogeneous data (worse for classification)\n",
    "- **Range**: 0 (perfect purity) to logâ‚‚(n) where n = number of classes\n",
    "\n",
    "### **Mathematical Formula:**\n",
    "```\n",
    "Entropy(S) = -âˆ‘(i=1 to c) p_i Ã— logâ‚‚(p_i)\n",
    "\n",
    "Where:\n",
    "- S = dataset or subset\n",
    "- c = number of classes\n",
    "- p_i = proportion of samples belonging to class i\n",
    "- logâ‚‚ = logarithm base 2\n",
    "```\n",
    "\n",
    "### **Interpretation:**\n",
    "- **Entropy = 0**: All samples belong to the same class (perfect purity)\n",
    "- **Entropy = 1**: For binary classification, equal distribution of classes (maximum impurity)\n",
    "- **Entropy = logâ‚‚(c)**: Maximum entropy for c classes with equal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy Calculation Examples and Visualizations\n",
    "print(\"ðŸ§® ENTROPY IN DECISION TREES - MATHEMATICAL EXAMPLES\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_entropy(class_counts):\n",
    "    \"\"\"Calculate entropy given class counts\"\"\"\n",
    "    total = sum(class_counts)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    \n",
    "    entropy = 0\n",
    "    for count in class_counts:\n",
    "        if count > 0:\n",
    "            probability = count / total\n",
    "            entropy -= probability * math.log2(probability)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def calculate_proportions(class_counts):\n",
    "    \"\"\"Calculate class proportions\"\"\"\n",
    "    total = sum(class_counts)\n",
    "    return [count/total for count in class_counts]\n",
    "\n",
    "# Example datasets for entropy calculation\n",
    "examples = [\n",
    "    {\n",
    "        'name': 'Perfect Purity (All Same Class)',\n",
    "        'data': 'Play Tennis Dataset',\n",
    "        'positive': 9, 'negative': 0,\n",
    "        'description': 'All samples want to play tennis'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Maximum Impurity (Equal Distribution)', \n",
    "        'data': 'Play Tennis Dataset',\n",
    "        'positive': 5, 'negative': 5,\n",
    "        'description': 'Equal number of positive and negative samples'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Moderate Impurity (Skewed Distribution)',\n",
    "        'data': 'Play Tennis Dataset', \n",
    "        'positive': 7, 'negative': 2,\n",
    "        'description': 'More positive samples than negative'\n",
    "    },\n",
    "    {\n",
    "        'name': 'High Impurity (Slightly Skewed)',\n",
    "        'data': 'Play Tennis Dataset',\n",
    "        'positive': 6, 'negative': 4, \n",
    "        'description': 'Slightly more positive samples'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Calculate and display entropy for each example\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Entropy Calculation Examples', fontsize=16, fontweight='bold')\n",
    "\n",
    "calculation_results = []\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    pos = example['positive']\n",
    "    neg = example['negative']\n",
    "    total = pos + neg\n",
    "    \n",
    "    # Calculate proportions\n",
    "    p_pos = pos / total if total > 0 else 0\n",
    "    p_neg = neg / total if total > 0 else 0\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = calculate_entropy([pos, neg])\n",
    "    \n",
    "    # Store results\n",
    "    calculation_results.append({\n",
    "        'example': example['name'],\n",
    "        'positive': pos,\n",
    "        'negative': neg,\n",
    "        'total': total,\n",
    "        'p_positive': p_pos,\n",
    "        'p_negative': p_neg,\n",
    "        'entropy': entropy\n",
    "    })\n",
    "    \n",
    "    # Visualization\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # Pie chart showing class distribution\n",
    "    if total > 0:\n",
    "        sizes = [pos, neg]\n",
    "        colors = ['lightgreen', 'lightcoral']\n",
    "        labels = [f'Positive ({pos})', f'Negative ({neg})']\n",
    "        \n",
    "        wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "                                         startangle=90, textprops={'fontsize': 10})\n",
    "    \n",
    "    # Add entropy calculation details\n",
    "    ax.text(0, -1.5, f\"Calculation:\", fontweight='bold', ha='center', fontsize=11)\n",
    "    \n",
    "    if total > 0:\n",
    "        if pos > 0 and neg > 0:\n",
    "            calc_text = f\"Entropy = -({p_pos:.3f} Ã— logâ‚‚({p_pos:.3f}) + {p_neg:.3f} Ã— logâ‚‚({p_neg:.3f}))\\n\"\n",
    "            calc_text += f\"        = -({p_pos:.3f} Ã— {math.log2(p_pos):.3f} + {p_neg:.3f} Ã— {math.log2(p_neg):.3f})\\n\"\n",
    "            calc_text += f\"        = {entropy:.4f}\"\n",
    "        elif pos == 0:\n",
    "            calc_text = f\"Entropy = -(0 Ã— logâ‚‚(0) + 1 Ã— logâ‚‚(1))\\n        = -(0 + 0) = 0\"\n",
    "        else:  # neg == 0\n",
    "            calc_text = f\"Entropy = -(1 Ã— logâ‚‚(1) + 0 Ã— logâ‚‚(0))\\n        = -(0 + 0) = 0\"\n",
    "    else:\n",
    "        calc_text = \"No data\"\n",
    "    \n",
    "    ax.text(0, -2.2, calc_text, ha='center', fontsize=9, fontfamily='monospace',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightyellow'))\n",
    "    \n",
    "    # Title and entropy result\n",
    "    ax.set_title(f\"{example['name']}\\nEntropy = {entropy:.4f}\", \n",
    "                fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed calculation table\n",
    "print(\"\\nðŸ“Š DETAILED ENTROPY CALCULATIONS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "results_df = pd.DataFrame(calculation_results)\n",
    "results_df['entropy_rounded'] = results_df['entropy'].round(4)\n",
    "\n",
    "print(results_df[['example', 'positive', 'negative', 'total', 'p_positive', 'p_negative', 'entropy_rounded']].to_string(index=False))\n",
    "\n",
    "# Real-world example: Email Classification\n",
    "print(f\"\\nðŸŒŸ REAL-WORLD EXAMPLE: Email Classification\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "email_scenarios = [\n",
    "    {'spam': 0, 'ham': 10, 'scenario': 'Trusted sender folder'},\n",
    "    {'spam': 5, 'ham': 5, 'scenario': 'Mixed inbox'},  \n",
    "    {'spam': 8, 'ham': 2, 'scenario': 'Suspicious folder'},\n",
    "    {'spam': 1, 'ham': 9, 'scenario': 'Clean inbox'}\n",
    "]\n",
    "\n",
    "print(\"Email Classification Entropy Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for scenario in email_scenarios:\n",
    "    spam_count = scenario['spam']\n",
    "    ham_count = scenario['ham']\n",
    "    total = spam_count + ham_count\n",
    "    entropy = calculate_entropy([spam_count, ham_count])\n",
    "    \n",
    "    print(f\"\\nðŸ“§ Scenario: {scenario['scenario']}\")\n",
    "    print(f\"   Spam emails: {spam_count}, Ham emails: {ham_count}\")\n",
    "    print(f\"   Total emails: {total}\")\n",
    "    print(f\"   Entropy: {entropy:.4f}\")\n",
    "    \n",
    "    if entropy == 0:\n",
    "        print(f\"   â†’ Perfect classification! All emails are the same type.\")\n",
    "    elif entropy > 0.9:\n",
    "        print(f\"   â†’ High uncertainty! Difficult to predict email type.\")\n",
    "    else:\n",
    "        print(f\"   â†’ Moderate uncertainty. Some predictability exists.\")\n",
    "\n",
    "# Entropy vs Number of Classes\n",
    "print(f\"\\nðŸ”¢ ENTROPY WITH MULTIPLE CLASSES\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "multi_class_examples = [\n",
    "    {'classes': [10], 'name': '1 class (impossible in practice)'},\n",
    "    {'classes': [5, 5], 'name': '2 classes (binary)'},\n",
    "    {'classes': [3, 3, 4], 'name': '3 classes'},\n",
    "    {'classes': [2, 2, 3, 3], 'name': '4 classes'},\n",
    "    {'classes': [2, 2, 2, 2, 2], 'name': '5 classes'}\n",
    "]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Maximum possible entropy for different number of classes\n",
    "num_classes = [1, 2, 3, 4, 5]\n",
    "max_entropies = [0 if n == 1 else math.log2(n) for n in num_classes]\n",
    "\n",
    "ax1.bar(num_classes, max_entropies, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "ax1.set_xlabel('Number of Classes')\n",
    "ax1.set_ylabel('Maximum Possible Entropy')\n",
    "ax1.set_title('Maximum Entropy vs Number of Classes')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (n, max_ent) in enumerate(zip(num_classes, max_entropies)):\n",
    "    ax1.text(n, max_ent + 0.05, f'{max_ent:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Actual entropy calculations for examples\n",
    "actual_entropies = []\n",
    "example_names = []\n",
    "\n",
    "for example in multi_class_examples:\n",
    "    if len(example['classes']) > 1:  # Skip single class\n",
    "        entropy = calculate_entropy(example['classes'])\n",
    "        actual_entropies.append(entropy)\n",
    "        example_names.append(f\"{len(example['classes'])} classes\")\n",
    "\n",
    "ax2.bar(range(len(actual_entropies)), actual_entropies, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "ax2.set_xlabel('Example Scenarios')\n",
    "ax2.set_ylabel('Calculated Entropy')\n",
    "ax2.set_title('Actual Entropy for Equal Distribution')\n",
    "ax2.set_xticks(range(len(example_names)))\n",
    "ax2.set_xticklabels(example_names, rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for i, ent in enumerate(actual_entropies):\n",
    "    ax2.text(i, ent + 0.05, f'{ent:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nðŸ’¡ KEY INSIGHTS ABOUT ENTROPY:\")\n",
    "print(\"-\" * 35)\n",
    "insights = [\n",
    "    \"ðŸŽ¯ Lower entropy = better for decision trees (more pure splits)\",\n",
    "    \"âš–ï¸ Entropy = 0 means perfect classification (all same class)\", \n",
    "    \"ðŸŒªï¸ Higher entropy = more mixed classes = harder to classify\",\n",
    "    \"ðŸ“Š Binary classification: max entropy = 1.0 (50-50 split)\",\n",
    "    \"ðŸ”¢ For n classes: max entropy = logâ‚‚(n) with equal distribution\",\n",
    "    \"ðŸŽ² Entropy guides feature selection in decision tree algorithms\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"   {insight}\")\n",
    "\n",
    "print(f\"\\nâœ… Entropy concept explanation with mathematical examples completed!\")\n",
    "print(f\"Entropy is crucial for determining the best splits in decision trees.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efecb5c9",
   "metadata": {},
   "source": [
    "# Q4. What is Information Gain? Explain with examples and show calculations\n",
    "\n",
    "**Information Gain** is the primary metric used in decision trees (especially ID3 algorithm) to determine the best feature for splitting the dataset at each node. It measures how much **uncertainty** is reduced by splitting on a particular feature.\n",
    "\n",
    "## ðŸŽ¯ **Key Concepts:**\n",
    "\n",
    "### **What is Information Gain?**\n",
    "- **Information Gain** = Reduction in entropy after splitting on a feature\n",
    "- **Higher Information Gain** = better feature for splitting\n",
    "- **Goal**: Select the feature that maximizes information gain at each split\n",
    "- **Result**: Creates the most informative and efficient decision tree\n",
    "\n",
    "### **Mathematical Formula:**\n",
    "```\n",
    "Information Gain(S, A) = Entropy(S) - Weighted_Average_Entropy(S, A)\n",
    "\n",
    "Where:\n",
    "- S = current dataset/subset\n",
    "- A = attribute/feature being considered for split\n",
    "- Weighted_Average_Entropy = Î£(|Sv|/|S|) Ã— Entropy(Sv)\n",
    "- Sv = subset of S where attribute A has value v\n",
    "- |S| = size of dataset S\n",
    "```\n",
    "\n",
    "### **Detailed Formula:**\n",
    "```\n",
    "IG(S, A) = Entropy(S) - Î£(v âˆˆ Values(A)) (|Sv|/|S|) Ã— Entropy(Sv)\n",
    "\n",
    "Steps:\n",
    "1. Calculate entropy of original dataset: Entropy(S)\n",
    "2. Split dataset based on feature A into subsets Sv\n",
    "3. Calculate weighted average entropy of subsets\n",
    "4. Information Gain = Original Entropy - Weighted Average Entropy\n",
    "```\n",
    "\n",
    "### **Interpretation:**\n",
    "- **IG = 0**: No information gained (useless split)\n",
    "- **IG > 0**: Some information gained (useful split)  \n",
    "- **Higher IG**: More valuable the split (better feature)\n",
    "- **IG = Entropy(S)**: Perfect split (pure subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec37bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Gain Calculation Examples and Visualizations\n",
    "print(\"ðŸ“ˆ INFORMATION GAIN IN DECISION TREES - COMPLETE EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_information_gain(original_entropy, subsets_info):\n",
    "    \"\"\"\n",
    "    Calculate information gain\n",
    "    subsets_info: list of tuples (subset_size, subset_entropy)\n",
    "    \"\"\"\n",
    "    total_size = sum(size for size, _ in subsets_info)\n",
    "    weighted_entropy = sum((size/total_size) * entropy for size, entropy in subsets_info)\n",
    "    return original_entropy - weighted_entropy\n",
    "\n",
    "# Example 1: Play Tennis Dataset (Classic Example)\n",
    "print(\"ðŸŽ¾ EXAMPLE 1: PLAY TENNIS DATASET\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Original dataset\n",
    "tennis_data = {\n",
    "    'total_samples': 14,\n",
    "    'play_yes': 9,\n",
    "    'play_no': 5\n",
    "}\n",
    "\n",
    "# Calculate original entropy\n",
    "original_entropy = calculate_entropy([tennis_data['play_yes'], tennis_data['play_no']])\n",
    "print(f\"Original Dataset: {tennis_data['play_yes']} Yes, {tennis_data['play_no']} No\")\n",
    "print(f\"Original Entropy: {original_entropy:.4f}\")\n",
    "\n",
    "# Feature 1: Weather (Sunny, Overcast, Rainy)\n",
    "weather_splits = {\n",
    "    'Sunny': {'yes': 2, 'no': 3, 'total': 5},\n",
    "    'Overcast': {'yes': 4, 'no': 0, 'total': 4}, \n",
    "    'Rainy': {'yes': 3, 'no': 2, 'total': 5}\n",
    "}\n",
    "\n",
    "# Feature 2: Humidity (High, Normal)  \n",
    "humidity_splits = {\n",
    "    'High': {'yes': 3, 'no': 4, 'total': 7},\n",
    "    'Normal': {'yes': 6, 'no': 1, 'total': 7}\n",
    "}\n",
    "\n",
    "# Feature 3: Wind (Weak, Strong)\n",
    "wind_splits = {\n",
    "    'Weak': {'yes': 6, 'no': 2, 'total': 8},\n",
    "    'Strong': {'yes': 3, 'no': 3, 'total': 6}\n",
    "}\n",
    "\n",
    "# Calculate Information Gain for each feature\n",
    "features = {\n",
    "    'Weather': weather_splits,\n",
    "    'Humidity': humidity_splits, \n",
    "    'Wind': wind_splits\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Information Gain Calculations - Play Tennis Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (feature_name, splits) in enumerate(features.items()):\n",
    "    print(f\"\\nðŸŒŸ Feature: {feature_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calculate entropy for each subset\n",
    "    subset_entropies = []\n",
    "    total_weighted_entropy = 0\n",
    "    \n",
    "    for value, counts in splits.items():\n",
    "        subset_entropy = calculate_entropy([counts['yes'], counts['no']])\n",
    "        subset_entropies.append((counts['total'], subset_entropy))\n",
    "        weight = counts['total'] / tennis_data['total_samples']\n",
    "        total_weighted_entropy += weight * subset_entropy\n",
    "        \n",
    "        print(f\"{value}: {counts['yes']} Yes, {counts['no']} No â†’ Entropy = {subset_entropy:.4f}\")\n",
    "    \n",
    "    # Calculate Information Gain\n",
    "    info_gain = calculate_information_gain(original_entropy, subset_entropies)\n",
    "    results[feature_name] = info_gain\n",
    "    \n",
    "    print(f\"Weighted Average Entropy: {total_weighted_entropy:.4f}\")\n",
    "    print(f\"Information Gain: {original_entropy:.4f} - {total_weighted_entropy:.4f} = {info_gain:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    if idx < 3:\n",
    "        ax = axes[idx//2, idx%2]\n",
    "        \n",
    "        # Create bar chart showing entropy reduction\n",
    "        categories = ['Original'] + list(splits.keys())\n",
    "        entropies = [original_entropy] + [calculate_entropy([splits[val]['yes'], splits[val]['no']]) for val in splits.keys()]\n",
    "        colors = ['red'] + ['lightblue'] * len(splits)\n",
    "        \n",
    "        bars = ax.bar(categories, entropies, color=colors, alpha=0.7, edgecolor='navy')\n",
    "        ax.set_ylabel('Entropy')\n",
    "        ax.set_title(f'{feature_name}\\nInformation Gain = {info_gain:.4f}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, entropy in zip(bars, entropies):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{entropy:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Add sample size annotations\n",
    "        for i, (val, counts) in enumerate(splits.items()):\n",
    "            ax.text(i+1, -0.1, f'n={counts[\"total\"]}', ha='center', va='top', \n",
    "                   fontsize=9, style='italic')\n",
    "\n",
    "# Best feature selection\n",
    "best_feature = max(results, key=results.get)\n",
    "ax_summary = axes[1, 1]\n",
    "ax_summary.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "ðŸ“Š INFORMATION GAIN COMPARISON\n",
    "\n",
    "Weather:   {results['Weather']:.4f}\n",
    "Humidity:  {results['Humidity']:.4f}  \n",
    "Wind:      {results['Wind']:.4f}\n",
    "\n",
    "ðŸ† BEST FEATURE: {best_feature}\n",
    "   (Highest Information Gain)\n",
    "\n",
    "ðŸ’¡ DECISION: Split on {best_feature} first\n",
    "   as it provides maximum information gain\n",
    "   and reduces uncertainty the most.\n",
    "\n",
    "ðŸ“ˆ INFORMATION GAIN RANKING:\n",
    "   1. {sorted(results.items(), key=lambda x: x[1], reverse=True)[0][0]}: {sorted(results.items(), key=lambda x: x[1], reverse=True)[0][1]:.4f}\n",
    "   2. {sorted(results.items(), key=lambda x: x[1], reverse=True)[1][0]}: {sorted(results.items(), key=lambda x: x[1], reverse=True)[1][1]:.4f}\n",
    "   3. {sorted(results.items(), key=lambda x: x[1], reverse=True)[2][0]}: {sorted(results.items(), key=lambda x: x[1], reverse=True)[2][1]:.4f}\n",
    "\"\"\"\n",
    "\n",
    "ax_summary.text(0.1, 0.9, summary_text, transform=ax_summary.transAxes, fontsize=11,\n",
    "               verticalalignment='top', fontfamily='monospace',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example 2: Detailed Step-by-Step Calculation\n",
    "print(f\"\\nðŸ” DETAILED STEP-BY-STEP CALCULATION\")\n",
    "print(\"=\"*45)\n",
    "print(f\"Let's work through the {best_feature} feature calculation in detail:\")\n",
    "\n",
    "if best_feature == 'Weather':\n",
    "    splits = weather_splits\n",
    "elif best_feature == 'Humidity':  \n",
    "    splits = humidity_splits\n",
    "else:\n",
    "    splits = wind_splits\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Feature: {best_feature}\")\n",
    "print(\"â”€\" * 25)\n",
    "\n",
    "print(f\"Step 1: Original Dataset Entropy\")\n",
    "print(f\"   Total: {tennis_data['total_samples']} samples ({tennis_data['play_yes']} Yes, {tennis_data['play_no']} No)\")\n",
    "p_yes = tennis_data['play_yes'] / tennis_data['total_samples']\n",
    "p_no = tennis_data['play_no'] / tennis_data['total_samples']\n",
    "print(f\"   P(Yes) = {tennis_data['play_yes']}/{tennis_data['total_samples']} = {p_yes:.3f}\")\n",
    "print(f\"   P(No) = {tennis_data['play_no']}/{tennis_data['total_samples']} = {p_no:.3f}\")\n",
    "print(f\"   Entropy(S) = -({p_yes:.3f} Ã— logâ‚‚({p_yes:.3f}) + {p_no:.3f} Ã— logâ‚‚({p_no:.3f}))\")\n",
    "print(f\"             = {original_entropy:.4f}\")\n",
    "\n",
    "print(f\"\\nStep 2: Calculate Entropy for Each Subset\")\n",
    "weighted_sum = 0\n",
    "for i, (value, counts) in enumerate(splits.items()):\n",
    "    print(f\"   {value}: {counts['total']} samples ({counts['yes']} Yes, {counts['no']} No)\")\n",
    "    if counts['total'] > 0:\n",
    "        p_yes_subset = counts['yes'] / counts['total']\n",
    "        p_no_subset = counts['no'] / counts['total']\n",
    "        subset_entropy = calculate_entropy([counts['yes'], counts['no']])\n",
    "        weight = counts['total'] / tennis_data['total_samples']\n",
    "        weighted_contribution = weight * subset_entropy\n",
    "        weighted_sum += weighted_contribution\n",
    "        \n",
    "        print(f\"      P(Yes|{value}) = {counts['yes']}/{counts['total']} = {p_yes_subset:.3f}\")\n",
    "        print(f\"      P(No|{value}) = {counts['no']}/{counts['total']} = {p_no_subset:.3f}\")\n",
    "        print(f\"      Entropy({value}) = {subset_entropy:.4f}\")\n",
    "        print(f\"      Weight = {counts['total']}/{tennis_data['total_samples']} = {weight:.3f}\")\n",
    "        print(f\"      Weighted Contribution = {weight:.3f} Ã— {subset_entropy:.4f} = {weighted_contribution:.4f}\")\n",
    "\n",
    "print(f\"\\nStep 3: Calculate Weighted Average Entropy\")\n",
    "print(f\"   Weighted Average = {weighted_sum:.4f}\")\n",
    "\n",
    "print(f\"\\nStep 4: Calculate Information Gain\")\n",
    "final_ig = original_entropy - weighted_sum\n",
    "print(f\"   Information Gain = {original_entropy:.4f} - {weighted_sum:.4f} = {final_ig:.4f}\")\n",
    "\n",
    "# Example 3: Medical Diagnosis Example\n",
    "print(f\"\\nðŸ¥ EXAMPLE 2: MEDICAL DIAGNOSIS\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "medical_data = {\n",
    "    'total': 20,\n",
    "    'disease_yes': 12,\n",
    "    'disease_no': 8\n",
    "}\n",
    "\n",
    "medical_original_entropy = calculate_entropy([medical_data['disease_yes'], medical_data['disease_no']])\n",
    "print(f\"Medical Dataset: {medical_data['disease_yes']} Disease, {medical_data['disease_no']} No Disease\")\n",
    "print(f\"Original Entropy: {medical_original_entropy:.4f}\")\n",
    "\n",
    "# Medical features\n",
    "medical_features = {\n",
    "    'Fever': {\n",
    "        'High': {'disease': 8, 'healthy': 2, 'total': 10},\n",
    "        'Low': {'disease': 4, 'healthy': 6, 'total': 10}\n",
    "    },\n",
    "    'Cough': {\n",
    "        'Present': {'disease': 10, 'healthy': 3, 'total': 13},\n",
    "        'Absent': {'disease': 2, 'healthy': 5, 'total': 7}\n",
    "    },\n",
    "    'Age': {\n",
    "        'Young': {'disease': 3, 'healthy': 5, 'total': 8},\n",
    "        'Old': {'disease': 9, 'healthy': 3, 'total': 12}\n",
    "    }\n",
    "}\n",
    "\n",
    "medical_results = {}\n",
    "\n",
    "print(f\"\\nMedical Feature Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for feature_name, splits in medical_features.items():\n",
    "    subset_info = []\n",
    "    for value, counts in splits.items():\n",
    "        subset_entropy = calculate_entropy([counts['disease'], counts['healthy']])\n",
    "        subset_info.append((counts['total'], subset_entropy))\n",
    "    \n",
    "    info_gain = calculate_information_gain(medical_original_entropy, subset_info)\n",
    "    medical_results[feature_name] = info_gain\n",
    "    print(f\"{feature_name}: Information Gain = {info_gain:.4f}\")\n",
    "\n",
    "best_medical_feature = max(medical_results, key=medical_results.get)\n",
    "print(f\"\\nðŸ† Best Medical Feature: {best_medical_feature} (IG = {medical_results[best_medical_feature]:.4f})\")\n",
    "\n",
    "# Information Gain Comparison Chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Tennis dataset comparison\n",
    "tennis_features = list(results.keys())\n",
    "tennis_gains = list(results.values())\n",
    "bars1 = ax1.bar(tennis_features, tennis_gains, color=['gold', 'lightblue', 'lightcoral'], \n",
    "               alpha=0.8, edgecolor='navy')\n",
    "ax1.set_title('Play Tennis Dataset\\nInformation Gain by Feature')\n",
    "ax1.set_ylabel('Information Gain')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, gain in zip(bars1, tennis_gains):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{gain:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Medical dataset comparison  \n",
    "medical_features_list = list(medical_results.keys())\n",
    "medical_gains = list(medical_results.values())\n",
    "bars2 = ax2.bar(medical_features_list, medical_gains, color=['lightgreen', 'orange', 'pink'],\n",
    "               alpha=0.8, edgecolor='navy') \n",
    "ax2.set_title('Medical Diagnosis Dataset\\nInformation Gain by Feature')\n",
    "ax2.set_ylabel('Information Gain')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, gain in zip(bars2, medical_gains):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{gain:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nðŸ’¡ KEY INSIGHTS ABOUT INFORMATION GAIN:\")\n",
    "print(\"-\" * 42)\n",
    "insights = [\n",
    "    \"ðŸŽ¯ Higher Information Gain = Better feature for splitting\",\n",
    "    \"âš¡ Information Gain = Entropy Reduction after splitting\", \n",
    "    \"ðŸ”„ ID3 algorithm uses Information Gain to build decision trees\",\n",
    "    \"ðŸ“Š Always choose feature with maximum Information Gain\",\n",
    "    \"ðŸŽ² Perfect split: IG = Original Entropy (pure subsets)\",\n",
    "    \"âŒ Useless split: IG = 0 (no entropy reduction)\",\n",
    "    \"ðŸŒ³ Greedy approach: locally optimal decisions at each node\",\n",
    "    \"âš–ï¸ Weighted average considers subset sizes proportionally\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"   {insight}\")\n",
    "\n",
    "print(f\"\\nâœ… Information Gain explanation with detailed calculations completed!\")\n",
    "print(f\"Information Gain is the driving force behind decision tree construction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f6189",
   "metadata": {},
   "source": [
    "# Q5. Compare Gini Impurity and Entropy as splitting criteria. Which one is better?\n",
    "\n",
    "Both **Gini Impurity** and **Entropy** are impurity measures used in decision trees to determine the best splits. While they serve the same purpose, they have different mathematical formulations and computational characteristics.\n",
    "\n",
    "## ðŸŽ¯ **Mathematical Formulations:**\n",
    "\n",
    "### **Gini Impurity:**\n",
    "```\n",
    "Gini(S) = 1 - Î£(i=1 to c) p_iÂ²\n",
    "\n",
    "Where:\n",
    "- S = dataset or subset  \n",
    "- c = number of classes\n",
    "- p_i = proportion of samples belonging to class i\n",
    "- Range: 0 (pure) to 0.5 (maximum impurity for binary classification)\n",
    "```\n",
    "\n",
    "### **Entropy:**\n",
    "```\n",
    "Entropy(S) = -Î£(i=1 to c) p_i Ã— logâ‚‚(p_i)\n",
    "\n",
    "Where:\n",
    "- S = dataset or subset\n",
    "- c = number of classes  \n",
    "- p_i = proportion of samples belonging to class i\n",
    "- Range: 0 (pure) to 1 (maximum impurity for binary classification)\n",
    "```\n",
    "\n",
    "## âš–ï¸ **Key Differences:**\n",
    "\n",
    "| Aspect | Gini Impurity | Entropy |\n",
    "|--------|---------------|---------|\n",
    "| **Formula** | 1 - Î£p_iÂ² | -Î£p_i Ã— logâ‚‚(p_i) |\n",
    "| **Computation** | Faster (no logarithm) | Slower (logarithm required) |\n",
    "| **Range (binary)** | 0 to 0.5 | 0 to 1.0 |\n",
    "| **Sensitivity** | Less sensitive to changes | More sensitive to changes |\n",
    "| **Algorithm** | CART (Classification Trees) | ID3, C4.5 |\n",
    "| **Curve Shape** | Quadratic | Logarithmic |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Impurity vs Entropy - Comprehensive Comparison\n",
    "print(\"âš–ï¸ GINI IMPURITY vs ENTROPY - COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "def calculate_gini(class_counts):\n",
    "    \"\"\"Calculate Gini Impurity given class counts\"\"\"\n",
    "    total = sum(class_counts)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    \n",
    "    gini = 1.0\n",
    "    for count in class_counts:\n",
    "        probability = count / total\n",
    "        gini -= probability ** 2\n",
    "    \n",
    "    return gini\n",
    "\n",
    "def calculate_gini_gain(original_gini, subsets_info):\n",
    "    \"\"\"Calculate Gini Gain (similar to Information Gain)\"\"\"\n",
    "    total_size = sum(size for size, _ in subsets_info)\n",
    "    weighted_gini = sum((size/total_size) * gini for size, gini in subsets_info)\n",
    "    return original_gini - weighted_gini\n",
    "\n",
    "# Example datasets for comparison\n",
    "comparison_datasets = [\n",
    "    {'name': 'Perfect Purity', 'positive': 10, 'negative': 0},\n",
    "    {'name': 'Maximum Impurity', 'positive': 5, 'negative': 5},\n",
    "    {'name': 'Moderate Skew', 'positive': 7, 'negative': 3},\n",
    "    {'name': 'High Skew', 'positive': 8, 'negative': 2},\n",
    "    {'name': 'Very High Skew', 'positive': 9, 'negative': 1}\n",
    "]\n",
    "\n",
    "# Calculate both measures for each dataset\n",
    "comparison_results = []\n",
    "\n",
    "print(\"ðŸ“Š DIRECT COMPARISON: GINI vs ENTROPY\")\n",
    "print(\"=\"*45)\n",
    "print(f\"{'Dataset':<20} {'Positive':<8} {'Negative':<8} {'Gini':<8} {'Entropy':<8} {'Difference':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for dataset in comparison_datasets:\n",
    "    pos = dataset['positive']\n",
    "    neg = dataset['negative']\n",
    "    \n",
    "    gini = calculate_gini([pos, neg])\n",
    "    entropy = calculate_entropy([pos, neg])\n",
    "    difference = abs(entropy - gini)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'name': dataset['name'],\n",
    "        'positive': pos,\n",
    "        'negative': neg,\n",
    "        'gini': gini,\n",
    "        'entropy': entropy,\n",
    "        'difference': difference\n",
    "    })\n",
    "    \n",
    "    print(f\"{dataset['name']:<20} {pos:<8} {neg:<8} {gini:<8.4f} {entropy:<8.4f} {difference:<10.4f}\")\n",
    "\n",
    "# Visualization: Gini vs Entropy curves\n",
    "print(f\"\\nðŸ“ˆ VISUALIZATION: GINI vs ENTROPY CURVES\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Create probability range for binary classification\n",
    "p_values = np.linspace(0.001, 0.999, 1000)  # Avoid 0 and 1 to prevent log(0)\n",
    "gini_values = [2 * p * (1 - p) for p in p_values]  # Gini = 2p(1-p) for binary\n",
    "entropy_values = [-p * math.log2(p) - (1-p) * math.log2(1-p) for p in p_values]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Gini Impurity vs Entropy - Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Impurity curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(p_values, gini_values, 'b-', linewidth=3, label='Gini Impurity', alpha=0.8)\n",
    "ax1.plot(p_values, entropy_values, 'r-', linewidth=3, label='Entropy', alpha=0.8)\n",
    "ax1.set_xlabel('Probability of Positive Class (p)')\n",
    "ax1.set_ylabel('Impurity Measure')\n",
    "ax1.set_title('Impurity Curves Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight key points\n",
    "key_points = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for p in key_points:\n",
    "    gini_val = 2 * p * (1 - p)\n",
    "    entropy_val = -p * math.log2(p) - (1-p) * math.log2(1-p)\n",
    "    ax1.plot(p, gini_val, 'bo', markersize=8)\n",
    "    ax1.plot(p, entropy_val, 'ro', markersize=8)\n",
    "    ax1.text(p, max(gini_val, entropy_val) + 0.05, f'p={p}', ha='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "# Plot 2: Difference between measures\n",
    "ax2 = axes[0, 1]\n",
    "difference_values = [abs(e - g) for e, g in zip(entropy_values, gini_values)]\n",
    "ax2.plot(p_values, difference_values, 'g-', linewidth=3, label='|Entropy - Gini|')\n",
    "ax2.set_xlabel('Probability of Positive Class (p)')\n",
    "ax2.set_ylabel('Absolute Difference')\n",
    "ax2.set_title('Difference Between Entropy and Gini')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Find maximum difference\n",
    "max_diff_idx = np.argmax(difference_values)\n",
    "max_diff_p = p_values[max_diff_idx]\n",
    "max_diff_val = difference_values[max_diff_idx]\n",
    "ax2.plot(max_diff_p, max_diff_val, 'ro', markersize=10)\n",
    "ax2.text(max_diff_p, max_diff_val + 0.01, f'Max diff at p={max_diff_p:.3f}', \n",
    "         ha='center', fontweight='bold', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow'))\n",
    "\n",
    "# Plot 3: Real dataset comparison\n",
    "ax3 = axes[1, 0]\n",
    "dataset_names = [result['name'] for result in comparison_results]\n",
    "gini_vals = [result['gini'] for result in comparison_results]\n",
    "entropy_vals = [result['entropy'] for result in comparison_results]\n",
    "\n",
    "x = np.arange(len(dataset_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, gini_vals, width, label='Gini Impurity', color='skyblue', alpha=0.8)\n",
    "bars2 = ax3.bar(x + width/2, entropy_vals, width, label='Entropy', color='lightcoral', alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Dataset Types')\n",
    "ax3.set_ylabel('Impurity Value')\n",
    "ax3.set_title('Real Dataset Comparison')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(dataset_names, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "\n",
    "# Plot 4: Decision tree splitting example\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "# Performance comparison table\n",
    "performance_text = \"\"\"\n",
    "ðŸƒ COMPUTATIONAL PERFORMANCE\n",
    "\n",
    "                    Gini        Entropy\n",
    "Computation Speed:  â­â­â­â­â­     â­â­â­\n",
    "Memory Usage:       â­â­â­â­â­     â­â­â­â­\n",
    "Sensitivity:        â­â­â­       â­â­â­â­â­\n",
    "Mathematical:       â­â­â­       â­â­â­â­â­\n",
    "\n",
    "ðŸŽ¯ USE CASES\n",
    "\n",
    "GINI IMPURITY:\n",
    "âœ… Large datasets (faster computation)\n",
    "âœ… Real-time applications  \n",
    "âœ… CART algorithm\n",
    "âœ… When speed matters more than precision\n",
    "\n",
    "ENTROPY:\n",
    "âœ… Theoretical analysis\n",
    "âœ… Information theory applications\n",
    "âœ… ID3, C4.5 algorithms\n",
    "âœ… When mathematical rigor is important\n",
    "\n",
    "ðŸ† WINNER: Context Dependent!\n",
    "â€¢ Speed needed â†’ Gini\n",
    "â€¢ Theory/Research â†’ Entropy\n",
    "â€¢ Most practical applications â†’ Gini\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, performance_text, transform=ax4.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed mathematical comparison\n",
    "print(f\"\\nðŸ”¢ DETAILED MATHEMATICAL COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"Example: Dataset with 6 positive, 4 negative samples\")\n",
    "pos, neg = 6, 4\n",
    "total = pos + neg\n",
    "\n",
    "print(f\"\\nGini Impurity Calculation:\")\n",
    "p_pos = pos / total\n",
    "p_neg = neg / total\n",
    "gini_result = 1 - (p_pos**2 + p_neg**2)\n",
    "print(f\"  Gini = 1 - (p_posÂ² + p_negÂ²)\")\n",
    "print(f\"       = 1 - ({p_pos:.2f}Â² + {p_neg:.2f}Â²)\")\n",
    "print(f\"       = 1 - ({p_pos**2:.4f} + {p_neg**2:.4f})\")\n",
    "print(f\"       = 1 - {p_pos**2 + p_neg**2:.4f}\")\n",
    "print(f\"       = {gini_result:.4f}\")\n",
    "\n",
    "print(f\"\\nEntropy Calculation:\")\n",
    "entropy_result = -(p_pos * math.log2(p_pos) + p_neg * math.log2(p_neg))\n",
    "print(f\"  Entropy = -(p_pos Ã— logâ‚‚(p_pos) + p_neg Ã— logâ‚‚(p_neg))\")\n",
    "print(f\"          = -({p_pos:.2f} Ã— logâ‚‚({p_pos:.2f}) + {p_neg:.2f} Ã— logâ‚‚({p_neg:.2f}))\")\n",
    "print(f\"          = -({p_pos:.2f} Ã— {math.log2(p_pos):.4f} + {p_neg:.2f} Ã— {math.log2(p_neg):.4f})\")\n",
    "print(f\"          = -({p_pos * math.log2(p_pos):.4f} + {p_neg * math.log2(p_neg):.4f})\")\n",
    "print(f\"          = {entropy_result:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference: |{entropy_result:.4f} - {gini_result:.4f}| = {abs(entropy_result - gini_result):.4f}\")\n",
    "\n",
    "# Practical example: Feature selection comparison\n",
    "print(f\"\\nðŸŒŸ PRACTICAL EXAMPLE: FEATURE SELECTION\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Sample dataset for feature selection\n",
    "sample_data = {\n",
    "    'total': 16,\n",
    "    'class_a': 9,\n",
    "    'class_b': 7\n",
    "}\n",
    "\n",
    "original_gini = calculate_gini([sample_data['class_a'], sample_data['class_b']])\n",
    "original_entropy = calculate_entropy([sample_data['class_a'], sample_data['class_b']])\n",
    "\n",
    "print(f\"Original Dataset: {sample_data['class_a']} Class A, {sample_data['class_b']} Class B\")\n",
    "print(f\"Original Gini: {original_gini:.4f}\")\n",
    "print(f\"Original Entropy: {original_entropy:.4f}\")\n",
    "\n",
    "# Feature comparison\n",
    "features_comparison = {\n",
    "    'Feature X': {\n",
    "        'Left': {'a': 2, 'b': 6},\n",
    "        'Right': {'a': 7, 'b': 1}\n",
    "    },\n",
    "    'Feature Y': {\n",
    "        'Left': {'a': 5, 'b': 3}, \n",
    "        'Right': {'a': 4, 'b': 4}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nFeature Selection Comparison:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for feature_name, splits in features_comparison.items():\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    \n",
    "    # Calculate Gini Gain\n",
    "    gini_subsets = []\n",
    "    entropy_subsets = []\n",
    "    \n",
    "    for split_name, counts in splits.items():\n",
    "        subset_gini = calculate_gini([counts['a'], counts['b']])\n",
    "        subset_entropy = calculate_entropy([counts['a'], counts['b']])\n",
    "        subset_size = counts['a'] + counts['b']\n",
    "        \n",
    "        gini_subsets.append((subset_size, subset_gini))\n",
    "        entropy_subsets.append((subset_size, subset_entropy))\n",
    "        \n",
    "        print(f\"  {split_name}: {counts['a']} A, {counts['b']} B â†’ Gini={subset_gini:.4f}, Entropy={subset_entropy:.4f}\")\n",
    "    \n",
    "    gini_gain = calculate_gini_gain(original_gini, gini_subsets)\n",
    "    info_gain = calculate_information_gain(original_entropy, entropy_subsets)\n",
    "    \n",
    "    print(f\"  Gini Gain: {gini_gain:.4f}\")\n",
    "    print(f\"  Information Gain: {info_gain:.4f}\")\n",
    "    \n",
    "    # Determine best feature by each metric\n",
    "    if feature_name == 'Feature X':\n",
    "        x_gini, x_entropy = gini_gain, info_gain\n",
    "    else:\n",
    "        y_gini, y_entropy = gini_gain, info_gain\n",
    "\n",
    "print(f\"\\nFeature Selection Results:\")\n",
    "print(f\"  Gini prefers: {'Feature X' if x_gini > y_gini else 'Feature Y'}\")\n",
    "print(f\"  Entropy prefers: {'Feature X' if x_entropy > y_entropy else 'Feature Y'}\")\n",
    "\n",
    "if (x_gini > y_gini) == (x_entropy > y_entropy):\n",
    "    print(\"  âœ… Both metrics agree on the best feature!\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Metrics disagree - rare but possible!\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\nðŸŽ¯ FINAL RECOMMENDATION\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "recommendations = [\n",
    "    \"ðŸš€ For production systems: Use Gini (faster computation)\",\n",
    "    \"ðŸ“š For research/education: Use Entropy (more interpretable)\", \n",
    "    \"âš¡ For large datasets: Definitely use Gini\",\n",
    "    \"ðŸ”¬ For theoretical work: Entropy provides better insights\",\n",
    "    \"ðŸ­ Most ML libraries default to Gini for good reason\",\n",
    "    \"ðŸ“Š Both usually give similar tree structures\",\n",
    "    \"ðŸŽ² Choice rarely affects final model performance significantly\",\n",
    "    \"ðŸ’¡ When in doubt, benchmark both on your specific data\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "print(f\"\\nâœ… Gini vs Entropy comparison completed!\")\n",
    "print(f\"Both are excellent metrics - choose based on your specific needs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ea1de",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Part 1 Conclusion: Decision Tree Theory Mastery\n",
    "\n",
    "## ðŸ“š **What We've Learned:**\n",
    "\n",
    "### **Core Concepts Covered:**\n",
    "1. **Decision Tree Fundamentals** - Understanding the algorithm and its applications\n",
    "2. **Tree Components** - Root nodes, internal nodes, leaf nodes, and branches  \n",
    "3. **Entropy** - Mathematical foundation for measuring dataset impurity\n",
    "4. **Information Gain** - The driving force behind optimal feature selection\n",
    "5. **Gini vs Entropy** - Comparative analysis of splitting criteria\n",
    "\n",
    "### **Key Mathematical Insights:**\n",
    "- **Entropy**: Measures uncertainty and guides optimal splits\n",
    "- **Information Gain**: Quantifies the value of each feature for classification\n",
    "- **Gini Impurity**: Provides a computationally efficient alternative to entropy\n",
    "- **Weighted Averages**: Essential for calculating gains across subsets\n",
    "\n",
    "### **Practical Knowledge Gained:**\n",
    "âœ… How to identify the best features for decision tree splits  \n",
    "âœ… Mathematical calculations behind tree construction  \n",
    "âœ… Understanding trade-offs between different splitting criteria  \n",
    "âœ… Real-world applications in various domains (medical, business, etc.)  \n",
    "âœ… Performance considerations for large datasets  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ **Ready for Part 2!**\n",
    "\n",
    "With this solid theoretical foundation, you're now prepared to tackle the **practical implementation** in **Part 2**, where we'll:\n",
    "\n",
    "- ðŸ„ Work with the **Mushroom Classification Dataset**\n",
    "- ðŸ› ï¸ Implement decision trees from scratch and using sklearn\n",
    "- ðŸ“Š Apply preprocessing techniques and feature engineering\n",
    "- ðŸŽ¯ Evaluate model performance with various metrics\n",
    "- ðŸ”§ Perform hyperparameter tuning and optimization\n",
    "- ðŸ“ˆ Create comprehensive visualizations and interpretations\n",
    "\n",
    "**The theory you've mastered here will directly inform every decision in the practical implementation!**\n",
    "\n",
    "---\n",
    "\n",
    "*Excellent work completing the theoretical foundation! Now let's put this knowledge into practice with real data.* ðŸŒŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40bcf8",
   "metadata": {},
   "source": [
    "# Decision Tree â€“ Part 1: Theoretical Understanding\n",
    "\n",
    "This notebook contains theoretical questions and explanations about Decision Trees, covering fundamental concepts, mathematical foundations, and comparative analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
