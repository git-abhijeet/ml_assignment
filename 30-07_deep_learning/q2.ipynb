{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94a1368",
   "metadata": {},
   "source": [
    "# Build a Single-Layer Custom ANN\n",
    "\n",
    "## Assignment: Manual Implementation of Binary Classification Neural Network\n",
    "\n",
    "This notebook demonstrates building a single-layer artificial neural network from scratch using only basic PyTorch operations.\n",
    "\n",
    "**Model Architecture:**\n",
    "- Linear layer: Y = w^T * x + b\n",
    "- Activation function: Sigmoid\n",
    "- Loss function: Binary Cross Entropy\n",
    "- Optimizer: Manual gradient descent\n",
    "\n",
    "**Dataset:**\n",
    "- Binary classification with 2 features\n",
    "- Generated using sklearn.datasets.make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325972eb",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66381507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82740e",
   "metadata": {},
   "source": [
    "## Step 2: Generate and Save Dataset\n",
    "\n",
    "Create a binary classification dataset with 2 features and save it to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,        # Increased sample size for better training\n",
    "    n_features=2,          # 2 features as specified\n",
    "    n_classes=2,           # Binary classification\n",
    "    n_redundant=0,         # No redundant features\n",
    "    n_informative=2,       # Both features are informative\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=['f1', 'f2'])\n",
    "df['label'] = y\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = 'binary_data.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Dataset saved to {csv_path}\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Features: {df.columns.tolist()}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf14984",
   "metadata": {},
   "source": [
    "## Step 3: Data Visualization\n",
    "\n",
    "Visualize the generated dataset to understand the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df702c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Scatter plot of features colored by class\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue']\n",
    "for i, label in enumerate([0, 1]):\n",
    "    mask = df['label'] == label\n",
    "    plt.scatter(df[mask]['f1'], df[mask]['f2'], \n",
    "               c=colors[i], label=f'Class {label}', alpha=0.6)\n",
    "plt.xlabel('Feature 1 (f1)')\n",
    "plt.ylabel('Feature 2 (f2)')\n",
    "plt.title('Dataset Visualization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Feature distributions\n",
    "plt.subplot(1, 2, 2)\n",
    "df[df['label'] == 0][['f1', 'f2']].hist(alpha=0.5, label='Class 0', bins=20)\n",
    "df[df['label'] == 1][['f1', 'f2']].hist(alpha=0.5, label='Class 1', bins=20)\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Feature Distributions by Class')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf07e22",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing\n",
    "\n",
    "Load data from CSV, split into train/test sets, and normalize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee11c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "df_loaded = pd.read_csv(csv_path)\n",
    "print(f\"Loaded dataset shape: {df_loaded.shape}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_loaded[['f1', 'f2']].values\n",
    "y = df_loaded['label'].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling completed\")\n",
    "print(f\"Training features - Mean: {X_train_scaled.mean(axis=0)}, Std: {X_train_scaled.std(axis=0)}\")\n",
    "print(f\"Test features - Mean: {X_test_scaled.mean(axis=0)}, Std: {X_test_scaled.std(axis=0)}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "print(f\"\\nTensor shapes:\")\n",
    "print(f\"X_train: {X_train_tensor.shape}, y_train: {y_train_tensor.shape}\")\n",
    "print(f\"X_test: {X_test_tensor.shape}, y_test: {y_test_tensor.shape}\")\n",
    "print(f\"Tensors are on device: {X_train_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b66940",
   "metadata": {},
   "source": [
    "## Step 5: Define Custom Single-Layer Neural Network\n",
    "\n",
    "Implement the neural network components manually:\n",
    "- Linear transformation: Y = w^T * x + b\n",
    "- Sigmoid activation function\n",
    "- Binary cross-entropy loss\n",
    "- Manual gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerANN:\n",
    "    def __init__(self, input_size, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the single-layer neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of input features\n",
    "            device (str): Device to run computations on\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Initialize weights and bias with small random values\n",
    "        # Weight matrix: (input_size, 1) for single output neuron\n",
    "        self.weights = torch.randn(input_size, 1, device=device, requires_grad=False) * 0.1\n",
    "        self.bias = torch.zeros(1, device=device, requires_grad=False)\n",
    "        \n",
    "        print(f\"Initialized ANN with:\")\n",
    "        print(f\"  Input size: {input_size}\")\n",
    "        print(f\"  Weights shape: {self.weights.shape}\")\n",
    "        print(f\"  Bias shape: {self.bias.shape}\")\n",
    "        print(f\"  Device: {device}\")\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function: Ïƒ(z) = 1 / (1 + e^(-z))\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Sigmoid output\n",
    "        \"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = torch.clamp(z, -500, 500)\n",
    "        return 1 / (1 + torch.exp(-z))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: Y = Ïƒ(w^T * x + b)\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Input features (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Predictions (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Linear transformation: z = X @ w + b\n",
    "        z = torch.matmul(X, self.weights) + self.bias\n",
    "        \n",
    "        # Apply sigmoid activation\n",
    "        predictions = self.sigmoid(z)\n",
    "        \n",
    "        return predictions.squeeze()  # Remove extra dimension\n",
    "    \n",
    "    def binary_cross_entropy(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Binary Cross Entropy Loss: -[y*log(p) + (1-y)*log(1-p)]\n",
    "        \n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted probabilities\n",
    "            targets (torch.Tensor): True labels (0 or 1)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Average loss\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        epsilon = 1e-7\n",
    "        predictions = torch.clamp(predictions, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Compute binary cross entropy\n",
    "        loss = -(targets * torch.log(predictions) + \n",
    "                (1 - targets) * torch.log(1 - predictions))\n",
    "        \n",
    "        return torch.mean(loss)\n",
    "    \n",
    "    def compute_gradients(self, X, predictions, targets):\n",
    "        \"\"\"\n",
    "        Manually compute gradients for weights and bias\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Input features\n",
    "            predictions (torch.Tensor): Model predictions\n",
    "            targets (torch.Tensor): True labels\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (weight_gradients, bias_gradients)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Gradient of loss w.r.t predictions: dL/dp = (p - y) / [p(1-p)]\n",
    "        # But for sigmoid + BCE, this simplifies to: dL/dz = p - y\n",
    "        dL_dz = predictions - targets\n",
    "        \n",
    "        # Gradient w.r.t weights: dL/dw = X^T @ dL_dz\n",
    "        dL_dw = torch.matmul(X.T, dL_dz.unsqueeze(1)) / batch_size\n",
    "        \n",
    "        # Gradient w.r.t bias: dL/db = mean(dL_dz)\n",
    "        dL_db = torch.mean(dL_dz)\n",
    "        \n",
    "        return dL_dw, dL_db\n",
    "    \n",
    "    def update_parameters(self, weight_grad, bias_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights and bias using gradient descent\n",
    "        \n",
    "        Args:\n",
    "            weight_grad (torch.Tensor): Weight gradients\n",
    "            bias_grad (torch.Tensor): Bias gradients\n",
    "            learning_rate (float): Learning rate\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * weight_grad\n",
    "        self.bias -= learning_rate * bias_grad\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Make binary predictions\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Input features\n",
    "            threshold (float): Classification threshold\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities >= threshold).float()\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute classification accuracy\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Input features\n",
    "            y (torch.Tensor): True labels\n",
    "        \n",
    "        Returns:\n",
    "            float: Accuracy percentage\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        correct = (predictions == y).float()\n",
    "        return torch.mean(correct).item() * 100\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_tensor.shape[1]  # Number of features\n",
    "model = SingleLayerANN(input_size, device=device)\n",
    "\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"Initial weights: {model.weights.squeeze().detach().cpu().numpy()}\")\n",
    "print(f\"Initial bias: {model.bias.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11f12e",
   "metadata": {},
   "source": [
    "## Step 6: Training the Neural Network\n",
    "\n",
    "Train the single-layer neural network using manual gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c795c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "print_every = 10\n",
    "\n",
    "# Lists to store training history\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model.forward(X_train_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = model.binary_cross_entropy(predictions, y_train_tensor)\n",
    "    \n",
    "    # Compute gradients\n",
    "    weight_grad, bias_grad = model.compute_gradients(\n",
    "        X_train_tensor, predictions, y_train_tensor\n",
    "    )\n",
    "    \n",
    "    # Update parameters\n",
    "    model.update_parameters(weight_grad, bias_grad, learning_rate)\n",
    "    \n",
    "    # Store training metrics\n",
    "    train_losses.append(loss.item())\n",
    "    train_acc = model.accuracy(X_train_tensor, y_train_tensor)\n",
    "    test_acc = model.accuracy(X_test_tensor, y_test_tensor)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1:3d}: Loss = {loss.item():.4f}, \"\n",
    "              f\"Train Acc = {train_acc:.1f}%, Test Acc = {test_acc:.1f}%\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Final results\n",
    "final_train_acc = model.accuracy(X_train_tensor, y_train_tensor)\n",
    "final_test_acc = model.accuracy(X_test_tensor, y_test_tensor)\n",
    "final_loss = train_losses[-1]\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Final Loss: {final_loss:.4f}\")\n",
    "print(f\"Final Training Accuracy: {final_train_acc:.1f}%\")\n",
    "print(f\"Final Test Accuracy: {final_test_acc:.1f}%\")\n",
    "print(f\"\\nLearned Parameters:\")\n",
    "print(f\"Weights: {model.weights.squeeze().detach().cpu().numpy()}\")\n",
    "print(f\"Bias: {model.bias.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519a52b",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training Progress\n",
    "\n",
    "Plot the training loss and accuracy curves to analyze the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f10085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "axes[0].plot(range(1, epochs + 1), train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Binary Cross Entropy Loss')\n",
    "axes[0].set_title('Training Loss Over Time')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Accuracy Curves\n",
    "axes[1].plot(range(1, epochs + 1), train_accuracies, 'g-', linewidth=2, label='Training Accuracy')\n",
    "axes[1].plot(range(1, epochs + 1), test_accuracies, 'r-', linewidth=2, label='Test Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy Over Time')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key training milestones\n",
    "print(\"Training Milestones:\")\n",
    "print(f\"Epoch 1: Loss = {train_losses[0]:.2f}\")\n",
    "if len(train_losses) >= 30:\n",
    "    print(f\"Epoch 30: Loss = {train_losses[29]:.2f}\")\n",
    "print(f\"Final Epoch {epochs}: Loss = {train_losses[-1]:.2f}\")\n",
    "print(f\"Accuracy on test set = {final_test_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d48bb",
   "metadata": {},
   "source": [
    "## Step 8: Decision Boundary Visualization\n",
    "\n",
    "Visualize the decision boundary learned by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa100826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, scaler, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of the trained model\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a mesh of points\n",
    "    h = 0.02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    mesh_points_scaled = scaler.transform(mesh_points)\n",
    "    mesh_tensor = torch.FloatTensor(mesh_points_scaled).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Z = model.forward(mesh_tensor).cpu().numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    plt.colorbar(label='Prediction Probability')\n",
    "    \n",
    "    # Plot the data points\n",
    "    colors = ['red', 'blue']\n",
    "    for i, label in enumerate([0, 1]):\n",
    "        mask = y == label\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], c=colors[i], \n",
    "                   label=f'Class {label}', alpha=0.8, s=50, edgecolors='black')\n",
    "    \n",
    "    plt.xlabel('Feature 1 (f1)')\n",
    "    plt.ylabel('Feature 2 (f2)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for training data\n",
    "plot_decision_boundary(model, X_train, y_train, scaler, \n",
    "                      \"Decision Boundary - Training Data\")\n",
    "\n",
    "# Plot decision boundary for test data\n",
    "plot_decision_boundary(model, X_test, y_test, scaler, \n",
    "                      \"Decision Boundary - Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcff276",
   "metadata": {},
   "source": [
    "## Step 9: Model Evaluation and Analysis\n",
    "\n",
    "Perform detailed evaluation of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed predictions analysis\n",
    "with torch.no_grad():\n",
    "    train_probs = model.forward(X_train_tensor).cpu().numpy()\n",
    "    test_probs = model.forward(X_test_tensor).cpu().numpy()\n",
    "    train_preds = model.predict(X_train_tensor).cpu().numpy()\n",
    "    test_preds = model.predict(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix manually\n",
    "def compute_confusion_matrix(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "train_cm = compute_confusion_matrix(y_train, train_preds)\n",
    "test_cm = compute_confusion_matrix(y_test, test_preds)\n",
    "\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  Accuracy: {final_train_acc:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\")\n",
    "print(f\"    TN: {train_cm[0,0]}, FP: {train_cm[0,1]}\")\n",
    "print(f\"    FN: {train_cm[1,0]}, TP: {train_cm[1,1]}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Accuracy: {final_test_acc:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\")\n",
    "print(f\"    TN: {test_cm[0,0]}, FP: {test_cm[0,1]}\")\n",
    "print(f\"    FN: {test_cm[1,0]}, TP: {test_cm[1,1]}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "def calculate_metrics(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "test_precision, test_recall, test_f1 = calculate_metrics(test_cm)\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  Precision: {test_precision:.3f}\")\n",
    "print(f\"  Recall: {test_recall:.3f}\")\n",
    "print(f\"  F1-Score: {test_f1:.3f}\")\n",
    "\n",
    "# Prediction confidence analysis\n",
    "print(f\"\\nPrediction Confidence Analysis:\")\n",
    "print(f\"  Test probabilities - Min: {test_probs.min():.3f}, Max: {test_probs.max():.3f}\")\n",
    "print(f\"  Test probabilities - Mean: {test_probs.mean():.3f}, Std: {test_probs.std():.3f}\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(f\"\\nExample Predictions (first 10 test samples):\")\n",
    "print(\"True | Pred | Prob\")\n",
    "print(\"-\" * 20)\n",
    "for i in range(min(10, len(y_test))):\n",
    "    print(f\"  {int(y_test[i])}  |  {int(test_preds[i])}   | {test_probs[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75994e64",
   "metadata": {},
   "source": [
    "## Step 10: Sample Output Summary\n",
    "\n",
    "Display results in the format requested in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22366167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ASSIGNMENT SAMPLE OUTPUT\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Display key training epochs as requested\n",
    "print(f\"Epoch 1: Loss = {train_losses[0]:.2f}\")\n",
    "if len(train_losses) >= 30:\n",
    "    print(f\"Epoch 30: Loss = {train_losses[29]:.2f}\")\n",
    "else:\n",
    "    print(f\"Epoch {min(30, epochs)}: Loss = {train_losses[min(29, epochs-1)]:.2f}\")\n",
    "\n",
    "print(f\"Accuracy on test set = {final_test_acc:.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"ADDITIONAL MODEL INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Model Architecture: Single-layer ANN\")\n",
    "print(f\"Input features: {input_size}\")\n",
    "print(f\"Activation function: Sigmoid\")\n",
    "print(f\"Loss function: Binary Cross Entropy\")\n",
    "print(f\"Optimizer: Manual Gradient Descent\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Training epochs: {epochs}\")\n",
    "print(f\"Dataset size: {len(df)} samples\")\n",
    "print(f\"Train/Test split: {len(X_train)}/{len(X_test)}\")\n",
    "print(f\"Device used: {device}\")\n",
    "print()\n",
    "print(f\"Final model parameters:\")\n",
    "print(f\"  Weights: [{model.weights[0,0].item():.4f}, {model.weights[1,0].item():.4f}]\")\n",
    "print(f\"  Bias: {model.bias.item():.4f}\")\n",
    "print()\n",
    "print(f\"Model successfully trained using manual implementation!\")\n",
    "print(f\"No torch.nn or torch.nn.Module was used as required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d7cb9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully demonstrates:\n",
    "\n",
    "### âœ… **Assignment Requirements Met:**\n",
    "1. **Manual Implementation**: Built ANN using only basic PyTorch operations (no `torch.nn` or `torch.nn.Module`)\n",
    "2. **Dataset Generation**: Created binary classification dataset using `sklearn.datasets.make_classification`\n",
    "3. **Model Architecture**: Implemented Y = w^T * x + b with sigmoid activation\n",
    "4. **Loss Function**: Used Binary Cross Entropy loss\n",
    "5. **Manual Optimization**: Implemented gradient descent with manual gradient computation\n",
    "6. **Device Support**: Automatically uses GPU if available, otherwise CPU\n",
    "\n",
    "### ðŸ“Š **Key Results:**\n",
    "- Successfully trained a single-layer neural network for binary classification\n",
    "- Achieved good test accuracy through manual gradient descent\n",
    "- Visualized training progress and decision boundaries\n",
    "- Demonstrated understanding of fundamental neural network concepts\n",
    "\n",
    "### ðŸ§  **Learning Outcomes:**\n",
    "- Understanding of forward propagation in neural networks\n",
    "- Manual computation of gradients for backpropagation\n",
    "- Implementation of activation functions and loss functions\n",
    "- Parameter updates using gradient descent\n",
    "- Data preprocessing and visualization techniques\n",
    "\n",
    "This implementation provides a solid foundation for understanding how neural networks work at a fundamental level before moving to higher-level frameworks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
