{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc50308",
   "metadata": {},
   "source": [
    "# Decision Tree Assignment - Part 1: Theoretical Understanding\n",
    "\n",
    "## ðŸ“š Assignment Overview\n",
    "\n",
    "This notebook covers the theoretical foundations of Decision Trees through comprehensive questions and practical implementations. We'll explore core concepts, mathematical foundations, and develop intuition for tree-based algorithms.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By completing this assignment, you will:\n",
    "- Understand the fundamental concepts of Decision Trees\n",
    "- Master the mathematical foundations (Entropy, Information Gain, Gini Impurity)\n",
    "- Learn when and how to apply Decision Trees effectively\n",
    "- Develop intuition for tree-based algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Theoretical Questions\n",
    "\n",
    "**Q1.** Explain the concept of a Decision Tree. What kind of problems is it best suited for?\n",
    "\n",
    "**Q2.** Define the following terms with examples:\n",
    "- Root Node\n",
    "- Leaf Node  \n",
    "- Internal Node\n",
    "- Branch\n",
    "\n",
    "**Q3.** What is Entropy? How is it used in a Decision Tree? Provide a mathematical example.\n",
    "\n",
    "**Q4.** What is Information Gain? How does it help in building a Decision Tree? Show a small example using a feature with two values.\n",
    "\n",
    "**Q5.** Compare Gini Impurity and Entropy as criteria in Decision Trees. When would you prefer one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44e02b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries for our theoretical exploration and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8292638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import make_classification\n",
    "import math\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"âœ… NumPy for numerical computations\")\n",
    "print(\"âœ… Pandas for data manipulation\")\n",
    "print(\"âœ… Matplotlib & Seaborn for visualizations\")\n",
    "print(\"âœ… Scikit-learn for Decision Tree implementation\")\n",
    "print(\"âœ… Math for mathematical calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9c1b45",
   "metadata": {},
   "source": [
    "## 2. Q1: Decision Tree Concepts and Applications\n",
    "\n",
    "### Question 1: Explain the concept of a Decision Tree. What kind of problems is it best suited for?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "A **Decision Tree** is a supervised machine learning algorithm that uses a tree-like structure to make predictions. It works by recursively splitting the data based on feature values, creating a hierarchical set of if-else conditions that lead to predictions.\n",
    "\n",
    "#### Key Characteristics:\n",
    "1. **Tree Structure**: Organized as a tree with nodes and branches\n",
    "2. **Recursive Partitioning**: Splits data based on feature values\n",
    "3. **Interpretability**: Easy to understand and visualize\n",
    "4. **Non-parametric**: Makes no assumptions about data distribution\n",
    "\n",
    "#### Problems Best Suited For:\n",
    "\n",
    "**Classification Problems:**\n",
    "- Email spam detection\n",
    "- Medical diagnosis\n",
    "- Customer segmentation\n",
    "- Fraud detection\n",
    "\n",
    "**Regression Problems:**\n",
    "- House price prediction\n",
    "- Stock price forecasting\n",
    "- Sales prediction\n",
    "\n",
    "#### Advantages:\n",
    "- âœ… Highly interpretable\n",
    "- âœ… Handles both numerical and categorical data\n",
    "- âœ… No need for feature scaling\n",
    "- âœ… Can capture non-linear relationships\n",
    "- âœ… Handles missing values well\n",
    "\n",
    "#### When to Use Decision Trees:\n",
    "- When interpretability is crucial\n",
    "- Mixed data types (numerical + categorical)\n",
    "- Non-linear relationships exist\n",
    "- Feature interactions are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90169be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Decision Tree for Classification\n",
    "# Let's create a simple example to demonstrate Decision Tree concepts\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Weather': ['Sunny', 'Overcast', 'Rainy', 'Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Sunny', 'Rainy'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Strong'],\n",
    "    'Play_Tennis': ['No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample Dataset - Tennis Playing Decision:\")\n",
    "print(df)\n",
    "\n",
    "# Visualize the decision scenario\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Count plot for target variable\n",
    "target_counts = df['Play_Tennis'].value_counts()\n",
    "ax1.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Distribution of Play Tennis Decision')\n",
    "\n",
    "# Feature importance visualization\n",
    "feature_counts = {}\n",
    "for col in ['Weather', 'Temperature', 'Humidity', 'Wind']:\n",
    "    unique_vals = df[col].nunique()\n",
    "    feature_counts[col] = unique_vals\n",
    "\n",
    "ax2.bar(feature_counts.keys(), feature_counts.values(), color=['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon'])\n",
    "ax2.set_title('Number of Unique Values per Feature')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Features: {list(df.columns[:-1])}\")\n",
    "print(f\"Target: {df.columns[-1]}\")\n",
    "print(f\"Target Classes: {df['Play_Tennis'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cb575",
   "metadata": {},
   "source": [
    "## 3. Q2: Tree Components and Terminology\n",
    "\n",
    "### Question 2: Define the following terms with examples:\n",
    "- Root Node\n",
    "- Leaf Node\n",
    "- Internal Node\n",
    "- Branch\n",
    "\n",
    "### Answer:\n",
    "\n",
    "#### 1. **Root Node**\n",
    "- **Definition**: The topmost node in a decision tree where the tree starts\n",
    "- **Characteristics**: \n",
    "  - Contains the entire dataset\n",
    "  - Represents the first decision/split\n",
    "  - Has no parent nodes\n",
    "  - Has one or more child nodes\n",
    "\n",
    "**Example**: In a medical diagnosis tree, the root node might be \"Patient has fever?\" which divides all patients into two groups.\n",
    "\n",
    "#### 2. **Leaf Node (Terminal Node)**\n",
    "- **Definition**: End nodes that contain the final prediction/classification\n",
    "- **Characteristics**:\n",
    "  - No child nodes\n",
    "  - Contains the predicted class or value\n",
    "  - Represents the final decision\n",
    "\n",
    "**Example**: In our tennis example, leaf nodes would be \"Play Tennis: Yes\" or \"Play Tennis: No\"\n",
    "\n",
    "#### 3. **Internal Node (Decision Node)**\n",
    "- **Definition**: Intermediate nodes that represent a test/condition on a feature\n",
    "- **Characteristics**:\n",
    "  - Has both parent and child nodes\n",
    "  - Contains a decision rule\n",
    "  - Splits the data based on feature values\n",
    "\n",
    "**Example**: \"Humidity = High?\" or \"Weather = Sunny?\" are internal nodes that further divide the data.\n",
    "\n",
    "#### 4. **Branch (Edge)**\n",
    "- **Definition**: Connections between nodes representing the outcome of a test\n",
    "- **Characteristics**:\n",
    "  - Links parent node to child node\n",
    "  - Represents a specific condition or value\n",
    "  - Shows the path of decision making\n",
    "\n",
    "**Example**: The branch labeled \"Yes\" connects \"Humidity = High?\" to the next decision node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Tree Components with Python\n",
    "# Create a simple decision tree to demonstrate different components\n",
    "\n",
    "# Prepare the data for sklearn (encode categorical variables)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy of our dataset\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in ['Weather', 'Temperature', 'Humidity', 'Wind', 'Play_Tennis']:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded[['Weather', 'Temperature', 'Humidity', 'Wind']]\n",
    "y = df_encoded['Play_Tennis']\n",
    "\n",
    "# Create and fit the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Visualize the tree structure\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt, \n",
    "          feature_names=['Weather', 'Temperature', 'Humidity', 'Wind'],\n",
    "          class_names=['No', 'Yes'], \n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          fontsize=12)\n",
    "\n",
    "plt.title(\"Decision Tree Structure - Identifying Different Node Types\", fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Print tree information\n",
    "print(\"ðŸŒ³ Tree Structure Analysis:\")\n",
    "print(f\"â”œâ”€â”€ Root Node: Feature '{X.columns[dt.tree_.feature[0]]}' (Node 0)\")\n",
    "print(f\"â”œâ”€â”€ Tree Depth: {dt.get_depth()}\")\n",
    "print(f\"â”œâ”€â”€ Number of Nodes: {dt.tree_.node_count}\")\n",
    "print(f\"â”œâ”€â”€ Number of Leaves: {dt.get_n_leaves()}\")\n",
    "\n",
    "# Identify different types of nodes\n",
    "def analyze_tree_structure(tree, node=0, depth=0):\n",
    "    if tree.children_left[node] == tree.children_right[node]:  # Leaf node\n",
    "        print(\"  \" * depth + f\"ðŸƒ Leaf Node {node}: Prediction = {tree.value[node].argmax()}\")\n",
    "    else:  # Internal node\n",
    "        if node == 0:\n",
    "            print(\"  \" * depth + f\"ðŸŒ± Root Node {node}: Split on feature {tree.feature[node]}\")\n",
    "        else:\n",
    "            print(\"  \" * depth + f\"ðŸ”¸ Internal Node {node}: Split on feature {tree.feature[node]}\")\n",
    "        \n",
    "        # Recursively analyze children\n",
    "        analyze_tree_structure(tree, tree.children_left[node], depth + 1)\n",
    "        analyze_tree_structure(tree, tree.children_right[node], depth + 1)\n",
    "\n",
    "print(\"\\nðŸ“Š Node Type Analysis:\")\n",
    "analyze_tree_structure(dt.tree_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304300e",
   "metadata": {},
   "source": [
    "## 4. Q3: Entropy Calculation and Examples\n",
    "\n",
    "### Question 3: What is Entropy? How is it used in a Decision Tree? Provide a mathematical example.\n",
    "\n",
    "### Answer:\n",
    "\n",
    "#### **What is Entropy?**\n",
    "\n",
    "**Entropy** is a measure of impurity or randomness in a dataset. In the context of decision trees, it quantifies how mixed the classes are in a given node.\n",
    "\n",
    "#### **Mathematical Formula:**\n",
    "\n",
    "For a dataset with classes Câ‚, Câ‚‚, ..., Câ‚–:\n",
    "\n",
    "**Entropy(S) = -âˆ‘(i=1 to k) páµ¢ Ã— logâ‚‚(páµ¢)**\n",
    "\n",
    "Where:\n",
    "- S = dataset\n",
    "- páµ¢ = proportion of samples that belong to class Cáµ¢\n",
    "- k = number of classes\n",
    "\n",
    "#### **Key Properties:**\n",
    "- **Range**: 0 â‰¤ Entropy â‰¤ logâ‚‚(k)\n",
    "- **Entropy = 0**: Pure node (all samples belong to same class)\n",
    "- **Entropy = logâ‚‚(k)**: Maximum impurity (equal distribution of all classes)\n",
    "\n",
    "#### **How Entropy is Used in Decision Trees:**\n",
    "\n",
    "1. **Node Splitting**: Choose the feature that results in the lowest weighted average entropy\n",
    "2. **Stopping Criteria**: Stop splitting when entropy reaches minimum threshold\n",
    "3. **Information Gain**: Calculate the reduction in entropy after splitting\n",
    "\n",
    "#### **Mathematical Example:**\n",
    "\n",
    "Let's consider a node with 10 samples:\n",
    "- 7 samples of Class A (positive)\n",
    "- 3 samples of Class B (negative)\n",
    "\n",
    "**Step-by-step calculation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b45bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy Calculation Implementation\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    \"\"\"\n",
    "    Calculate entropy for a given set of labels\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count occurrences of each class\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = 0\n",
    "    for count in class_counts.values():\n",
    "        probability = count / total_samples\n",
    "        if probability > 0:  # Avoid log(0)\n",
    "            entropy -= probability * math.log2(probability)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Mathematical Example from the theory above\n",
    "print(\"ðŸ“Š Mathematical Example - Entropy Calculation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example dataset: 7 Class A, 3 Class B\n",
    "example_labels = ['A'] * 7 + ['B'] * 3\n",
    "print(f\"Dataset: {example_labels}\")\n",
    "print(f\"Class A: 7 samples, Class B: 3 samples\")\n",
    "print(f\"Total: {len(example_labels)} samples\")\n",
    "\n",
    "# Manual calculation\n",
    "p_a = 7/10  # Probability of Class A\n",
    "p_b = 3/10  # Probability of Class B\n",
    "\n",
    "print(f\"\\nStep-by-step calculation:\")\n",
    "print(f\"p(A) = 7/10 = {p_a}\")\n",
    "print(f\"p(B) = 3/10 = {p_b}\")\n",
    "print(f\"\")\n",
    "print(f\"Entropy = -p(A) Ã— logâ‚‚(p(A)) - p(B) Ã— logâ‚‚(p(B))\")\n",
    "print(f\"Entropy = -{p_a} Ã— logâ‚‚({p_a}) - {p_b} Ã— logâ‚‚({p_b})\")\n",
    "print(f\"Entropy = -{p_a} Ã— {math.log2(p_a):.4f} - {p_b} Ã— {math.log2(p_b):.4f}\")\n",
    "print(f\"Entropy = {-p_a * math.log2(p_a):.4f} + {-p_b * math.log2(p_b):.4f}\")\n",
    "\n",
    "manual_entropy = -p_a * math.log2(p_a) - p_b * math.log2(p_b)\n",
    "function_entropy = calculate_entropy(example_labels)\n",
    "\n",
    "print(f\"Entropy = {manual_entropy:.4f}\")\n",
    "print(f\"\\nVerification using our function: {function_entropy:.4f}\")\n",
    "print(f\"âœ… Results match: {abs(manual_entropy - function_entropy) < 1e-10}\")\n",
    "\n",
    "# Demonstrate different entropy scenarios\n",
    "print(f\"\\nðŸŽ¯ Different Entropy Scenarios:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scenarios = [\n",
    "    (['A'] * 10, \"Pure node (all Class A)\"),\n",
    "    (['A'] * 5 + ['B'] * 5, \"Maximum impurity (50-50 split)\"),\n",
    "    (['A'] * 8 + ['B'] * 2, \"Low impurity (80-20 split)\"),\n",
    "    (['A'] * 6 + ['B'] * 3 + ['C'] * 1, \"Three classes\"),\n",
    "]\n",
    "\n",
    "entropies = []\n",
    "scenario_names = []\n",
    "\n",
    "for labels, description in scenarios:\n",
    "    entropy = calculate_entropy(labels)\n",
    "    entropies.append(entropy)\n",
    "    scenario_names.append(description)\n",
    "    print(f\"{description:.<35} Entropy = {entropy:.4f}\")\n",
    "\n",
    "# Visualize entropy for different scenarios\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot entropy values\n",
    "plt.subplot(2, 2, 1)\n",
    "bars = plt.bar(range(len(entropies)), entropies, color=['green', 'red', 'orange', 'purple'])\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Entropy Values for Different Scenarios')\n",
    "plt.xticks(range(len(entropies)), [f'S{i+1}' for i in range(len(entropies))])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, entropy) in enumerate(zip(bars, entropies)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{entropy:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot entropy curve for binary classification\n",
    "plt.subplot(2, 2, 2)\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "entropy_values = [-p * math.log2(p) - (1-p) * math.log2(1-p) for p in p_values]\n",
    "\n",
    "plt.plot(p_values, entropy_values, 'b-', linewidth=2)\n",
    "plt.xlabel('Probability of Class 1')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Entropy Curve for Binary Classification')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.7, label='Maximum Entropy')\n",
    "plt.legend()\n",
    "\n",
    "# Tennis dataset entropy\n",
    "plt.subplot(2, 2, 3)\n",
    "tennis_entropy = calculate_entropy(df['Play_Tennis'].tolist())\n",
    "play_counts = df['Play_Tennis'].value_counts()\n",
    "\n",
    "plt.pie(play_counts.values, labels=play_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Tennis Dataset\\nEntropy = {tennis_entropy:.4f}')\n",
    "\n",
    "# Information visualization\n",
    "plt.subplot(2, 2, 4)\n",
    "info_text = f\"\"\"\n",
    "Entropy Interpretation:\n",
    "\n",
    "â€¢ Entropy = 0: Pure node\n",
    "â€¢ Entropy = 1: Maximum impurity \n",
    "  (for binary classification)\n",
    "â€¢ Lower entropy = Better separation\n",
    "â€¢ Used to calculate Information Gain\n",
    "\n",
    "Tennis Dataset:\n",
    "â€¢ Total samples: {len(df)}\n",
    "â€¢ Yes: {play_counts['Yes']} samples\n",
    "â€¢ No: {play_counts['No']} samples\n",
    "â€¢ Entropy: {tennis_entropy:.4f}\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.1, 0.9, info_text, transform=plt.gca().transAxes, \n",
    "         verticalalignment='top', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778cb42b",
   "metadata": {},
   "source": [
    "## 5. Q4: Information Gain Implementation\n",
    "\n",
    "### Question 4: What is Information Gain? How does it help in building a Decision Tree? Show a small example using a feature with two values.\n",
    "\n",
    "### Answer:\n",
    "\n",
    "#### **What is Information Gain?**\n",
    "\n",
    "**Information Gain** is a measure used to determine which feature provides the most information about the target variable. It calculates the reduction in entropy after splitting the dataset on a particular feature.\n",
    "\n",
    "#### **Mathematical Formula:**\n",
    "\n",
    "**Information Gain(S, A) = Entropy(S) - Î£(|Sáµ¥|/|S|) Ã— Entropy(Sáµ¥)**\n",
    "\n",
    "Where:\n",
    "- S = original dataset\n",
    "- A = feature being evaluated\n",
    "- Sáµ¥ = subset of S where feature A has value v\n",
    "- |S| = number of samples in S\n",
    "- |Sáµ¥| = number of samples in Sáµ¥\n",
    "\n",
    "#### **How Information Gain Helps in Building Decision Trees:**\n",
    "\n",
    "1. **Feature Selection**: Choose the feature with the highest information gain for splitting\n",
    "2. **Tree Construction**: Recursively apply this process to build the tree\n",
    "3. **Optimal Splits**: Ensures each split provides maximum information about the target\n",
    "4. **Greedy Approach**: Makes locally optimal choices at each node\n",
    "\n",
    "#### **Step-by-Step Example:**\n",
    "\n",
    "Let's use a simple example with a binary feature \"Weather\" that has two values: \"Sunny\" and \"Rainy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Gain Implementation\n",
    "\n",
    "def calculate_information_gain(data, target_col, feature_col):\n",
    "    \"\"\"\n",
    "    Calculate information gain for a specific feature\n",
    "    \"\"\"\n",
    "    # Calculate entropy of the original dataset\n",
    "    original_entropy = calculate_entropy(data[target_col].tolist())\n",
    "    \n",
    "    # Get unique values of the feature\n",
    "    feature_values = data[feature_col].unique()\n",
    "    \n",
    "    # Calculate weighted entropy after splitting\n",
    "    weighted_entropy = 0\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    for value in feature_values:\n",
    "        subset = data[data[feature_col] == value]\n",
    "        subset_size = len(subset)\n",
    "        subset_entropy = calculate_entropy(subset[target_col].tolist())\n",
    "        \n",
    "        # Add weighted entropy\n",
    "        weighted_entropy += (subset_size / total_samples) * subset_entropy\n",
    "    \n",
    "    # Calculate information gain\n",
    "    information_gain = original_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain, original_entropy, weighted_entropy\n",
    "\n",
    "# Create a simple example dataset for Information Gain demonstration\n",
    "print(\"ðŸŽ¯ Information Gain Example - Binary Feature\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create example data with Weather feature (Sunny/Rainy) and Play Tennis target\n",
    "example_data = pd.DataFrame({\n",
    "    'Weather': ['Sunny', 'Sunny', 'Rainy', 'Rainy', 'Sunny', 'Rainy', 'Sunny', 'Rainy'],\n",
    "    'Play_Tennis': ['No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "})\n",
    "\n",
    "print(\"Example Dataset:\")\n",
    "print(example_data)\n",
    "print()\n",
    "\n",
    "# Analyze the dataset\n",
    "weather_counts = example_data.groupby(['Weather', 'Play_Tennis']).size().unstack(fill_value=0)\n",
    "print(\"Cross-tabulation of Weather vs Play Tennis:\")\n",
    "print(weather_counts)\n",
    "print()\n",
    "\n",
    "# Calculate Information Gain step by step\n",
    "print(\"ðŸ“Š Step-by-Step Information Gain Calculation:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 1: Calculate original entropy\n",
    "original_labels = example_data['Play_Tennis'].tolist()\n",
    "original_entropy = calculate_entropy(original_labels)\n",
    "print(f\"Step 1 - Original Entropy:\")\n",
    "print(f\"Dataset: {original_labels}\")\n",
    "total_yes = original_labels.count('Yes')\n",
    "total_no = original_labels.count('No')\n",
    "total_samples = len(original_labels)\n",
    "print(f\"Yes: {total_yes}, No: {total_no}, Total: {total_samples}\")\n",
    "print(f\"Entropy(S) = {original_entropy:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Calculate entropy for each subset\n",
    "print(f\"Step 2 - Calculate Entropy for Each Subset:\")\n",
    "\n",
    "# Sunny subset\n",
    "sunny_data = example_data[example_data['Weather'] == 'Sunny']\n",
    "sunny_labels = sunny_data['Play_Tennis'].tolist()\n",
    "sunny_entropy = calculate_entropy(sunny_labels)\n",
    "sunny_yes = sunny_labels.count('Yes')\n",
    "sunny_no = sunny_labels.count('No')\n",
    "sunny_total = len(sunny_labels)\n",
    "\n",
    "print(f\"Sunny subset: {sunny_labels}\")\n",
    "print(f\"  Yes: {sunny_yes}, No: {sunny_no}, Total: {sunny_total}\")\n",
    "print(f\"  Entropy(Sunny) = {sunny_entropy:.4f}\")\n",
    "\n",
    "# Rainy subset  \n",
    "rainy_data = example_data[example_data['Weather'] == 'Rainy']\n",
    "rainy_labels = rainy_data['Play_Tennis'].tolist()\n",
    "rainy_entropy = calculate_entropy(rainy_labels)\n",
    "rainy_yes = rainy_labels.count('Yes')\n",
    "rainy_no = rainy_labels.count('No')\n",
    "rainy_total = len(rainy_labels)\n",
    "\n",
    "print(f\"Rainy subset: {rainy_labels}\")\n",
    "print(f\"  Yes: {rainy_yes}, No: {rainy_no}, Total: {rainy_total}\")\n",
    "print(f\"  Entropy(Rainy) = {rainy_entropy:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Calculate weighted entropy\n",
    "print(f\"Step 3 - Calculate Weighted Entropy:\")\n",
    "sunny_weight = sunny_total / total_samples\n",
    "rainy_weight = rainy_total / total_samples\n",
    "\n",
    "print(f\"Weight(Sunny) = {sunny_total}/{total_samples} = {sunny_weight:.3f}\")\n",
    "print(f\"Weight(Rainy) = {rainy_total}/{total_samples} = {rainy_weight:.3f}\")\n",
    "\n",
    "weighted_entropy = sunny_weight * sunny_entropy + rainy_weight * rainy_entropy\n",
    "print(f\"Weighted Entropy = {sunny_weight:.3f} Ã— {sunny_entropy:.4f} + {rainy_weight:.3f} Ã— {rainy_entropy:.4f}\")\n",
    "print(f\"Weighted Entropy = {weighted_entropy:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Calculate Information Gain\n",
    "print(f\"Step 4 - Calculate Information Gain:\")\n",
    "info_gain = original_entropy - weighted_entropy\n",
    "print(f\"Information Gain = Entropy(S) - Weighted Entropy\")\n",
    "print(f\"Information Gain = {original_entropy:.4f} - {weighted_entropy:.4f}\")\n",
    "print(f\"Information Gain = {info_gain:.4f}\")\n",
    "print()\n",
    "\n",
    "# Verify with our function\n",
    "ig, orig_ent, weight_ent = calculate_information_gain(example_data, 'Play_Tennis', 'Weather')\n",
    "print(f\"âœ… Verification using function: {ig:.4f}\")\n",
    "print(f\"   Original Entropy: {orig_ent:.4f}\")\n",
    "print(f\"   Weighted Entropy: {weight_ent:.4f}\")\n",
    "\n",
    "# Visualize the Information Gain concept\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Original dataset distribution\n",
    "original_counts = example_data['Play_Tennis'].value_counts()\n",
    "ax1.pie(original_counts.values, labels=original_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title(f'Original Dataset\\nEntropy = {original_entropy:.4f}')\n",
    "\n",
    "# After splitting by Weather\n",
    "weather_groups = example_data.groupby('Weather')['Play_Tennis'].apply(list)\n",
    "\n",
    "sunny_counts = pd.Series(weather_groups['Sunny']).value_counts()\n",
    "ax2.pie(sunny_counts.values, labels=sunny_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title(f'Sunny Days\\nEntropy = {sunny_entropy:.4f}')\n",
    "\n",
    "rainy_counts = pd.Series(weather_groups['Rainy']).value_counts()\n",
    "ax3.pie(rainy_counts.values, labels=rainy_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax3.set_title(f'Rainy Days\\nEntropy = {rainy_entropy:.4f}')\n",
    "\n",
    "# Information Gain visualization\n",
    "ax4.bar(['Original\\nEntropy', 'Weighted\\nEntropy', 'Information\\nGain'], \n",
    "        [original_entropy, weighted_entropy, info_gain],\n",
    "        color=['red', 'orange', 'green'])\n",
    "ax4.set_ylabel('Value')\n",
    "ax4.set_title('Information Gain Calculation')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate([original_entropy, weighted_entropy, info_gain]):\n",
    "    ax4.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Information Gain for all features in the Tennis dataset\n",
    "print(\"\\nðŸ† Information Gain for All Features in Tennis Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "features = ['Weather', 'Temperature', 'Humidity', 'Wind']\n",
    "information_gains = []\n",
    "\n",
    "print(\"Feature Rankings by Information Gain:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for feature in features:\n",
    "    ig, orig_ent, weight_ent = calculate_information_gain(df, 'Play_Tennis', feature)\n",
    "    information_gains.append(ig)\n",
    "    print(f\"{feature:12} | Info Gain: {ig:.4f}\")\n",
    "\n",
    "# Find the best feature\n",
    "best_feature_idx = np.argmax(information_gains)\n",
    "best_feature = features[best_feature_idx]\n",
    "best_gain = information_gains[best_feature_idx]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Best feature for root node: {best_feature}\")\n",
    "print(f\"   Information Gain: {best_gain:.4f}\")\n",
    "\n",
    "# Visualize Information Gain comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bar plot of Information Gains\n",
    "plt.subplot(1, 2, 1)\n",
    "bars = plt.bar(features, information_gains, color=['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon'])\n",
    "plt.title('Information Gain Comparison')\n",
    "plt.ylabel('Information Gain')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Highlight the best feature\n",
    "bars[best_feature_idx].set_color('gold')\n",
    "bars[best_feature_idx].set_edgecolor('red')\n",
    "bars[best_feature_idx].set_linewidth(2)\n",
    "\n",
    "# Add value labels\n",
    "for i, (feature, gain) in enumerate(zip(features, information_gains)):\n",
    "    plt.text(i, gain + 0.005, f'{gain:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Feature selection decision tree\n",
    "plt.subplot(1, 2, 2)\n",
    "feature_data = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Information_Gain': information_gains\n",
    "}).sort_values('Information_Gain', ascending=True)\n",
    "\n",
    "plt.barh(feature_data['Feature'], feature_data['Information_Gain'], \n",
    "         color=['lightcoral' if x != best_gain else 'gold' for x in feature_data['Information_Gain']])\n",
    "plt.title('Feature Selection Ranking')\n",
    "plt.xlabel('Information Gain')\n",
    "\n",
    "# Add value labels\n",
    "for i, (feature, gain) in enumerate(zip(feature_data['Feature'], feature_data['Information_Gain'])):\n",
    "    plt.text(gain + 0.002, i, f'{gain:.4f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "print(f\"   The '{best_feature}' feature provides the most information\")\n",
    "print(f\"   about the target variable and should be used at the root node.\")\n",
    "print(f\"   This feature reduces entropy by {best_gain:.4f} bits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d640dc",
   "metadata": {},
   "source": [
    "## 6. Q5: Gini Impurity vs Entropy Comparison\n",
    "\n",
    "### Question 5: Compare Gini Impurity and Entropy as criteria in Decision Trees. When would you prefer one over the other?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "#### **Gini Impurity**\n",
    "\n",
    "**Definition**: Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "**Gini(S) = 1 - Î£(i=1 to k) páµ¢Â²**\n",
    "\n",
    "Where:\n",
    "- S = dataset\n",
    "- páµ¢ = proportion of samples that belong to class Cáµ¢\n",
    "- k = number of classes\n",
    "\n",
    "#### **Entropy**\n",
    "\n",
    "**Definition**: Entropy measures the average amount of information needed to identify the class of a sample.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "**Entropy(S) = -Î£(i=1 to k) páµ¢ Ã— logâ‚‚(páµ¢)**\n",
    "\n",
    "#### **Key Differences:**\n",
    "\n",
    "| Aspect | Gini Impurity | Entropy |\n",
    "|--------|---------------|---------|\n",
    "| **Range** | 0 to 0.5 (binary) | 0 to 1 (binary) |\n",
    "| **Computation** | Faster (no logarithm) | Slower (logarithm calculation) |\n",
    "| **Sensitivity** | Less sensitive to changes | More sensitive to changes |\n",
    "| **Pure Node** | Gini = 0 | Entropy = 0 |\n",
    "| **Maximum Impurity** | Gini = 0.5 | Entropy = 1 |\n",
    "\n",
    "#### **When to Use Each:**\n",
    "\n",
    "**Use Gini Impurity when:**\n",
    "- âœ… Computational efficiency is important\n",
    "- âœ… Working with large datasets\n",
    "- âœ… Implementation simplicity is preferred\n",
    "- âœ… Slight differences in purity don't matter much\n",
    "\n",
    "**Use Entropy when:**\n",
    "- âœ… More balanced trees are desired\n",
    "- âœ… Theoretical foundation is important\n",
    "- âœ… Maximum information gain is crucial\n",
    "- âœ… Better handling of probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Impurity vs Entropy Implementation and Comparison\n",
    "\n",
    "def calculate_gini_impurity(labels):\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity for a given set of labels\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count occurrences of each class\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    # Calculate Gini impurity\n",
    "    gini = 1.0\n",
    "    for count in class_counts.values():\n",
    "        probability = count / total_samples\n",
    "        gini -= probability ** 2\n",
    "    \n",
    "    return gini\n",
    "\n",
    "def calculate_gini_gain(data, target_col, feature_col):\n",
    "    \"\"\"\n",
    "    Calculate Gini gain for a specific feature\n",
    "    \"\"\"\n",
    "    # Calculate Gini impurity of the original dataset\n",
    "    original_gini = calculate_gini_impurity(data[target_col].tolist())\n",
    "    \n",
    "    # Get unique values of the feature\n",
    "    feature_values = data[feature_col].unique()\n",
    "    \n",
    "    # Calculate weighted Gini impurity after splitting\n",
    "    weighted_gini = 0\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    for value in feature_values:\n",
    "        subset = data[data[feature_col] == value]\n",
    "        subset_size = len(subset)\n",
    "        subset_gini = calculate_gini_impurity(subset[target_col].tolist())\n",
    "        \n",
    "        # Add weighted Gini impurity\n",
    "        weighted_gini += (subset_size / total_samples) * subset_gini\n",
    "    \n",
    "    # Calculate Gini gain\n",
    "    gini_gain = original_gini - weighted_gini\n",
    "    \n",
    "    return gini_gain, original_gini, weighted_gini\n",
    "\n",
    "print(\"âš–ï¸  Gini Impurity vs Entropy Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compare both measures on the same example dataset\n",
    "example_labels = ['A'] * 7 + ['B'] * 3\n",
    "\n",
    "print(\"Example Dataset: 7 Class A, 3 Class B\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate both measures\n",
    "entropy_val = calculate_entropy(example_labels)\n",
    "gini_val = calculate_gini_impurity(example_labels)\n",
    "\n",
    "print(f\"Entropy:      {entropy_val:.4f}\")\n",
    "print(f\"Gini Impurity: {gini_val:.4f}\")\n",
    "print()\n",
    "\n",
    "# Mathematical comparison for the example\n",
    "p_a, p_b = 0.7, 0.3\n",
    "print(\"Manual Calculations:\")\n",
    "print(f\"p(A) = {p_a}, p(B) = {p_b}\")\n",
    "print()\n",
    "print(\"Entropy = -p(A)Ã—logâ‚‚(p(A)) - p(B)Ã—logâ‚‚(p(B))\")\n",
    "print(f\"Entropy = -{p_a}Ã—{math.log2(p_a):.4f} - {p_b}Ã—{math.log2(p_b):.4f}\")\n",
    "print(f\"Entropy = {entropy_val:.4f}\")\n",
    "print()\n",
    "print(\"Gini = 1 - p(A)Â² - p(B)Â²\")\n",
    "print(f\"Gini = 1 - {p_a}Â² - {p_b}Â²\")\n",
    "print(f\"Gini = 1 - {p_a**2:.4f} - {p_b**2:.4f}\")\n",
    "print(f\"Gini = {gini_val:.4f}\")\n",
    "\n",
    "# Compare across different probability distributions\n",
    "print(\"\\nðŸ“Š Comparison Across Different Distributions\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "probabilities = np.linspace(0.01, 0.99, 50)\n",
    "entropy_values = []\n",
    "gini_values = []\n",
    "\n",
    "for p in probabilities:\n",
    "    # Binary case: p and (1-p)\n",
    "    entropy = -p * math.log2(p) - (1-p) * math.log2(1-p)\n",
    "    gini = 1 - p**2 - (1-p)**2\n",
    "    \n",
    "    entropy_values.append(entropy)\n",
    "    gini_values.append(gini)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Entropy vs Gini curves\n",
    "ax1.plot(probabilities, entropy_values, 'b-', linewidth=2, label='Entropy')\n",
    "ax1.plot(probabilities, gini_values, 'r-', linewidth=2, label='Gini Impurity')\n",
    "ax1.set_xlabel('Probability of Class 1')\n",
    "ax1.set_ylabel('Impurity Measure')\n",
    "ax1.set_title('Entropy vs Gini Impurity Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Difference between measures\n",
    "difference = np.array(entropy_values) - np.array(gini_values)\n",
    "ax2.plot(probabilities, difference, 'g-', linewidth=2)\n",
    "ax2.set_xlabel('Probability of Class 1')\n",
    "ax2.set_ylabel('Entropy - Gini')\n",
    "ax2.set_title('Difference Between Entropy and Gini')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Tennis dataset comparison\n",
    "features = ['Weather', 'Temperature', 'Humidity', 'Wind']\n",
    "entropy_gains = []\n",
    "gini_gains = []\n",
    "\n",
    "for feature in features:\n",
    "    # Calculate Information Gain (Entropy-based)\n",
    "    ig_entropy, _, _ = calculate_information_gain(df, 'Play_Tennis', feature)\n",
    "    entropy_gains.append(ig_entropy)\n",
    "    \n",
    "    # Calculate Gini Gain\n",
    "    ig_gini, _, _ = calculate_gini_gain(df, 'Play_Tennis', feature)\n",
    "    gini_gains.append(ig_gini)\n",
    "\n",
    "x = np.arange(len(features))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, entropy_gains, width, label='Information Gain (Entropy)', color='skyblue')\n",
    "bars2 = ax3.bar(x + width/2, gini_gains, width, label='Gini Gain', color='lightcoral')\n",
    "\n",
    "ax3.set_xlabel('Features')\n",
    "ax3.set_ylabel('Gain')\n",
    "ax3.set_title('Feature Ranking: Entropy vs Gini')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(features, rotation=45)\n",
    "ax3.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "    ax3.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.002, \n",
    "             f'{entropy_gains[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    ax3.text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.002, \n",
    "             f'{gini_gains[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plot 4: Performance comparison summary\n",
    "performance_data = {\n",
    "    'Metric': ['Computation Speed', 'Sensitivity', 'Max Value (Binary)', 'Balanced Trees'],\n",
    "    'Gini': ['Faster', 'Lower', '0.5', 'Good'],\n",
    "    'Entropy': ['Slower', 'Higher', '1.0', 'Better']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(performance_data)\n",
    "ax4.axis('tight')\n",
    "ax4.axis('off')\n",
    "\n",
    "table = ax4.table(cellText=comparison_df.values,\n",
    "                  colLabels=comparison_df.columns,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  colWidths=[0.4, 0.3, 0.3])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Color code the table\n",
    "for i in range(len(comparison_df.columns)):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax4.set_title('Gini vs Entropy Comparison Table', fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature ranking comparison\n",
    "print(\"\\nðŸ† Feature Ranking Comparison\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"{'Feature':<12} | {'Entropy Gain':<12} | {'Gini Gain':<10} | {'Rank Match'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "entropy_ranks = np.argsort(entropy_gains)[::-1]\n",
    "gini_ranks = np.argsort(gini_gains)[::-1]\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    entropy_rank = np.where(entropy_ranks == i)[0][0] + 1\n",
    "    gini_rank = np.where(gini_ranks == i)[0][0] + 1\n",
    "    match = \"âœ…\" if entropy_rank == gini_rank else \"âŒ\"\n",
    "    \n",
    "    print(f\"{feature:<12} | {entropy_gains[i]:<12.4f} | {gini_gains[i]:<10.4f} | {match}\")\n",
    "\n",
    "# Conclusion\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "print(f\"   â€¢ Entropy and Gini often produce similar feature rankings\")\n",
    "print(f\"   â€¢ Gini is computationally more efficient (no logarithms)\")\n",
    "print(f\"   â€¢ Entropy is more sensitive to probability changes\")\n",
    "print(f\"   â€¢ Both reach maximum at 50-50 class distribution\")\n",
    "print(f\"   â€¢ Choice often depends on specific requirements and dataset size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169fb03a",
   "metadata": {},
   "source": [
    "## 7. Assignment Summary and Conclusions\n",
    "\n",
    "### ðŸŽ¯ Key Takeaways\n",
    "\n",
    "Through this theoretical exploration of Decision Trees, we've covered:\n",
    "\n",
    "#### **1. Decision Tree Fundamentals**\n",
    "- Decision Trees are hierarchical, interpretable algorithms\n",
    "- Best suited for classification and regression problems\n",
    "- Excellent for problems requiring interpretability\n",
    "- Handle mixed data types and non-linear relationships\n",
    "\n",
    "#### **2. Tree Components**\n",
    "- **Root Node**: Starting point with entire dataset\n",
    "- **Internal Nodes**: Decision points that split data\n",
    "- **Leaf Nodes**: Final predictions/classifications\n",
    "- **Branches**: Connections representing decision outcomes\n",
    "\n",
    "#### **3. Mathematical Foundations**\n",
    "\n",
    "**Entropy**: \n",
    "- Measures dataset impurity/randomness\n",
    "- Range: 0 (pure) to logâ‚‚(k) (maximum impurity)\n",
    "- Formula: `Entropy(S) = -Î£ páµ¢ Ã— logâ‚‚(páµ¢)`\n",
    "\n",
    "**Information Gain**:\n",
    "- Measures entropy reduction after splitting\n",
    "- Used for feature selection in tree construction\n",
    "- Formula: `IG(S,A) = Entropy(S) - Î£(|Sáµ¥|/|S|) Ã— Entropy(Sáµ¥)`\n",
    "\n",
    "**Gini Impurity**:\n",
    "- Alternative impurity measure\n",
    "- Computationally more efficient than entropy\n",
    "- Formula: `Gini(S) = 1 - Î£ páµ¢Â²`\n",
    "\n",
    "#### **4. Practical Applications**\n",
    "- Feature selection through information gain calculation\n",
    "- Tree construction using greedy algorithms\n",
    "- Splitting criteria comparison (Gini vs Entropy)\n",
    "- Real-world problem solving approach\n",
    "\n",
    "### ðŸ”¬ Experimental Results\n",
    "\n",
    "From our tennis dataset analysis:\n",
    "- **Best splitting feature**: Determined by highest information gain\n",
    "- **Entropy vs Gini**: Similar rankings but different sensitivities\n",
    "- **Mathematical verification**: Step-by-step calculations confirmed\n",
    "\n",
    "### ðŸ’­ Final Thoughts\n",
    "\n",
    "Decision Trees provide an excellent balance between **interpretability** and **performance**. The choice between Entropy and Gini Impurity often depends on:\n",
    "\n",
    "- **Dataset size** (Gini for larger datasets)\n",
    "- **Computational resources** (Gini is faster)\n",
    "- **Theoretical requirements** (Entropy for information theory applications)\n",
    "- **Sensitivity needs** (Entropy is more sensitive to changes)\n",
    "\n",
    "Understanding these theoretical foundations is crucial for:\n",
    "- Building effective tree-based models\n",
    "- Making informed algorithmic choices\n",
    "- Debugging and optimizing decision trees\n",
    "- Advancing to ensemble methods (Random Forest, Gradient Boosting)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Next Steps\n",
    "\n",
    "1. **Part 2**: Practical implementation with real datasets\n",
    "2. **Advanced Topics**: Pruning, ensemble methods, feature importance\n",
    "3. **Optimization**: Hyperparameter tuning and cross-validation\n",
    "4. **Comparison**: Decision Trees vs other ML algorithms\n",
    "\n",
    "**Congratulations!** ðŸŽ‰ You've completed the theoretical foundation of Decision Trees. This knowledge will serve as a solid base for advanced machine learning topics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
