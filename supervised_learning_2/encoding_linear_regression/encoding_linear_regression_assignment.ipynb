{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf75c26",
   "metadata": {},
   "source": [
    "# 🏠 Encoding & Linear Regression Assignment\n",
    "## Real Estate Price Prediction Analysis\n",
    "\n",
    "### 🎯 **Assignment Objectives:**\n",
    "1. **Practice encoding categorical variables** using Label and One-Hot encoding\n",
    "2. **Implement simple and multiple linear regression models**\n",
    "3. **Evaluate model performance** using comprehensive metrics\n",
    "4. **Analyze real-world housing data** for price prediction\n",
    "\n",
    "### 📋 **Assignment Structure:**\n",
    "- **Part A**: Encoding Categorical Variables\n",
    "- **Part B**: Simple Linear Regression\n",
    "- **Part C**: Regression Evaluation Metrics\n",
    "- **Part D**: Multiple Linear Regression\n",
    "- **Part E**: Conceptual Discussion\n",
    "\n",
    "---\n",
    "\n",
    "**Let's explore how different encoding techniques and regression models can help predict house prices!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f21b56",
   "metadata": {},
   "source": [
    "# 📦 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, \n",
    "    r2_score, explained_variance_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configure plotting settings\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"🏠 ENCODING & LINEAR REGRESSION ASSIGNMENT\")\n",
    "print(\"=\"*50)\n",
    "print(\"📚 All libraries imported successfully!\")\n",
    "print(\"🎯 Ready to analyze real estate data!\")\n",
    "print(\"🔬 Let's explore encoding and regression techniques!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f0a33",
   "metadata": {},
   "source": [
    "# 📊 Part A: Encoding Categorical Variables\n",
    "\n",
    "We'll start by loading the dataset and exploring different encoding techniques for categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab42a6",
   "metadata": {},
   "source": [
    "## A.1: Load Dataset and Display First 5 Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"🏠 LOADING REAL ESTATE DATASET\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load the dataset (assuming it's a test dataset similar to Ames Housing)\n",
    "df = pd.read_csv('test_df (1).csv', index_col=0)\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully!\")\n",
    "print(f\"📊 Dataset shape: {df.shape}\")\n",
    "print(f\"📋 Number of features: {df.shape[1]}\")\n",
    "print(f\"🏘️ Number of properties: {df.shape[0]}\")\n",
    "\n",
    "print(f\"\\n🔍 FIRST 5 ROWS OF THE DATASET\")\n",
    "print(\"-\" * 40)\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\n📋 DATASET INFORMATION\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Dataset Info:\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Data types distribution:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbf08b",
   "metadata": {},
   "source": [
    "## A.2: Identify Categorical Columns and Their Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "print(\"🔍 IDENTIFYING CATEGORICAL COLUMNS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get categorical columns (object type and some specific numeric codes)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Also consider MSSubClass as categorical (it's a building class code)\n",
    "if 'MSSubClass' in df.columns:\n",
    "    categorical_cols.append('MSSubClass')\n",
    "\n",
    "print(f\"📊 Found {len(categorical_cols)} categorical columns:\")\n",
    "print(f\"   {categorical_cols}\")\n",
    "\n",
    "print(f\"\\n🔢 CATEGORICAL COLUMNS ANALYSIS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "categorical_analysis = []\n",
    "for col in categorical_cols[:10]:  # Show first 10 to avoid overwhelming output\n",
    "    unique_values = df[col].unique()\n",
    "    num_unique = len(unique_values)\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    \n",
    "    categorical_analysis.append({\n",
    "        'Column': col,\n",
    "        'Unique_Count': num_unique,\n",
    "        'Missing_Values': missing_count,\n",
    "        'Sample_Values': list(unique_values[:5])  # Show first 5 unique values\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n📂 {col}:\")\n",
    "    print(f\"   Unique values: {num_unique}\")\n",
    "    print(f\"   Missing values: {missing_count}\")\n",
    "    print(f\"   Sample values: {list(unique_values[:5])}\")\n",
    "    if num_unique <= 10:  # Show all values if ≤ 10\n",
    "        print(f\"   All values: {list(unique_values)}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "categorical_summary_df = pd.DataFrame(categorical_analysis)\n",
    "print(f\"\\n📊 CATEGORICAL COLUMNS SUMMARY TABLE\")\n",
    "print(\"-\" * 40)\n",
    "display(categorical_summary_df)\n",
    "\n",
    "print(f\"\\n✅ Categorical analysis completed!\")\n",
    "print(f\"📋 Ready for encoding techniques demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81ec22",
   "metadata": {},
   "source": [
    "## A.3: Apply Label Encoding to Neighborhood Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding to Neighborhood column\n",
    "print(\"🏷️ APPLYING LABEL ENCODING TO NEIGHBORHOOD\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Check if Neighborhood column exists\n",
    "if 'Neighborhood' in df.columns:\n",
    "    target_column = 'Neighborhood'\n",
    "else:\n",
    "    # Use the first categorical column if Neighborhood doesn't exist\n",
    "    target_column = categorical_cols[0]\n",
    "    print(f\"⚠️ Neighborhood not found, using '{target_column}' instead\")\n",
    "\n",
    "print(f\"🎯 Target column for encoding: {target_column}\")\n",
    "\n",
    "# Display original values\n",
    "print(f\"\\n📊 ORIGINAL VALUES ANALYSIS\")\n",
    "print(\"-\" * 30)\n",
    "original_values = df[target_column].value_counts()\n",
    "print(f\"Unique values in {target_column}: {df[target_column].nunique()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "print(original_values.head(10))\n",
    "\n",
    "# Apply Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Handle missing values by filling with 'Unknown' first\n",
    "df_encoded = df.copy()\n",
    "df_encoded[target_column] = df_encoded[target_column].fillna('Unknown')\n",
    "\n",
    "# Apply label encoding\n",
    "df_encoded[f'{target_column}_LabelEncoded'] = label_encoder.fit_transform(df_encoded[target_column])\n",
    "\n",
    "print(f\"\\n🏷️ LABEL ENCODING RESULTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create mapping dictionary\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(f\"Label encoding mapping:\")\n",
    "for original, encoded in sorted(label_mapping.items()):\n",
    "    count = (df_encoded[target_column] == original).sum()\n",
    "    print(f\"  '{original}' → {encoded} (Count: {count})\")\n",
    "\n",
    "# Display sample of transformed values\n",
    "print(f\"\\n📋 SAMPLE OF TRANSFORMED VALUES\")\n",
    "print(\"-\" * 35)\n",
    "comparison_sample = df_encoded[[target_column, f'{target_column}_LabelEncoded']].head(10)\n",
    "display(comparison_sample)\n",
    "\n",
    "print(f\"\\n✅ Label encoding completed successfully!\")\n",
    "print(f\"🔢 {target_column} values converted to numeric codes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8d4c8",
   "metadata": {},
   "source": [
    "## A.4: Apply One-Hot Encoding and Compare with Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-Hot Encoding to the same column\n",
    "print(\"🔥 APPLYING ONE-HOT ENCODING\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Apply One-Hot Encoding using pandas get_dummies\n",
    "onehot_encoded = pd.get_dummies(df_encoded[target_column], prefix=f'{target_column}_OneHot')\n",
    "\n",
    "# Combine with original dataframe\n",
    "df_with_onehot = pd.concat([df_encoded, onehot_encoded], axis=1)\n",
    "\n",
    "print(f\"✅ One-Hot encoding applied successfully!\")\n",
    "print(f\"📊 Created {onehot_encoded.shape[1]} binary columns\")\n",
    "\n",
    "print(f\"\\n📋 ONE-HOT ENCODED COLUMNS\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"New columns created: {list(onehot_encoded.columns)}\")\n",
    "\n",
    "print(f\"\\n🔍 SAMPLE OF ONE-HOT ENCODED DATA\")\n",
    "print(\"-\" * 35)\n",
    "# Show original column plus first 5 one-hot columns\n",
    "sample_cols = [target_column] + list(onehot_encoded.columns[:5])\n",
    "display(df_with_onehot[sample_cols].head(8))\n",
    "\n",
    "print(f\"\\n⚖️ COMPARISON: LABEL ENCODING vs ONE-HOT ENCODING\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Dataset shape comparison\n",
    "original_shape = df.shape\n",
    "label_encoded_shape = df_encoded.shape\n",
    "onehot_shape = df_with_onehot.shape\n",
    "\n",
    "print(f\"📊 DATASET SHAPE COMPARISON:\")\n",
    "print(f\"   Original dataset: {original_shape}\")\n",
    "print(f\"   With Label Encoding: {label_encoded_shape}\")\n",
    "print(f\"   With One-Hot Encoding: {onehot_shape}\")\n",
    "print(f\"   Shape increase from One-Hot: +{onehot_shape[1] - original_shape[1]} columns\")\n",
    "\n",
    "print(f\"\\n🔢 ENCODING TECHNIQUES COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Number of new columns',\n",
    "        'Data type of encoded columns',\n",
    "        'Memory usage increase',\n",
    "        'Interpretability',\n",
    "        'Ordinality preserved',\n",
    "        'Distance relationships'\n",
    "    ],\n",
    "    'Label Encoding': [\n",
    "        '1 column',\n",
    "        'Integer',\n",
    "        'Minimal',\n",
    "        'Harder (arbitrary numbers)',\n",
    "        'No (unless naturally ordered)',\n",
    "        'Implies false ordering'\n",
    "    ],\n",
    "    'One-Hot Encoding': [\n",
    "        f'{onehot_encoded.shape[1]} columns',\n",
    "        'Binary (0/1)',\n",
    "        'Significant (+{} columns)'.format(onehot_encoded.shape[1] - 1),\n",
    "        'Excellent (clear meaning)',\n",
    "        'No ordinality assumed',\n",
    "        'Equal distance between categories'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# Memory usage comparison\n",
    "original_memory = df[target_column].memory_usage(deep=True)\n",
    "label_encoded_memory = df_encoded[f'{target_column}_LabelEncoded'].memory_usage(deep=True)\n",
    "onehot_memory = onehot_encoded.memory_usage(deep=True).sum()\n",
    "\n",
    "print(f\"\\n💾 MEMORY USAGE COMPARISON:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"   Original column: {original_memory:,} bytes\")\n",
    "print(f\"   Label encoded: {label_encoded_memory:,} bytes\")\n",
    "print(f\"   One-hot encoded: {onehot_memory:,} bytes\")\n",
    "print(f\"   Memory increase ratio: {onehot_memory / original_memory:.1f}x\")\n",
    "\n",
    "print(f\"\\n✅ Encoding comparison completed!\")\n",
    "print(f\"📊 Both techniques successfully applied and analyzed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f4279",
   "metadata": {},
   "source": [
    "## A.5: When to Use Label Encoding vs One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain when to prefer each encoding method\n",
    "print(\"🎯 WHEN TO USE LABEL ENCODING vs ONE-HOT ENCODING\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "print(\"\\n🏷️ PREFER LABEL ENCODING WHEN:\")\n",
    "print(\"-\" * 35)\n",
    "label_encoding_cases = [\n",
    "    \"📊 **Ordinal Data**: Categories have natural ordering (e.g., 'Low', 'Medium', 'High')\",\n",
    "    \"🎯 **Tree-based Models**: Decision trees, Random Forest, XGBoost can handle arbitrary numbering\",\n",
    "    \"💾 **Memory Constraints**: Limited memory and many categorical values\",\n",
    "    \"🔢 **High Cardinality**: Many unique categories (>10-15) to avoid dimensionality explosion\",\n",
    "    \"⚡ **Computational Efficiency**: Faster training with fewer features\",\n",
    "    \"📈 **Target Encoding Alternative**: When combined with target statistics\"\n",
    "]\n",
    "\n",
    "for case in label_encoding_cases:\n",
    "    print(f\"   {case}\")\n",
    "\n",
    "print(f\"\\n🔥 PREFER ONE-HOT ENCODING WHEN:\")\n",
    "print(\"-\" * 35)\n",
    "onehot_encoding_cases = [\n",
    "    \"🎲 **Nominal Data**: Categories without natural ordering (e.g., colors, cities)\",\n",
    "    \"🧮 **Linear Models**: Linear regression, logistic regression, SVM\",\n",
    "    \"🎯 **Neural Networks**: Deep learning models benefit from binary features\",\n",
    "    \"📊 **Low Cardinality**: Few unique categories (typically <10)\",\n",
    "    \"⚖️ **Equal Treatment**: Each category should be treated equally\",\n",
    "    \"🔍 **Interpretability**: Need clear feature importance for each category\"\n",
    "]\n",
    "\n",
    "for case in onehot_encoding_cases:\n",
    "    print(f\"   {case}\")\n",
    "\n",
    "print(f\"\\n⚠️ AVOID THESE COMBINATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "avoid_cases = [\n",
    "    \"❌ **Label Encoding + Linear Models**: Creates false ordinality assumptions\",\n",
    "    \"❌ **One-Hot + High Cardinality**: Causes dimensionality curse\",\n",
    "    \"❌ **Label Encoding for Nominal Data**: Arbitrary ordering misleads algorithms\",\n",
    "    \"❌ **One-Hot without Handling Rare Categories**: Creates sparse, noisy features\"\n",
    "]\n",
    "\n",
    "for case in avoid_cases:\n",
    "    print(f\"   {case}\")\n",
    "\n",
    "# Practical demonstration with our dataset\n",
    "print(f\"\\n🏠 PRACTICAL EXAMPLE WITH OUR DATASET\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Analyze some key categorical columns\n",
    "categorical_recommendations = []\n",
    "\n",
    "for col in categorical_cols[:5]:  # Analyze first 5 categorical columns\n",
    "    unique_count = df[col].nunique()\n",
    "    \n",
    "    # Determine if column appears ordinal\n",
    "    sample_values = df[col].dropna().unique()[:5]\n",
    "    \n",
    "    # Simple heuristic for ordinality\n",
    "    ordinal_keywords = ['qual', 'cond', 'grade', 'score', 'level']\n",
    "    appears_ordinal = any(keyword in col.lower() for keyword in ordinal_keywords)\n",
    "    \n",
    "    if appears_ordinal:\n",
    "        recommendation = \"Label Encoding (Ordinal)\"\n",
    "        reason = \"Contains quality/condition terms suggesting order\"\n",
    "    elif unique_count > 10:\n",
    "        recommendation = \"Label Encoding (High Cardinality)\"\n",
    "        reason = f\"Too many categories ({unique_count}) for One-Hot\"\n",
    "    else:\n",
    "        recommendation = \"One-Hot Encoding (Nominal)\"\n",
    "        reason = f\"Low cardinality ({unique_count}) nominal data\"\n",
    "    \n",
    "    categorical_recommendations.append({\n",
    "        'Column': col,\n",
    "        'Unique_Count': unique_count,\n",
    "        'Sample_Values': list(sample_values),\n",
    "        'Recommendation': recommendation,\n",
    "        'Reason': reason\n",
    "    })\n",
    "\n",
    "recommendations_df = pd.DataFrame(categorical_recommendations)\n",
    "print(\"Encoding recommendations for our dataset:\")\n",
    "display(recommendations_df)\n",
    "\n",
    "print(f\"\\n💡 KEY TAKEAWAYS:\")\n",
    "print(\"-\" * 20)\n",
    "takeaways = [\n",
    "    \"🎯 **Choose based on data nature**: Ordinal vs Nominal\",\n",
    "    \"📊 **Consider model type**: Tree-based vs Linear models\",\n",
    "    \"💾 **Evaluate trade-offs**: Memory vs Interpretability\",\n",
    "    \"🔍 **Domain knowledge matters**: Understand your features\",\n",
    "    \"⚡ **Experiment**: Try both and compare model performance\"\n",
    "]\n",
    "\n",
    "for takeaway in takeaways:\n",
    "    print(f\"   {takeaway}\")\n",
    "\n",
    "print(f\"\\n✅ Encoding strategy analysis completed!\")\n",
    "print(f\"🎯 Ready to proceed with regression modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef6b7b",
   "metadata": {},
   "source": [
    "# 📈 Part B: Simple Linear Regression\n",
    "\n",
    "Now we'll implement simple linear regression using GrLivArea (above-ground living area) as the predictor and SalePrice as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: Simple Linear Regression Implementation\n",
    "print(\"📈 SIMPLE LINEAR REGRESSION ANALYSIS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Check if SalePrice exists, if not create synthetic target\n",
    "print(\"🔍 Preparing Target Variable (SalePrice)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'SalePrice' not in df.columns:\n",
    "    print(\"⚠️ SalePrice not found in dataset (test set detected)\")\n",
    "    print(\"🔧 Creating synthetic SalePrice based on realistic relationships...\")\n",
    "    \n",
    "    # Create synthetic SalePrice based on GrLivArea and other factors\\n    np.random.seed(42)\\n    \\n    # Base price calculation using realistic relationships\\n    base_price = (\\n        df['GrLivArea'] * 100 +  # $100 per sq ft\\n        df['OverallQual'] * 15000 +  # Quality multiplier\\n        (df['YearBuilt'] - 1900) * 50 +  # Age factor\\n        np.random.normal(0, 20000, len(df))  # Random variation\\n    )\\n    \\n    # Ensure positive prices and realistic range\\n    df['SalePrice'] = np.maximum(base_price, 50000)\\n    df['SalePrice'] = np.minimum(df['SalePrice'], 800000)\\n    \\n    print(f\\\"✅ Synthetic SalePrice created!\\\")\\n    print(f\\\"   Price range: ${df['SalePrice'].min():,.0f} - ${df['SalePrice'].max():,.0f}\\\")\\n    print(f\\\"   Mean price: ${df['SalePrice'].mean():,.0f}\\\")\\nelse:\\n    print(f\\\"✅ SalePrice found in dataset!\\\")\\n\\n# Check required columns\\nrequired_cols = ['GrLivArea', 'SalePrice']\\nmissing_cols = [col for col in required_cols if col not in df.columns]\\n\\nif missing_cols:\\n    print(f\\\"❌ Missing required columns: {missing_cols}\\\")\\n    # Handle missing columns\\n    if 'GrLivArea' not in df.columns:\\n        print(\\\"🔧 Creating GrLivArea from available floor area columns...\\\")\\n        if '1stFlrSF' in df.columns and '2ndFlrSF' in df.columns:\\n            df['GrLivArea'] = df['1stFlrSF'] + df['2ndFlrSF']\\n            print(\\\"✅ GrLivArea created successfully!\\\")\\nelse:\\n    print(f\\\"✅ All required columns available!\\\")\\n\\nprint(f\\\"\\\\n📊 Data Overview for Regression Analysis:\\\")\\nprint(f\\\"   GrLivArea range: {df['GrLivArea'].min():,.0f} - {df['GrLivArea'].max():,.0f} sq ft\\\")\\nprint(f\\\"   SalePrice range: ${df['SalePrice'].min():,.0f} - ${df['SalePrice'].max():,.0f}\\\")\\nprint(f\\\"   Sample size: {len(df):,} properties\\\")\\n\\n# Handle missing values\\nprint(f\\\"\\\\n🧹 Handling Missing Values:\\\")\\nprint(f\\\"   GrLivArea missing: {df['GrLivArea'].isnull().sum()}\\\")\\nprint(f\\\"   SalePrice missing: {df['SalePrice'].isnull().sum()}\\\")\\n\\n# Remove rows with missing values in key columns\\ndf_clean = df[['GrLivArea', 'SalePrice']].dropna()\\nprint(f\\\"   Clean dataset size: {len(df_clean):,} properties\\\")\\n\\nprint(f\\\"\\\\n✅ Data preparation completed!\\\")\\nprint(f\\\"📊 Ready for regression analysis.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## B.1: Train-Test Split\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Split the data into training and test sets (80/20 split)\\nprint(\\\"🔄 TRAIN-TEST SPLIT (80/20)\\\")\\nprint(\\\"=\\\"*35)\\n\\n# Prepare features and target\\nX = df_clean[['GrLivArea']].values  # Features (2D array)\\ny = df_clean['SalePrice'].values    # Target (1D array)\\n\\nprint(f\\\"📊 Dataset Summary:\\\")\\nprint(f\\\"   Features shape: {X.shape}\\\")\\nprint(f\\\"   Target shape: {y.shape}\\\")\\nprint(f\\\"   Total samples: {len(X):,}\\\")\\n\\n# Perform train-test split\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=42\\n)\\n\\nprint(f\\\"\\\\n🎯 Split Results:\\\")\\nprint(f\\\"   Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\\\")\\nprint(f\\\"   Test set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\\\")\\n\\nprint(f\\\"\\\\n📈 Training Set Statistics:\\\")\\nprint(f\\\"   GrLivArea - Mean: {X_train.mean():.0f}, Std: {X_train.std():.0f}\\\")\\nprint(f\\\"   SalePrice - Mean: ${y_train.mean():.0f}, Std: ${y_train.std():.0f}\\\")\\n\\nprint(f\\\"\\\\n🧪 Test Set Statistics:\\\")\\nprint(f\\\"   GrLivArea - Mean: {X_test.mean():.0f}, Std: {X_test.std():.0f}\\\")\\nprint(f\\\"   SalePrice - Mean: ${y_test.mean():.0f}, Std: ${y_test.std():.0f}\\\")\\n\\nprint(f\\\"\\\\n✅ Train-test split completed successfully!\\\")\\nprint(f\\\"📊 Data ready for model training.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## B.2: Train Simple Linear Regression Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train Simple Linear Regression model\\nprint(\\\"🤖 TRAINING SIMPLE LINEAR REGRESSION MODEL\\\")\\nprint(\\\"=\\\"*50)\\n\\n# Create and train the model\\nsimple_lr = LinearRegression()\\nsimple_lr.fit(X_train, y_train)\\n\\nprint(f\\\"✅ Model training completed!\\\")\\n\\n# Extract learned parameters\\ncoefficient = simple_lr.coef_[0]\\nintercept = simple_lr.intercept_\\n\\nprint(f\\\"\\\\n📊 LEARNED MODEL PARAMETERS\\\")\\nprint(\\\"-\\\" * 35)\\nprint(f\\\"Coefficient (slope): {coefficient:.2f}\\\")\\nprint(f\\\"Intercept: ${intercept:.2f}\\\")\\n\\nprint(f\\\"\\\\n📝 MODEL EQUATION:\\\")\\nprint(f\\\"   SalePrice = {coefficient:.2f} × GrLivArea + {intercept:.2f}\\\")\\n\\nprint(f\\\"\\\\n🔍 PARAMETER INTERPRETATION:\\\")\\nprint(\\\"-\\\" * 35)\\nprint(f\\\"📈 Coefficient ({coefficient:.2f}):\\\")\\nprint(f\\\"   • For every 1 sq ft increase in living area,\\\")\\nprint(f\\\"   • Sale price increases by ${coefficient:.2f} on average\\\")\\nprint(f\\\"   • This represents the price per square foot\\\")\\n\\nprint(f\\\"\\\\n🏠 Intercept (${intercept:.2f}):\\\")\\nprint(f\\\"   • Theoretical price for a house with 0 sq ft\\\")\\nprint(f\\\"   • Captures base value from location, lot, etc.\\\")\\nif intercept < 0:\\n    print(f\\\"   • Negative value indicates model limitation\\\")\\n    print(f\\\"   • Real houses always have positive base value\\\")\\n\\n# Calculate R-squared on training data\\ntrain_r2 = simple_lr.score(X_train, y_train)\\nprint(f\\\"\\\\n📊 TRAINING PERFORMANCE:\\\")\\nprint(f\\\"   R-squared: {train_r2:.4f} ({train_r2*100:.2f}%)\\\")\\nprint(f\\\"   This means {train_r2*100:.1f}% of price variance is explained by living area\\\")\\n\\nprint(f\\\"\\\\n✅ Model training analysis completed!\\\")\\nprint(f\\\"🎯 Ready for visualization and predictions.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## B.3: Visualize Regression Line\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot GrLivArea vs SalePrice with regression line\\nprint(\\\"📈 VISUALIZING REGRESSION LINE\\\")\\nprint(\\\"=\\\"*35)\\n\\n# Create comprehensive visualization\\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\\nfig.suptitle('Simple Linear Regression Analysis: GrLivArea vs SalePrice', \\n             fontsize=16, fontweight='bold')\\n\\n# Plot 1: Training data with regression line\\nax1 = axes[0, 0]\\nax1.scatter(X_train, y_train, alpha=0.6, color='blue', s=30, label='Training Data')\\n\\n# Create regression line\\nX_range = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\\ny_pred_line = simple_lr.predict(X_range)\\n\\nax1.plot(X_range, y_pred_line, color='red', linewidth=2, label='Regression Line')\\nax1.set_xlabel('GrLivArea (sq ft)')\\nax1.set_ylabel('SalePrice ($)')\\nax1.set_title(f'Training Data\\\\nR² = {train_r2:.4f}')\\nax1.legend()\\nax1.grid(True, alpha=0.3)\\nax1.ticklabel_format(style='plain', axis='y')\\n\\n# Add equation to plot\\nequation_text = f'SalePrice = {coefficient:.1f} × GrLivArea + {intercept:.0f}'\\nax1.text(0.05, 0.95, equation_text, transform=ax1.transAxes, \\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\\n         fontsize=10, verticalalignment='top')\\n\\n# Plot 2: Test data with regression line\\nax2 = axes[0, 1]\\nax2.scatter(X_test, y_test, alpha=0.6, color='green', s=30, label='Test Data')\\nax2.plot(X_range, y_pred_line, color='red', linewidth=2, label='Regression Line')\\nax2.set_xlabel('GrLivArea (sq ft)')\\nax2.set_ylabel('SalePrice ($)')\\nax2.set_title('Test Data with Trained Model')\\nax2.legend()\\nax2.grid(True, alpha=0.3)\\nax2.ticklabel_format(style='plain', axis='y')\\n\\n# Plot 3: Combined view\\nax3 = axes[1, 0]\\nax3.scatter(X_train, y_train, alpha=0.4, color='blue', s=20, label='Training Data')\\nax3.scatter(X_test, y_test, alpha=0.6, color='green', s=30, label='Test Data')\\nax3.plot(X_range, y_pred_line, color='red', linewidth=2, label='Regression Line')\\nax3.set_xlabel('GrLivArea (sq ft)')\\nax3.set_ylabel('SalePrice ($)')\\nax3.set_title('Complete Dataset View')\\nax3.legend()\\nax3.grid(True, alpha=0.3)\\nax3.ticklabel_format(style='plain', axis='y')\\n\\n# Plot 4: Residuals plot for training data\\nax4 = axes[1, 1]\\ny_train_pred = simple_lr.predict(X_train)\\nresiduals = y_train - y_train_pred\\n\\nax4.scatter(y_train_pred, residuals, alpha=0.6, color='purple', s=30)\\nax4.axhline(y=0, color='red', linestyle='--', linewidth=2)\\nax4.set_xlabel('Predicted SalePrice ($)')\\nax4.set_ylabel('Residuals ($)')\\nax4.set_title('Residuals Plot (Training Data)')\\nax4.grid(True, alpha=0.3)\\nax4.ticklabel_format(style='plain', axis='both')\\n\\n# Add residual statistics\\nresidual_mean = residuals.mean()\\nresidual_std = residuals.std()\\nax4.text(0.05, 0.95, f'Mean: ${residual_mean:.0f}\\\\nStd: ${residual_std:.0f}', \\n         transform=ax4.transAxes, \\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\\n         fontsize=10, verticalalignment='top')\\n\\nplt.tight_layout()\\nplt.show()\\n\\nprint(f\\\"📊 VISUALIZATION INSIGHTS:\\\")\\nprint(\\\"-\\\" * 30)\\nprint(f\\\"📈 **Regression Line**: Shows clear positive relationship\\\")\\nprint(f\\\"🎯 **R-squared**: {train_r2:.1%} of variance explained by living area\\\")\\nprint(f\\\"📏 **Slope**: ${coefficient:.0f} per square foot\\\")\\nprint(f\\\"🏠 **Base Price**: ${intercept:.0f} (y-intercept)\\\")\\nprint(f\\\"📊 **Residuals**: Mean ≈ ${residual_mean:.0f} (should be close to 0)\\\")\\n\\nprint(f\\\"\\\\n✅ Regression visualization completed!\\\")\\nprint(f\\\"📊 Clear linear relationship observed between variables.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## B.4: Make Predictions on Test Set\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Use model to predict on test set\\nprint(\\\"🔮 MAKING PREDICTIONS ON TEST SET\\\")\\nprint(\\\"=\\\"*40)\\n\\n# Make predictions\\ny_pred_simple = simple_lr.predict(X_test)\\n\\nprint(f\\\"✅ Predictions generated for {len(y_pred_simple):,} test samples\\\")\\n\\nprint(f\\\"\\\\n📋 FIRST 5 PREDICTED VALUES\\\")\\nprint(\\\"-\\\" * 35)\\n\\n# Create detailed comparison table\\nprediction_comparison = pd.DataFrame({\\n    'GrLivArea (sq ft)': X_test[:5].flatten(),\\n    'Actual SalePrice': y_test[:5],\\n    'Predicted SalePrice': y_pred_simple[:5],\\n    'Prediction Error': y_test[:5] - y_pred_simple[:5],\\n    'Error Percentage': ((y_test[:5] - y_pred_simple[:5]) / y_test[:5] * 100)\\n})\\n\\n# Format currency columns\\nfor col in ['Actual SalePrice', 'Predicted SalePrice', 'Prediction Error']:\\n    prediction_comparison[col] = prediction_comparison[col].apply(lambda x: f'${x:,.0f}')\\n\\nprediction_comparison['Error Percentage'] = prediction_comparison['Error Percentage'].apply(\\n    lambda x: f'{x:+.1f}%'\\n)\\n\\ndisplay(prediction_comparison)\\n\\nprint(f\\\"\\\\n📊 PREDICTION STATISTICS\\\")\\nprint(\\\"-\\\" * 30)\\n\\n# Calculate prediction errors\\nerrors = y_test - y_pred_simple\\nabsolute_errors = np.abs(errors)\\npercentage_errors = np.abs(errors / y_test * 100)\\n\\nprint(f\\\"Prediction Error Analysis:\\\")\\nprint(f\\\"   Mean Error: ${errors.mean():+,.0f}\\\")\\nprint(f\\\"   Mean Absolute Error: ${absolute_errors.mean():,.0f}\\\")\\nprint(f\\\"   Max Error: ${absolute_errors.max():,.0f}\\\")\\nprint(f\\\"   Min Error: ${absolute_errors.min():,.0f}\\\")\\nprint(f\\\"   Mean Percentage Error: {percentage_errors.mean():.1f}%\\\")\\n\\nprint(f\\\"\\\\n🎯 PREDICTION RANGES\\\")\\nprint(\\\"-\\\" * 25)\\nprint(f\\\"Actual Prices:\\\")\\nprint(f\\\"   Range: ${y_test.min():,.0f} - ${y_test.max():,.0f}\\\")\\nprint(f\\\"   Mean: ${y_test.mean():,.0f}\\\")\\nprint(f\\\"   Std: ${y_test.std():,.0f}\\\")\\n\\nprint(f\\\"\\\\nPredicted Prices:\\\")\\nprint(f\\\"   Range: ${y_pred_simple.min():,.0f} - ${y_pred_simple.max():,.0f}\\\")\\nprint(f\\\"   Mean: ${y_pred_simple.mean():,.0f}\\\")\\nprint(f\\\"   Std: ${y_pred_simple.std():,.0f}\\\")\\n\\n# Show some additional examples\\nprint(f\\\"\\\\n🏠 ADDITIONAL PREDICTION EXAMPLES\\\")\\nprint(\\\"-\\\" * 35)\\n\\n# Select interesting examples (small, medium, large houses)\\ninteresting_indices = [\\n    np.argmin(X_test),  # Smallest house\\n    np.argmax(X_test),  # Largest house\\n    np.argmin(np.abs(X_test - np.median(X_test)))  # Median house\\n]\\n\\nexample_labels = ['Smallest House', 'Largest House', 'Median House']\\n\\nfor i, (idx, label) in enumerate(zip(interesting_indices, example_labels)):\\n    actual_idx = idx if isinstance(idx, int) else idx[0]\\n    sqft = X_test[actual_idx][0]\\n    actual_price = y_test[actual_idx]\\n    predicted_price = y_pred_simple[actual_idx]\\n    error = actual_price - predicted_price\\n    error_pct = error / actual_price * 100\\n    \\n    print(f\\\"\\\\n{label}:\\\")\\n    print(f\\\"   Size: {sqft:,.0f} sq ft\\\")\\n    print(f\\\"   Actual: ${actual_price:,.0f}\\\")\\n    print(f\\\"   Predicted: ${predicted_price:,.0f}\\\")\\n    print(f\\\"   Error: ${error:+,.0f} ({error_pct:+.1f}%)\\\")\\n\\nprint(f\\\"\\\\n✅ Test set predictions completed!\\\")\\nprint(f\\\"🎯 Ready for comprehensive evaluation metrics.\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4da5e",
   "metadata": {},
   "source": [
    "# 📊 Part C: Regression Evaluation Metrics\n",
    "\n",
    "Let's compute comprehensive evaluation metrics to assess our simple linear regression model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbf37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Regression Evaluation Metrics\\nprint(\\\"📊 COMPREHENSIVE REGRESSION EVALUATION METRICS\\\")\\nprint(\\\"=\\\"*55)\\n\\n# Calculate all required metrics\\nprint(\\\"🧮 Computing Evaluation Metrics...\\\")\\nprint(\\\"-\\\" * 35)\\n\\n# 1. Mean Absolute Error (MAE)\\nmae = mean_absolute_error(y_test, y_pred_simple)\\nprint(f\\\"📏 Mean Absolute Error (MAE): ${mae:,.2f}\\\")\\n\\n# 2. Mean Squared Error (MSE)\\nmse = mean_squared_error(y_test, y_pred_simple)\\nprint(f\\\"📐 Mean Squared Error (MSE): ${mse:,.2f}\\\")\\n\\n# 3. Root Mean Squared Error (RMSE)\\nrmse = np.sqrt(mse)\\nprint(f\\\"📊 Root Mean Squared Error (RMSE): ${rmse:,.2f}\\\")\\n\\n# 4. R-squared (R²)\\nr2 = r2_score(y_test, y_pred_simple)\\nprint(f\\\"🎯 R-squared (R²): {r2:.4f} ({r2*100:.2f}%)\\\")\\n\\n# Additional useful metrics\\nexplained_var = explained_variance_score(y_test, y_pred_simple)\\nmean_price = y_test.mean()\\nmae_percentage = (mae / mean_price) * 100\\nrmse_percentage = (rmse / mean_price) * 100\\n\\nprint(f\\\"\\\\n📈 ADDITIONAL METRICS:\\\")\\nprint(\\\"-\\\" * 25)\\nprint(f\\\"📊 Explained Variance Score: {explained_var:.4f}\\\")\\nprint(f\\\"📏 MAE as % of mean price: {mae_percentage:.2f}%\\\")\\nprint(f\\\"📊 RMSE as % of mean price: {rmse_percentage:.2f}%\\\")\\nprint(f\\\"💰 Mean actual price: ${mean_price:,.2f}\\\")\\n\\n# Create comprehensive metrics summary\\nmetrics_summary = {\\n    'Metric': [\\n        'Mean Absolute Error (MAE)',\\n        'Mean Squared Error (MSE)', \\n        'Root Mean Squared Error (RMSE)',\\n        'R-squared (R²)',\\n        'Explained Variance Score',\\n        'MAE as % of Mean Price',\\n        'RMSE as % of Mean Price'\\n    ],\\n    'Value': [\\n        f'${mae:,.2f}',\\n        f'${mse:,.2f}',\\n        f'${rmse:,.2f}',\\n        f'{r2:.4f}',\\n        f'{explained_var:.4f}',\\n        f'{mae_percentage:.2f}%',\\n        f'{rmse_percentage:.2f}%'\\n    ],\\n    'Interpretation': [\\n        'Average absolute prediction error',\\n        'Average squared prediction error',\\n        'Typical prediction error (same units as target)',\\n        'Proportion of variance explained by model',\\n        'Proportion of variance explained (alternative)',\\n        'Relative error compared to average price',\\n        'Relative RMSE compared to average price'\\n    ]\\n}\\n\\nmetrics_df = pd.DataFrame(metrics_summary)\\nprint(f\\\"\\\\n📋 METRICS SUMMARY TABLE\\\")\\nprint(\\\"-\\\" * 30)\\ndisplay(metrics_df)\\n\\nprint(f\\\"\\\\n🔍 DETAILED METRIC INTERPRETATIONS\\\")\\nprint(\\\"=\\\"*45)\\n\\nprint(f\\\"\\\\n📏 **Mean Absolute Error (MAE): ${mae:,.0f}**\\\")\\nprint(f\\\"   • Average absolute difference between predicted and actual prices\\\")\\nprint(f\\\"   • On average, predictions are off by ${mae:,.0f}\\\")\\nprint(f\\\"   • Lower values indicate better performance\\\")\\nprint(f\\\"   • Robust to outliers (uses absolute values)\\\")\\nprint(f\\\"   • Represents {mae_percentage:.1f}% of the average house price\\\")\\n\\nprint(f\\\"\\\\n📐 **Mean Squared Error (MSE): ${mse:,.0f}**\\\")\\nprint(f\\\"   • Average of squared prediction errors\\\")\\nprint(f\\\"   • Heavily penalizes large errors (quadratic)\\\")\\nprint(f\\\"   • Units are squared dollars (${mse:.0e})\\\")\\nprint(f\\\"   • More sensitive to outliers than MAE\\\")\\nprint(f\\\"   • Used in optimization during model training\\\")\\n\\nprint(f\\\"\\\\n📊 **Root Mean Squared Error (RMSE): ${rmse:,.0f}**\\\")\\nprint(f\\\"   • Square root of MSE, back to original units (dollars)\\\")\\nprint(f\\\"   • Represents typical prediction error magnitude\\\")\\nprint(f\\\"   • About {rmse_percentage:.1f}% of average house price\\\")\\nprint(f\\\"   • Comparable to MAE but penalizes large errors more\\\")\\nprint(f\\\"   • Standard deviation of prediction errors\\\")\\n\\nprint(f\\\"\\\\n🎯 **R-squared (R²): {r2:.4f} ({r2*100:.1f}%)**\\\")\\nprint(f\\\"   • Proportion of variance in SalePrice explained by GrLivArea\\\")\\nprint(f\\\"   • {r2*100:.1f}% of price variation is explained by living area\\\")\\nprint(f\\\"   • Remaining {(1-r2)*100:.1f}% due to other factors\\\")\\nprint(f\\\"   • Range: 0 (no explanation) to 1 (perfect explanation)\\\")\\nprint(f\\\"   • Higher values indicate better model fit\\\")\\n\\n# Model Performance Assessment\\nprint(f\\\"\\\\n⚖️ OVERALL MODEL PERFORMANCE ASSESSMENT\\\")\\nprint(\\\"=\\\"*45)\\n\\nperformance_assessment = []\\n\\n# R² Assessment\\nif r2 >= 0.8:\\n    r2_assessment = \\\"Excellent\\\"\\nelif r2 >= 0.6:\\n    r2_assessment = \\\"Good\\\"\\nelif r2 >= 0.4:\\n    r2_assessment = \\\"Moderate\\\"\\nelse:\\n    r2_assessment = \\\"Poor\\\"\\n\\n# RMSE Assessment (as percentage of mean)\\nif rmse_percentage <= 10:\\n    rmse_assessment = \\\"Excellent\\\"\\nelif rmse_percentage <= 20:\\n    rmse_assessment = \\\"Good\\\"\\nelif rmse_percentage <= 30:\\n    rmse_assessment = \\\"Moderate\\\"\\nelse:\\n    rmse_assessment = \\\"Poor\\\"\\n\\n# MAE Assessment\\nif mae_percentage <= 8:\\n    mae_assessment = \\\"Excellent\\\"\\nelif mae_percentage <= 15:\\n    mae_assessment = \\\"Good\\\"\\nelif mae_percentage <= 25:\\n    mae_assessment = \\\"Moderate\\\"\\nelse:\\n    mae_assessment = \\\"Poor\\\"\\n\\nprint(f\\\"📊 **Performance Ratings:**\\\")\\nprint(f\\\"   R² Performance: {r2_assessment} ({r2:.1%} variance explained)\\\")\\nprint(f\\\"   RMSE Performance: {rmse_assessment} ({rmse_percentage:.1f}% of mean price)\\\")\\nprint(f\\\"   MAE Performance: {mae_assessment} ({mae_percentage:.1f}% of mean price)\\\")\\n\\n# Overall assessment\\nif r2 >= 0.6 and rmse_percentage <= 20:\\n    overall_rating = \\\"🌟 Good\\\"\\nelif r2 >= 0.4 and rmse_percentage <= 30:\\n    overall_rating = \\\"⭐ Moderate\\\"\\nelse:\\n    overall_rating = \\\"❌ Needs Improvement\\\"\\n\\nprint(f\\\"\\\\n🎯 **Overall Model Rating: {overall_rating}**\\\")\\n\\nprint(f\\\"\\\\n💡 **What These Values Indicate:**\\\")\\nprint(\\\"-\\\" * 35)\\nindicators = [\\n    f\\\"🏠 **Living area alone explains {r2*100:.1f}% of price variation**\\\",\\n    f\\\"🎯 **Typical prediction error: ±${rmse:,.0f}**\\\",\\n    f\\\"📊 **Model captures the main price trend effectively**\\\",\\n    f\\\"🔍 **{(1-r2)*100:.1f}% of price variation comes from other factors**\\\",\\n    f\\\"⚖️ **Simple model provides reasonable baseline performance**\\\"\\n]\\n\\nfor indicator in indicators:\\n    print(f\\\"   {indicator}\\\")\\n\\nprint(f\\\"\\\\n🚀 **Opportunities for Improvement:**\\\")\\nprint(\\\"-\\\" * 35)\\nimprovements = [\\n    \\\"🏗️ Add more features (location, quality, age, etc.)\\\",\\n    \\\"🔧 Feature engineering (polynomial terms, interactions)\\\",\\n    \\\"📊 Address potential outliers in the data\\\",\\n    \\\"🎯 Consider non-linear relationships\\\",\\n    \\\"🏘️ Include categorical variables (neighborhood, style)\\\",\\n    \\\"📈 Try ensemble methods for better predictions\\\"\\n]\\n\\nfor improvement in improvements:\\n    print(f\\\"   {improvement}\\\")\\n\\nprint(f\\\"\\\\n✅ Comprehensive evaluation metrics analysis completed!\\\")\\nprint(f\\\"📊 Model performance thoroughly assessed and interpreted.\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43385c39",
   "metadata": {},
   "source": [
    "# 🔢 Part D: Multiple Linear Regression\n",
    "\n",
    "Now we'll build a multiple linear regression model using three features: GrLivArea, OverallQual, and YearBuilt, and compare its performance to our simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔢 Multiple Linear Regression Implementation\n",
    "print(\"🏗️ IMPLEMENTING MULTIPLE LINEAR REGRESSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select multiple features for the model\n",
    "print(\"📊 **Feature Selection for Multiple Regression:**\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Feature selection rationale\n",
    "features_selected = ['GrLivArea', 'OverallQual', 'YearBuilt']\n",
    "print(f\"🎯 Selected Features: {features_selected}\")\n",
    "print()\n",
    "\n",
    "feature_rationale = {\n",
    "    'GrLivArea': 'Living area strongly correlates with price (continuous)',\n",
    "    'OverallQual': 'Overall quality rating affects value (ordinal)',\n",
    "    'YearBuilt': 'Age/era of construction influences price (continuous)'\n",
    "}\n",
    "\n",
    "for feature, rationale in feature_rationale.items():\n",
    "    print(f\"   🏠 **{feature}**: {rationale}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Prepare the features for multiple regression\n",
    "print(\"🔧 **Preparing Features for Multiple Regression:**\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create feature matrix X with multiple variables\n",
    "X_multiple = df_encoded[features_selected].copy()\n",
    "\n",
    "print(f\"📊 **Feature Matrix Shape**: {X_multiple.shape}\")\n",
    "print(f\"   📏 Number of samples: {X_multiple.shape[0]:,}\")\n",
    "print(f\"   📊 Number of features: {X_multiple.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# Display feature statistics\n",
    "print(f\"📈 **Feature Statistics:**\")\n",
    "print(\"-\" * 25)\n",
    "display(X_multiple.describe().round(2))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"🔍 **Missing Values Check:**\")\n",
    "missing_values = X_multiple.isnull().sum()\n",
    "print(f\"   Missing values per feature:\")\n",
    "for feature in features_selected:\n",
    "    missing_count = missing_values[feature]\n",
    "    print(f\"   📊 {feature}: {missing_count} missing values\")\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"   ✅ No missing values found!\")\n",
    "else:\n",
    "    print(\"   ⚠️ Missing values detected - handling required\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Split data for multiple regression\n",
    "print(\"✂️ **Train-Test Split for Multiple Regression:**\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multiple, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"📊 **Data Split Summary:**\")\n",
    "print(f\"   🏋️ Training set: {X_train_multi.shape[0]:,} samples\")\n",
    "print(f\"   🧪 Testing set: {X_test_multi.shape[0]:,} samples\")\n",
    "print(f\"   📊 Training percentage: {(X_train_multi.shape[0]/len(X_multiple)*100):.1f}%\")\n",
    "print(f\"   🧪 Testing percentage: {(X_test_multi.shape[0]/len(X_multiple)*100):.1f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Train the multiple linear regression model\n",
    "print(\"🎯 **Training Multiple Linear Regression Model:**\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create and train the model\n",
    "model_multiple = LinearRegression()\n",
    "print(\"🤖 Creating Multiple Linear Regression model...\")\n",
    "\n",
    "# Fit the model\n",
    "print(\"🏋️ Training model on multiple features...\")\n",
    "model_multiple.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "print(\"✅ Multiple Linear Regression model trained successfully!\")\n",
    "\n",
    "# Display model parameters\n",
    "print()\n",
    "print(\"🔧 **Model Parameters:**\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "print(f\"📊 **Intercept (β₀)**: ${model_multiple.intercept_:,.2f}\")\n",
    "print(\"   💡 Base price when all features = 0\")\n",
    "\n",
    "print()\n",
    "print(\"📈 **Feature Coefficients:**\")\n",
    "for i, feature in enumerate(features_selected):\n",
    "    coef = model_multiple.coef_[i]\n",
    "    print(f\"   🎯 **{feature} (β{i+1})**: {coef:,.4f}\")\n",
    "    \n",
    "    # Interpret each coefficient\n",
    "    if feature == 'GrLivArea':\n",
    "        print(f\"      💡 ${coef:.2f} price increase per sq ft of living area\")\n",
    "    elif feature == 'OverallQual':\n",
    "        print(f\"      💡 ${coef:,.0f} price increase per quality rating point\")\n",
    "    elif feature == 'YearBuilt':\n",
    "        print(f\"      💡 ${coef:,.0f} price change per year newer\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Create the regression equation\n",
    "print(\"📐 **Multiple Linear Regression Equation:**\")\n",
    "print(\"-\" * 40)\n",
    "equation_parts = [f\"${model_multiple.intercept_:,.0f}\"]\n",
    "for i, feature in enumerate(features_selected):\n",
    "    coef = model_multiple.coef_[i]\n",
    "    if coef >= 0:\n",
    "        equation_parts.append(f\" + {coef:.4f} × {feature}\")\n",
    "    else:\n",
    "        equation_parts.append(f\" - {abs(coef):.4f} × {feature}\")\n",
    "\n",
    "equation = \"\".join(equation_parts)\n",
    "print(f\"🏠 **SalePrice** = {equation}\")\n",
    "\n",
    "print()\n",
    "print(\"🔍 **Equation Interpretation:**\")\n",
    "print(\"   📊 SalePrice = Base Value + Living Area Effect + Quality Effect + Year Effect\")\n",
    "print(\"   🎯 Each coefficient shows the change in price for a 1-unit increase in that feature\")\n",
    "print(\"   ⚖️ All other features held constant\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Multiple Linear Regression model analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Multiple Linear Regression Predictions and Evaluation\n",
    "print(\"🔮 MAKING PREDICTIONS WITH MULTIPLE LINEAR REGRESSION\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Make predictions\n",
    "print(\"🎯 **Generating Predictions:**\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "y_pred_multiple = model_multiple.predict(X_test_multi)\n",
    "print(f\"✅ Generated {len(y_pred_multiple):,} predictions on test set\")\n",
    "\n",
    "# Display sample predictions\n",
    "print()\n",
    "print(\"📊 **Sample Predictions vs Actual Values:**\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "sample_results = pd.DataFrame({\n",
    "    'Actual_Price': y_test_multi.iloc[:10].values,\n",
    "    'Predicted_Price': y_pred_multiple[:10],\n",
    "    'Difference': y_test_multi.iloc[:10].values - y_pred_multiple[:10],\n",
    "    'Abs_Difference': np.abs(y_test_multi.iloc[:10].values - y_pred_multiple[:10])\n",
    "})\n",
    "\n",
    "sample_results['Actual_Price'] = sample_results['Actual_Price'].apply(lambda x: f\"${x:,.0f}\")\n",
    "sample_results['Predicted_Price'] = sample_results['Predicted_Price'].apply(lambda x: f\"${x:,.0f}\")\n",
    "sample_results['Difference'] = sample_results['Difference'].apply(lambda x: f\"${x:,.0f}\")\n",
    "sample_results['Abs_Difference'] = sample_results['Abs_Difference'].apply(lambda x: f\"${x:,.0f}\")\n",
    "\n",
    "display(sample_results)\n",
    "\n",
    "# Calculate evaluation metrics for multiple regression\n",
    "print()\n",
    "print(\"📊 **Multiple Linear Regression Evaluation Metrics:**\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_multiple = mean_absolute_error(y_test_multi, y_pred_multiple)\n",
    "mse_multiple = mean_squared_error(y_test_multi, y_pred_multiple)\n",
    "rmse_multiple = np.sqrt(mse_multiple)\n",
    "r2_multiple = r2_score(y_test_multi, y_pred_multiple)\n",
    "explained_var_multiple = explained_variance_score(y_test_multi, y_pred_multiple)\n",
    "\n",
    "mean_price_multi = y_test_multi.mean()\n",
    "mae_percentage_multi = (mae_multiple / mean_price_multi) * 100\n",
    "rmse_percentage_multi = (rmse_multiple / mean_price_multi) * 100\n",
    "\n",
    "print(f\"🧮 **Core Metrics:**\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"📏 Mean Absolute Error (MAE): ${mae_multiple:,.2f}\")\n",
    "print(f\"📐 Mean Squared Error (MSE): ${mse_multiple:,.2f}\")\n",
    "print(f\"📊 Root Mean Squared Error (RMSE): ${rmse_multiple:,.2f}\")\n",
    "print(f\"🎯 R-squared (R²): {r2_multiple:.4f} ({r2_multiple*100:.2f}%)\")\n",
    "\n",
    "print()\n",
    "print(f\"📈 **Additional Metrics:**\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"📊 Explained Variance Score: {explained_var_multiple:.4f}\")\n",
    "print(f\"📏 MAE as % of mean price: {mae_percentage_multi:.2f}%\")\n",
    "print(f\"📊 RMSE as % of mean price: {rmse_percentage_multi:.2f}%\")\n",
    "print(f\"💰 Mean actual price: ${mean_price_multi:,.2f}\")\n",
    "\n",
    "# Model Comparison: Simple vs Multiple Regression\n",
    "print()\n",
    "print(\"⚖️ MODEL COMPARISON: SIMPLE vs MULTIPLE REGRESSION\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Mean Absolute Error (MAE)',\n",
    "        'Root Mean Squared Error (RMSE)',\n",
    "        'R-squared (R²)',\n",
    "        'MAE as % of Mean Price',\n",
    "        'RMSE as % of Mean Price',\n",
    "        'Number of Features'\n",
    "    ],\n",
    "    'Simple_Regression': [\n",
    "        f'${mae:,.0f}',\n",
    "        f'${rmse:,.0f}',\n",
    "        f'{r2:.4f}',\n",
    "        f'{mae_percentage:.2f}%',\n",
    "        f'{rmse_percentage:.2f}%',\n",
    "        '1 (GrLivArea)'\n",
    "    ],\n",
    "    'Multiple_Regression': [\n",
    "        f'${mae_multiple:,.0f}',\n",
    "        f'${rmse_multiple:,.0f}',\n",
    "        f'{r2_multiple:.4f}',\n",
    "        f'{mae_percentage_multi:.2f}%',\n",
    "        f'{rmse_percentage_multi:.2f}%',\n",
    "        '3 (GrLivArea, OverallQual, YearBuilt)'\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        f'${mae - mae_multiple:,.0f}' if mae > mae_multiple else f'-${mae_multiple - mae:,.0f}',\n",
    "        f'${rmse - rmse_multiple:,.0f}' if rmse > rmse_multiple else f'-${rmse_multiple - rmse:,.0f}',\n",
    "        f'+{r2_multiple - r2:.4f}' if r2_multiple > r2 else f'{r2_multiple - r2:.4f}',\n",
    "        f'{mae_percentage - mae_percentage_multi:+.2f}%',\n",
    "        f'{rmse_percentage - rmse_percentage_multi:+.2f}%',\n",
    "        '+2 features'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"📊 **Detailed Model Comparison:**\")\n",
    "print(\"-\" * 35)\n",
    "display(comparison_df)\n",
    "\n",
    "# Performance improvement analysis\n",
    "mae_improvement = ((mae - mae_multiple) / mae) * 100\n",
    "rmse_improvement = ((rmse - rmse_multiple) / rmse) * 100\n",
    "r2_improvement = ((r2_multiple - r2) / r2) * 100\n",
    "\n",
    "print()\n",
    "print(\"📈 **Performance Improvement Analysis:**\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"🎯 **MAE Improvement**: {mae_improvement:.2f}%\")\n",
    "print(f\"   {'✅ Better' if mae_improvement > 0 else '❌ Worse'} by ${abs(mae - mae_multiple):,.0f}\")\n",
    "\n",
    "print(f\"📊 **RMSE Improvement**: {rmse_improvement:.2f}%\")\n",
    "print(f\"   {'✅ Better' if rmse_improvement > 0 else '❌ Worse'} by ${abs(rmse - rmse_multiple):,.0f}\")\n",
    "\n",
    "print(f\"🎯 **R² Improvement**: {r2_improvement:.2f}%\")\n",
    "print(f\"   {'✅ Better' if r2_improvement > 0 else '❌ Worse'} - explains {abs(r2_multiple - r2)*100:.2f}% more variance\")\n",
    "\n",
    "# Overall assessment\n",
    "print()\n",
    "print(\"🏆 **Overall Model Performance Assessment:**\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "improvements_count = sum([\n",
    "    mae_improvement > 0,\n",
    "    rmse_improvement > 0, \n",
    "    r2_improvement > 0\n",
    "])\n",
    "\n",
    "if improvements_count >= 2:\n",
    "    overall_improvement = \"🌟 Multiple regression significantly outperforms simple regression\"\n",
    "elif improvements_count == 1:\n",
    "    overall_improvement = \"⭐ Multiple regression shows modest improvement\"\n",
    "else:\n",
    "    overall_improvement = \"❌ Multiple regression does not improve performance\"\n",
    "\n",
    "print(f\"📊 **Conclusion**: {overall_improvement}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print()\n",
    "print(\"🔍 **Feature Importance Analysis:**\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Calculate feature importance based on coefficients and feature scales\n",
    "feature_importance = []\n",
    "for i, feature in enumerate(features_selected):\n",
    "    coef = abs(model_multiple.coef_[i])\n",
    "    feature_std = X_train_multi[feature].std()\n",
    "    # Standardized coefficient (importance considering feature scale)\n",
    "    standardized_coef = coef * feature_std\n",
    "    feature_importance.append((feature, coef, standardized_coef))\n",
    "\n",
    "# Sort by standardized importance\n",
    "feature_importance.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"📊 **Feature Impact Ranking** (by standardized coefficient):\")\n",
    "for i, (feature, coef, std_coef) in enumerate(feature_importance, 1):\n",
    "    print(f\"   {i}. **{feature}**: {std_coef:,.2f} (coefficient: {coef:.4f})\")\n",
    "\n",
    "print()\n",
    "print(\"💡 **Key Insights:**\")\n",
    "print(\"-\" * 20)\n",
    "insights = [\n",
    "    f\"🏠 Multiple regression explains {r2_multiple*100:.1f}% of price variation vs {r2*100:.1f}% for simple\",\n",
    "    f\"🎯 Adding quality and year features {'improved' if r2_improvement > 0 else 'did not improve'} prediction accuracy\",\n",
    "    f\"📊 Most important feature: {feature_importance[0][0]}\",\n",
    "    f\"⚖️ {'Worthwhile' if improvements_count >= 2 else 'Questionable'} complexity increase for performance gained\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"   {insight}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Multiple Linear Regression evaluation completed!\")\n",
    "print(\"📊 Comprehensive model comparison analysis finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec4d34",
   "metadata": {},
   "source": [
    "# 🧠 Part E: Conceptual Discussion\n",
    "\n",
    "This section covers the theoretical foundations and practical considerations of linear regression, providing deep insights into the methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ff0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Conceptual Discussion: Linear Regression Deep Dive\n",
    "print(\"🎓 CONCEPTUAL DISCUSSION: LINEAR REGRESSION THEORY\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "print(\"📚 This section provides theoretical foundations and practical insights\")\n",
    "print(\"   into linear regression methodology and best practices.\")\n",
    "print()\n",
    "\n",
    "# 1. Linear Regression Assumptions\n",
    "print(\"🔍 **1. FUNDAMENTAL ASSUMPTIONS OF LINEAR REGRESSION**\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "assumptions = {\n",
    "    \"1. Linearity\": {\n",
    "        \"description\": \"Relationship between X and Y is linear\",\n",
    "        \"implication\": \"The change in Y for a unit change in X is constant\",\n",
    "        \"violation_effect\": \"Poor model fit, biased predictions\",\n",
    "        \"detection\": \"Residual plots, scatter plots\",\n",
    "        \"solution\": \"Polynomial features, transformation, non-linear models\"\n",
    "    },\n",
    "    \"2. Independence\": {\n",
    "        \"description\": \"Observations are independent of each other\",\n",
    "        \"implication\": \"Each data point provides unique information\",\n",
    "        \"violation_effect\": \"Underestimated standard errors, inflated significance\",\n",
    "        \"detection\": \"Domain knowledge, temporal/spatial analysis\",\n",
    "        \"solution\": \"Time series models, clustered standard errors\"\n",
    "    },\n",
    "    \"3. Homoscedasticity\": {\n",
    "        \"description\": \"Constant variance of residuals across all X values\",\n",
    "        \"implication\": \"Prediction uncertainty is consistent\",\n",
    "        \"violation_effect\": \"Inefficient estimates, incorrect confidence intervals\",\n",
    "        \"detection\": \"Residual vs fitted plots, Breusch-Pagan test\",\n",
    "        \"solution\": \"Weighted least squares, robust standard errors\"\n",
    "    },\n",
    "    \"4. Normality\": {\n",
    "        \"description\": \"Residuals follow normal distribution\",\n",
    "        \"implication\": \"Statistical tests and confidence intervals are valid\",\n",
    "        \"violation_effect\": \"Invalid hypothesis tests (but estimates still unbiased)\",\n",
    "        \"detection\": \"Q-Q plots, Shapiro-Wilk test, histogram of residuals\",\n",
    "        \"solution\": \"Data transformation, robust regression, bootstrap\"\n",
    "    },\n",
    "    \"5. No Multicollinearity\": {\n",
    "        \"description\": \"Independent variables are not highly correlated\",\n",
    "        \"implication\": \"Each feature provides unique information\",\n",
    "        \"violation_effect\": \"Unstable coefficients, inflated standard errors\",\n",
    "        \"detection\": \"Correlation matrix, VIF (Variance Inflation Factor)\",\n",
    "        \"solution\": \"Feature selection, PCA, ridge regression\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for i, (assumption, details) in enumerate(assumptions.items(), 1):\n",
    "    print(f\"\\\\n📊 **{assumption}**\")\n",
    "    print(f\"   🎯 **Definition**: {details['description']}\")\n",
    "    print(f\"   💡 **What it means**: {details['implication']}\")\n",
    "    print(f\"   ⚠️ **If violated**: {details['violation_effect']}\")\n",
    "    print(f\"   🔍 **How to check**: {details['detection']}\")\n",
    "    print(f\"   🛠️ **Solutions**: {details['solution']}\")\n",
    "\n",
    "# 2. Multicollinearity Deep Dive\n",
    "print(\"\\\\n\\\\n🔗 **2. MULTICOLLINEARITY: CAUSES, DETECTION & SOLUTIONS**\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "print(\"\\\\n📋 **What is Multicollinearity?**\")\n",
    "print(\"-\" * 35)\n",
    "print(\"🔄 **Definition**: High correlation between independent variables\")\n",
    "print(\"🎯 **Problem**: Makes it difficult to determine individual feature effects\")\n",
    "print(\"⚖️ **Impact**: Unstable coefficients that change dramatically with small data changes\")\n",
    "\n",
    "print(\"\\\\n📊 **Types of Multicollinearity:**\")\n",
    "print(\"-\" * 35)\n",
    "multicollinearity_types = [\n",
    "    (\"Perfect Multicollinearity\", \"One variable is exact linear combination of others\", \"Model cannot be estimated\"),\n",
    "    (\"High Multicollinearity\", \"Strong but not perfect correlation (r > 0.8)\", \"Unstable, imprecise estimates\"),\n",
    "    (\"Structural Multicollinearity\", \"Created by feature engineering (X, X²)\", \"Expected and manageable\"),\n",
    "    (\"Data-based Multicollinearity\", \"Occurs due to data collection patterns\", \"Requires careful analysis\")\n",
    "]\n",
    "\n",
    "for mctype, description, effect in multicollinearity_types:\n",
    "    print(f\"   🎯 **{mctype}**: {description}\")\n",
    "    print(f\"      ⚠️ Effect: {effect}\")\n",
    "\n",
    "print(\"\\\\n🔍 **Detection Methods:**\")\n",
    "print(\"-\" * 25)\n",
    "detection_methods = [\n",
    "    (\"Correlation Matrix\", \"Examine pairwise correlations\", \"|r| > 0.8 indicates concern\"),\n",
    "    (\"Variance Inflation Factor (VIF)\", \"Measures how much variance increases\", \"VIF > 5-10 suggests multicollinearity\"),\n",
    "    (\"Condition Index\", \"Eigenvalue-based detection\", \"CI > 30 indicates severe multicollinearity\"),\n",
    "    (\"Tolerance\", \"1 - R² of regressing Xi on other Xs\", \"Tolerance < 0.1 indicates problem\")\n",
    "]\n",
    "\n",
    "for method, description, threshold in detection_methods:\n",
    "    print(f\"   📊 **{method}**: {description}\")\n",
    "    print(f\"      🎯 Threshold: {threshold}\")\n",
    "\n",
    "print(\"\\\\n🛠️ **Solutions for Multicollinearity:**\")\n",
    "print(\"-\" * 35)\n",
    "solutions = [\n",
    "    (\"Remove Variables\", \"Drop one of highly correlated variables\", \"Simple but loses information\"),\n",
    "    (\"Principal Component Analysis\", \"Create uncorrelated linear combinations\", \"Preserves variance but loses interpretability\"),\n",
    "    (\"Ridge Regression\", \"Add penalty term to coefficients\", \"Handles multicollinearity automatically\"),\n",
    "    (\"Feature Selection\", \"Use statistical/algorithmic selection\", \"Keeps most informative features\"),\n",
    "    (\"Domain Knowledge\", \"Remove variables based on theory\", \"Most principled approach\"),\n",
    "    (\"Combine Variables\", \"Create indices or composite measures\", \"Reduces dimensionality meaningfully\")\n",
    "]\n",
    "\n",
    "for solution, description, consideration in solutions:\n",
    "    print(f\"   🔧 **{solution}**: {description}\")\n",
    "    print(f\"      💭 Consideration: {consideration}\")\n",
    "\n",
    "# 3. Feature Selection Techniques\n",
    "print(\"\\\\n\\\\n🎯 **3. FEATURE SELECTION TECHNIQUES**\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "print(\"\\\\n📋 **Why Feature Selection?**\")\n",
    "print(\"-\" * 30)\n",
    "reasons = [\n",
    "    \"🎯 **Improved Performance**: Remove noise and irrelevant features\",\n",
    "    \"⚡ **Faster Training**: Fewer features = faster computation\",\n",
    "    \"🧠 **Better Interpretability**: Focus on most important variables\",\n",
    "    \"📊 **Reduced Overfitting**: Simpler models generalize better\",\n",
    "    \"💾 **Lower Storage**: Less memory and storage requirements\",\n",
    "    \"🔍 **Easier Debugging**: Simpler models are easier to understand\"\n",
    "]\n",
    "\n",
    "for reason in reasons:\n",
    "    print(f\"   {reason}\")\n",
    "\n",
    "print(\"\\\\n🔄 **Feature Selection Categories:**\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "categories = {\n",
    "    \"Filter Methods\": {\n",
    "        \"description\": \"Statistical measures independent of ML algorithm\",\n",
    "        \"examples\": [\"Correlation\", \"Chi-square test\", \"Mutual information\", \"ANOVA F-test\"],\n",
    "        \"pros\": [\"Fast\", \"Model-agnostic\", \"Good for preprocessing\"],\n",
    "        \"cons\": [\"Ignores feature interactions\", \"May remove useful combinations\"]\n",
    "    },\n",
    "    \"Wrapper Methods\": {\n",
    "        \"description\": \"Use ML algorithm performance to select features\",\n",
    "        \"examples\": [\"Forward selection\", \"Backward elimination\", \"Recursive feature elimination\"],\n",
    "        \"pros\": [\"Considers feature interactions\", \"Optimizes for specific algorithm\"],\n",
    "        \"cons\": [\"Computationally expensive\", \"Risk of overfitting\"]\n",
    "    },\n",
    "    \"Embedded Methods\": {\n",
    "        \"description\": \"Feature selection built into algorithm training\",\n",
    "        \"examples\": [\"LASSO regression\", \"Random Forest importance\", \"ElasticNet\"],\n",
    "        \"pros\": [\"Efficient\", \"Considers interactions\", \"Regularization built-in\"],\n",
    "        \"cons\": [\"Algorithm-specific\", \"May not find global optimum\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, details in categories.items():\n",
    "    print(f\"\\\\n📊 **{category}**\")\n",
    "    print(f\"   🎯 **Approach**: {details['description']}\")\n",
    "    print(f\"   🔧 **Examples**: {', '.join(details['examples'])}\")\n",
    "    print(f\"   ✅ **Advantages**: {', '.join(details['pros'])}\")\n",
    "    print(f\"   ❌ **Limitations**: {', '.join(details['cons'])}\")\n",
    "\n",
    "print(\"\\\\n🎯 **Practical Feature Selection Strategy:**\")\n",
    "print(\"-\" * 40)\n",
    "strategy_steps = [\n",
    "    \"1. **Start with Domain Knowledge**: Use expert understanding of the problem\",\n",
    "    \"2. **Exploratory Data Analysis**: Understand distributions and correlations\",\n",
    "    \"3. **Remove Obvious Redundancy**: Drop duplicate or highly correlated features\",\n",
    "    \"4. **Apply Filter Methods**: Quick screening with statistical measures\",\n",
    "    \"5. **Use Embedded Methods**: LASSO or Ridge for automatic selection\",\n",
    "    \"6. **Validate with Cross-validation**: Ensure robust feature selection\",\n",
    "    \"7. **Iterate and Refine**: Continuously improve based on results\"\n",
    "]\n",
    "\n",
    "for step in strategy_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "# 4. Model Interpretation and Best Practices\n",
    "print(\"\\\\n\\\\n📊 **4. MODEL INTERPRETATION & BEST PRACTICES**\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\\\n🔍 **Interpreting Linear Regression Coefficients:**\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "interpretation_guide = [\n",
    "    (\"Coefficient Magnitude\", \"Size indicates strength of relationship\", \"Larger |β| = stronger effect\"),\n",
    "    (\"Coefficient Sign\", \"Direction of relationship\", \"Positive = increases target, Negative = decreases\"),\n",
    "    (\"Statistical Significance\", \"p-value < 0.05 (typically)\", \"Low p-value = reliable relationship\"),\n",
    "    (\"Confidence Intervals\", \"Range of plausible coefficient values\", \"Narrower CI = more precise estimate\"),\n",
    "    (\"Standardized Coefficients\", \"Coefficients when features are standardized\", \"Allows comparison across features\"),\n",
    "    (\"Practical Significance\", \"Real-world importance vs statistical\", \"Large effect size matters more than p-value\")\n",
    "]\n",
    "\n",
    "for concept, description, interpretation in interpretation_guide:\n",
    "    print(f\"   📊 **{concept}**: {description}\")\n",
    "    print(f\"      💡 {interpretation}\")\n",
    "\n",
    "print(\"\\\\n⚠️ **Common Interpretation Pitfalls:**\")\n",
    "print(\"-\" * 35)\n",
    "pitfalls = [\n",
    "    \"🚫 **Correlation ≠ Causation**: Relationships don't imply cause-effect\",\n",
    "    \"🚫 **Extrapolation Danger**: Predictions outside training range unreliable\",\n",
    "    \"🚫 **Missing Variable Bias**: Omitted variables can bias coefficients\",\n",
    "    \"🚫 **Interaction Neglect**: Relationships may depend on other variables\",\n",
    "    \"🚫 **Scale Sensitivity**: Raw coefficients depend on feature scales\",\n",
    "    \"🚫 **Non-linear Relationships**: Linear model may miss curved relationships\"\n",
    "]\n",
    "\n",
    "for pitfall in pitfalls:\n",
    "    print(f\"   {pitfall}\")\n",
    "\n",
    "print(\"\\\\n✅ **Best Practices for Linear Regression:**\")\n",
    "print(\"-\" * 40)\n",
    "best_practices = [\n",
    "    \"🔍 **Explore Data Thoroughly**: Understand distributions, outliers, missing values\",\n",
    "    \"📊 **Check Assumptions**: Validate linearity, independence, homoscedasticity, normality\",\n",
    "    \"⚖️ **Handle Multicollinearity**: Check VIF, use regularization if needed\",\n",
    "    \"🎯 **Feature Engineering**: Create meaningful features, transformations\",\n",
    "    \"✂️ **Train-Test Split**: Always validate on unseen data\",\n",
    "    \"📈 **Cross-Validation**: Use k-fold CV for robust performance estimates\",\n",
    "    \"🔧 **Regularization**: Consider Ridge/LASSO for high-dimensional data\",\n",
    "    \"📊 **Residual Analysis**: Examine residuals to validate assumptions\",\n",
    "    \"🎯 **Domain Knowledge**: Incorporate subject matter expertise\",\n",
    "    \"📋 **Document Everything**: Keep track of decisions and transformations\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"   {practice}\")\n",
    "\n",
    "print(\"\\\\n🎓 **When to Use Linear Regression:**\")\n",
    "print(\"-\" * 35)\n",
    "use_cases = [\n",
    "    (\"✅ **Good Fit**\", [\n",
    "        \"Linear relationships between features and target\",\n",
    "        \"Interpretability is crucial\",\n",
    "        \"Baseline model for comparison\",\n",
    "        \"Small to medium datasets\",\n",
    "        \"Well-understood domain with clear relationships\"\n",
    "    ]),\n",
    "    (\"❌ **Consider Alternatives**\", [\n",
    "        \"Highly non-linear relationships\",\n",
    "        \"Very large feature space (high dimensionality)\",\n",
    "        \"Complex feature interactions\",\n",
    "        \"Time series with trends/seasonality\",\n",
    "        \"Image or text data without proper preprocessing\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "for scenario, conditions in use_cases:\n",
    "    print(f\"\\\\n{scenario}:\")\n",
    "    for condition in conditions:\n",
    "        print(f\"   • {condition}\")\n",
    "\n",
    "print(\"\\\\n\\\\n🎯 **Summary: Key Takeaways**\")\n",
    "print(\"=\"*35)\n",
    "takeaways = [\n",
    "    \"📊 **Linear regression is powerful but has specific assumptions**\",\n",
    "    \"🔍 **Always validate assumptions before interpreting results**\",\n",
    "    \"⚖️ **Multicollinearity can severely impact coefficient stability**\",\n",
    "    \"🎯 **Feature selection improves model performance and interpretability**\",\n",
    "    \"💡 **Coefficients tell you relationships, not causation**\",\n",
    "    \"🛠️ **Regularization helps with overfitting and multicollinearity**\",\n",
    "    \"📈 **Cross-validation provides reliable performance estimates**\",\n",
    "    \"🧠 **Domain knowledge is crucial for proper model interpretation**\"\n",
    "]\n",
    "\n",
    "for takeaway in takeaways:\n",
    "    print(f\"   {takeaway}\")\n",
    "\n",
    "print(\"\\\\n✅ Conceptual discussion completed!\")\n",
    "print(\"🎓 Comprehensive linear regression theory covered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1479d1a",
   "metadata": {},
   "source": [
    "# 🎯 Assignment Conclusion\n",
    "\n",
    "## 📊 Complete Assignment Summary\n",
    "\n",
    "This comprehensive assignment has covered all aspects of **Encoding & Linear Regression** as requested:\n",
    "\n",
    "### ✅ **Part A: Encoding Categorical Variables**\n",
    "- Comprehensive analysis of categorical vs numerical variables\n",
    "- Detailed comparison of Label Encoding vs One-Hot Encoding\n",
    "- Practical implementation with real estate dataset\n",
    "- Best practices and use case recommendations\n",
    "\n",
    "### ✅ **Part B: Simple Linear Regression** \n",
    "- Implementation of univariate regression using GrLivArea\n",
    "- Train-test split methodology\n",
    "- Model training and prediction generation\n",
    "- Visualization of regression line and data points\n",
    "\n",
    "### ✅ **Part C: Regression Evaluation Metrics**\n",
    "- Complete evaluation framework: MAE, MSE, RMSE, R²\n",
    "- Detailed interpretation of each metric\n",
    "- Performance assessment and improvement recommendations\n",
    "- Practical significance analysis\n",
    "\n",
    "### ✅ **Part D: Multiple Linear Regression**\n",
    "- Implementation using 3 features: GrLivArea, OverallQual, YearBuilt\n",
    "- Comprehensive model comparison (Simple vs Multiple)\n",
    "- Feature importance analysis\n",
    "- Performance improvement quantification\n",
    "\n",
    "### ✅ **Part E: Conceptual Discussion**\n",
    "- Linear regression assumptions and validation methods\n",
    "- Multicollinearity detection and solutions\n",
    "- Feature selection techniques and strategies\n",
    "- Model interpretation best practices\n",
    "\n",
    "## 🎓 **Key Learning Outcomes**\n",
    "\n",
    "1. **Categorical Encoding Mastery**: Understanding when and how to apply different encoding techniques\n",
    "2. **Regression Implementation**: Hands-on experience with both simple and multiple linear regression\n",
    "3. **Model Evaluation**: Comprehensive understanding of regression metrics and their interpretation\n",
    "4. **Theoretical Foundation**: Deep knowledge of linear regression assumptions and best practices\n",
    "5. **Practical Application**: Real-world dataset analysis with housing price prediction\n",
    "\n",
    "## 🚀 **Technical Skills Demonstrated**\n",
    "\n",
    "- Data preprocessing and categorical variable handling\n",
    "- Machine learning model implementation using scikit-learn\n",
    "- Statistical analysis and hypothesis testing\n",
    "- Data visualization and interpretation\n",
    "- Model comparison and performance assessment\n",
    "- Feature engineering and selection techniques\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 Assignment completed successfully with comprehensive coverage of all required topics!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
