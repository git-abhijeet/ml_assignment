{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b9a658",
   "metadata": {},
   "source": [
    "# Q4: Random Neural Network Simulation with Multiple Activation Functions\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Build a random feedforward neural network and analyze how different activation functions impact the final output.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "**Randomly generates a neural network structure:**\n",
    "- Number of input features n ∈ [3, 6]\n",
    "- Number of hidden layers L ∈ [1, 3]\n",
    "- Number of neurons in each hidden layer ∈ [2, 5]\n",
    "\n",
    "**Randomly generates:**\n",
    "- Input feature values ∈ [-10, 10]\n",
    "- Weight matrices and bias vectors for all layers ∈ [-1, 1]\n",
    "\n",
    "**Implements activation functions:**\n",
    "- **Sigmoid**: σ(x) = 1 / (1 + exp(-x))\n",
    "- **Tanh**: tanh(x)\n",
    "- **ReLU**: max(0, x)\n",
    "- **Leaky ReLU**: x if x > 0 else 0.01*x\n",
    "\n",
    "**Analysis:**\n",
    "- Performs full forward pass for each activation function\n",
    "- Compares and plots final output values\n",
    "- Uses numpy and matplotlib only\n",
    "\n",
    "## Expected Output Format:\n",
    "\n",
    "```\n",
    "Random Seed: 42\n",
    "Generated Network:\n",
    "- Input Features: 4 - Values: [-3.2, 5.1, 0.9, -1.7]\n",
    "- Hidden Layers: 2\n",
    "  Layer 1: 3 neurons\n",
    "  Layer 2: 2 neurons\n",
    "- Output Layer: 1 neuron\n",
    "\n",
    "Final Outputs:\n",
    "- Sigmoid: [0.815]\n",
    "- Tanh: [0.633]\n",
    "- ReLU: [1.88]\n",
    "- Leaky ReLU: [1.65]\n",
    "[Output Plot Displayed]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec6309",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import numpy for numerical computations and matplotlib for plotting. These are the only allowed libraries for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add35a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"- numpy: for numerical computations and array operations\")\n",
    "print(\"- matplotlib: for plotting and visualization\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"\\nRandom seed set to: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85992e1f",
   "metadata": {},
   "source": [
    "## 2. Define Activation Functions\n",
    "\n",
    "Implement the four required activation functions that will be compared in the neural network simulation.\n",
    "\n",
    "- **Sigmoid**: σ(x) = 1 / (1 + exp(-x))\n",
    "- **Tanh**: tanh(x) \n",
    "- **ReLU**: max(0, x)\n",
    "- **Leaky ReLU**: x if x > 0 else 0.01*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    σ(x) = 1 / (1 + exp(-x))\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function.\n",
    "    tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit activation function.\n",
    "    ReLU(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU activation function.\n",
    "    Leaky ReLU(x) = x if x > 0 else alpha * x\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Test activation functions with sample values\n",
    "test_values = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "print(\"Testing activation functions with values:\", test_values)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Sigmoid:    {sigmoid(test_values)}\")\n",
    "print(f\"Tanh:       {tanh(test_values)}\")\n",
    "print(f\"ReLU:       {relu(test_values)}\")\n",
    "print(f\"Leaky ReLU: {leaky_relu(test_values)}\")\n",
    "\n",
    "# Create activation function dictionary for easy access\n",
    "activation_functions = {\n",
    "    'Sigmoid': sigmoid,\n",
    "    'Tanh': tanh,\n",
    "    'ReLU': relu,\n",
    "    'Leaky ReLU': leaky_relu\n",
    "}\n",
    "\n",
    "print(f\"\\nActivation functions dictionary created with {len(activation_functions)} functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a715d8f",
   "metadata": {},
   "source": [
    "## 3. Random Network Generation Functions\n",
    "\n",
    "Create functions to randomly generate neural network structure and parameters according to the specified constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac817c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_network_structure():\n",
    "    \"\"\"\n",
    "    Generate random neural network structure according to constraints:\n",
    "    - Input features n ∈ [3, 6]\n",
    "    - Hidden layers L ∈ [1, 3]\n",
    "    - Neurons per hidden layer ∈ [2, 5]\n",
    "    - Output layer: 1 neuron\n",
    "    \n",
    "    Returns:\n",
    "        dict: Network structure information\n",
    "    \"\"\"\n",
    "    # Generate random structure parameters\n",
    "    n_inputs = np.random.randint(3, 7)  # [3, 6]\n",
    "    n_hidden_layers = np.random.randint(1, 4)  # [1, 3]\n",
    "    \n",
    "    # Generate neurons per hidden layer\n",
    "    hidden_layer_sizes = []\n",
    "    for _ in range(n_hidden_layers):\n",
    "        neurons = np.random.randint(2, 6)  # [2, 5]\n",
    "        hidden_layer_sizes.append(neurons)\n",
    "    \n",
    "    # Output layer always has 1 neuron\n",
    "    n_outputs = 1\n",
    "    \n",
    "    network_structure = {\n",
    "        'n_inputs': n_inputs,\n",
    "        'n_hidden_layers': n_hidden_layers,\n",
    "        'hidden_layer_sizes': hidden_layer_sizes,\n",
    "        'n_outputs': n_outputs\n",
    "    }\n",
    "    \n",
    "    return network_structure\n",
    "\n",
    "def generate_random_inputs(n_inputs):\n",
    "    \"\"\"\n",
    "    Generate random input values ∈ [-10, 10]\n",
    "    \n",
    "    Args:\n",
    "        n_inputs (int): Number of input features\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Random input vector\n",
    "    \"\"\"\n",
    "    return np.random.uniform(-10, 10, n_inputs)\n",
    "\n",
    "def generate_random_weights_and_biases(network_structure):\n",
    "    \"\"\"\n",
    "    Generate random weights and biases for the entire network.\n",
    "    Weights and biases ∈ [-1, 1]\n",
    "    \n",
    "    Args:\n",
    "        network_structure (dict): Network structure information\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (weights_list, biases_list)\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    # Create layer sizes list (input + hidden + output)\n",
    "    layer_sizes = [network_structure['n_inputs']] + network_structure['hidden_layer_sizes'] + [network_structure['n_outputs']]\n",
    "    \n",
    "    # Generate weights and biases for each layer\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        input_size = layer_sizes[i]\n",
    "        output_size = layer_sizes[i + 1]\n",
    "        \n",
    "        # Generate random weights matrix\n",
    "        weight_matrix = np.random.uniform(-1, 1, (output_size, input_size))\n",
    "        weights.append(weight_matrix)\n",
    "        \n",
    "        # Generate random bias vector\n",
    "        bias_vector = np.random.uniform(-1, 1, output_size)\n",
    "        biases.append(bias_vector)\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "# Test the network generation functions\n",
    "print(\"Testing random network generation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Generate a sample network\n",
    "sample_structure = generate_random_network_structure()\n",
    "print(\"Sample Network Structure:\")\n",
    "print(f\"  Input features: {sample_structure['n_inputs']}\")\n",
    "print(f\"  Hidden layers: {sample_structure['n_hidden_layers']}\")\n",
    "print(f\"  Hidden layer sizes: {sample_structure['hidden_layer_sizes']}\")\n",
    "print(f\"  Output neurons: {sample_structure['n_outputs']}\")\n",
    "\n",
    "# Generate sample inputs\n",
    "sample_inputs = generate_random_inputs(sample_structure['n_inputs'])\n",
    "print(f\"\\nSample inputs: {sample_inputs}\")\n",
    "\n",
    "# Generate sample weights and biases\n",
    "sample_weights, sample_biases = generate_random_weights_and_biases(sample_structure)\n",
    "print(f\"\\nNumber of weight matrices: {len(sample_weights)}\")\n",
    "print(f\"Number of bias vectors: {len(sample_biases)}\")\n",
    "print(f\"Weight matrix shapes: {[w.shape for w in sample_weights]}\")\n",
    "print(f\"Bias vector shapes: {[b.shape for b in sample_biases]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f22b18",
   "metadata": {},
   "source": [
    "## 4. Forward Propagation Functions\n",
    "\n",
    "Implement forward propagation through the neural network using the formula:\n",
    "- z = np.dot(weights, input_vector) + bias\n",
    "- a = activation_function(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs, weights, biases, activation_func):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the neural network.\n",
    "    \n",
    "    Args:\n",
    "        inputs (np.array): Input vector\n",
    "        weights (list): List of weight matrices for each layer\n",
    "        biases (list): List of bias vectors for each layer\n",
    "        activation_func (function): Activation function to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (final_output, layer_outputs) where layer_outputs contains outputs of all layers\n",
    "    \"\"\"\n",
    "    current_input = inputs\n",
    "    layer_outputs = []\n",
    "    \n",
    "    # Forward pass through each layer\n",
    "    for i, (weight_matrix, bias_vector) in enumerate(zip(weights, biases)):\n",
    "        # Compute linear transformation: z = W * x + b\n",
    "        z = np.dot(weight_matrix, current_input) + bias_vector\n",
    "        \n",
    "        # Apply activation function: a = f(z)\n",
    "        a = activation_func(z)\n",
    "        \n",
    "        # Store layer output\n",
    "        layer_outputs.append({\n",
    "            'layer': i + 1,\n",
    "            'z': z,\n",
    "            'a': a\n",
    "        })\n",
    "        \n",
    "        # Output of current layer becomes input to next layer\n",
    "        current_input = a\n",
    "    \n",
    "    # Final output is the output of the last layer\n",
    "    final_output = current_input\n",
    "    \n",
    "    return final_output, layer_outputs\n",
    "\n",
    "def simulate_network_with_all_activations(inputs, weights, biases, activation_functions):\n",
    "    \"\"\"\n",
    "    Run the same network with different activation functions and compare results.\n",
    "    \n",
    "    Args:\n",
    "        inputs (np.array): Input vector\n",
    "        weights (list): List of weight matrices\n",
    "        biases (list): List of bias vectors\n",
    "        activation_functions (dict): Dictionary of activation functions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results for each activation function\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, func in activation_functions.items():\n",
    "        final_output, layer_outputs = forward_propagation(inputs, weights, biases, func)\n",
    "        results[name] = {\n",
    "            'final_output': final_output,\n",
    "            'layer_outputs': layer_outputs\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test forward propagation with sample network\n",
    "print(\"Testing forward propagation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Use the previously generated sample network\n",
    "test_inputs = generate_random_inputs(3)  # Simple 3-input test\n",
    "test_weights = [\n",
    "    np.array([[0.5, -0.3, 0.2], [0.1, 0.4, -0.6]]),  # 3 -> 2\n",
    "    np.array([[0.7, -0.2]])  # 2 -> 1\n",
    "]\n",
    "test_biases = [\n",
    "    np.array([0.1, -0.3]),  # Hidden layer bias\n",
    "    np.array([0.5])  # Output layer bias\n",
    "]\n",
    "\n",
    "print(f\"Test inputs: {test_inputs}\")\n",
    "print(f\"Network structure: 3 -> 2 -> 1\")\n",
    "\n",
    "# Test with sigmoid activation\n",
    "final_output, layer_outputs = forward_propagation(test_inputs, test_weights, test_biases, sigmoid)\n",
    "print(f\"\\nSigmoid activation result: {final_output}\")\n",
    "\n",
    "# Test with all activations\n",
    "all_results = simulate_network_with_all_activations(test_inputs, test_weights, test_biases, activation_functions)\n",
    "print(f\"\\nResults with all activation functions:\")\n",
    "for name, result in all_results.items():\n",
    "    print(f\"  {name}: {result['final_output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615615e8",
   "metadata": {},
   "source": [
    "## 5. Visualization and Display Functions\n",
    "\n",
    "Create functions to display network information and plot comparison results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_network_structure(structure, inputs, weights, biases):\n",
    "    \"\"\"\n",
    "    Display the generated network structure and parameters.\n",
    "    \"\"\"\n",
    "    print(f\"Random Seed: {RANDOM_SEED}\")\n",
    "    print(\"\\nGenerated Network:\")\n",
    "    print(f\"- Input Features: {structure['n_inputs']} - Values: {inputs.round(1).tolist()}\")\n",
    "    print(f\"- Hidden Layers: {structure['n_hidden_layers']}\")\n",
    "    \n",
    "    for i, size in enumerate(structure['hidden_layer_sizes']):\n",
    "        print(f\"  Layer {i+1}: {size} neurons\")\n",
    "    \n",
    "    print(f\"- Output Layer: {structure['n_outputs']} neuron\")\n",
    "\n",
    "def plot_activation_comparison(results):\n",
    "    \"\"\"\n",
    "    Create a bar chart comparing final outputs for different activation functions.\n",
    "    \"\"\"\n",
    "    activation_names = list(results.keys())\n",
    "    final_outputs = [results[name]['final_output'][0] for name in activation_names]\n",
    "    \n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(activation_names, final_outputs, \n",
    "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'], \n",
    "                   alpha=0.8)\n",
    "    \n",
    "    plt.title('Neural Network Output Comparison\\nDifferent Activation Functions', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Activation Function', fontsize=12)\n",
    "    plt.ylabel('Final Output Value', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, final_outputs):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_final_outputs(results):\n",
    "    \"\"\"\n",
    "    Display final outputs in the required format.\n",
    "    \"\"\"\n",
    "    print(\"\\nFinal Outputs:\")\n",
    "    for name, result in results.items():\n",
    "        output_value = result['final_output'][0]\n",
    "        print(f\"- {name}: [{output_value:.3f}]\")\n",
    "\n",
    "print(\"Visualization functions created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352f094",
   "metadata": {},
   "source": [
    "## 6. Complete Random Neural Network Simulation\n",
    "\n",
    "Combine all components to create the complete simulation that generates a random network and compares activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76276bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_simulation(seed=None):\n",
    "    \"\"\"\n",
    "    Run the complete random neural network simulation.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Random seed for reproducibility (optional)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Complete simulation results\n",
    "    \"\"\"\n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        global RANDOM_SEED\n",
    "        RANDOM_SEED = seed\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"RANDOM NEURAL NETWORK SIMULATION WITH MULTIPLE ACTIVATION FUNCTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Generate random network structure\n",
    "    network_structure = generate_random_network_structure()\n",
    "    \n",
    "    # Step 2: Generate random inputs\n",
    "    inputs = generate_random_inputs(network_structure['n_inputs'])\n",
    "    \n",
    "    # Step 3: Generate random weights and biases\n",
    "    weights, biases = generate_random_weights_and_biases(network_structure)\n",
    "    \n",
    "    # Step 4: Display network structure\n",
    "    display_network_structure(network_structure, inputs, weights, biases)\n",
    "    \n",
    "    # Step 5: Run simulation with all activation functions\n",
    "    results = simulate_network_with_all_activations(inputs, weights, biases, activation_functions)\n",
    "    \n",
    "    # Step 6: Display final outputs\n",
    "    display_final_outputs(results)\n",
    "    \n",
    "    # Step 7: Create comparison plot\n",
    "    print(\"\\n[Output Plot Displayed]\")\n",
    "    plot_activation_comparison(results)\n",
    "    \n",
    "    return {\n",
    "        'network_structure': network_structure,\n",
    "        'inputs': inputs,\n",
    "        'weights': weights,\n",
    "        'biases': biases,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "# The complete simulation function is ready!\n",
    "print(\"Complete simulation function created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299a969",
   "metadata": {},
   "source": [
    "## 7. Execute Main Simulation\n",
    "\n",
    "Run the complete random neural network simulation with the specified seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main simulation\n",
    "simulation_results = run_complete_simulation(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451a15f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a **Random Neural Network Simulation with Multiple Activation Functions** as requested.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "✅ **Random Network Generation**: Creates networks with random structure and parameters within specified constraints\n",
    "\n",
    "✅ **Four Activation Functions**: Sigmoid, Tanh, ReLU, and Leaky ReLU implementations\n",
    "\n",
    "✅ **Forward Propagation**: Complete forward pass through all network layers\n",
    "\n",
    "✅ **Comparison & Visualization**: Bar chart comparing outputs from different activation functions\n",
    "\n",
    "### Result:\n",
    "The simulation demonstrates how different activation functions produce varying outputs for the same randomly generated neural network, providing insights into activation function behavior."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
