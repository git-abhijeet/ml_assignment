{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9d3a32",
   "metadata": {},
   "source": [
    "# Q3: Configurable Neural Network Output\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Write a Python program where the user specifies:\n",
    "- Number of inputs (n)\n",
    "- Number of neurons in the hidden layer (h)\n",
    "- Whether to use ReLU or Sigmoid as activation\n",
    "\n",
    "Then:\n",
    "- Generate n random input values\n",
    "- Generate random weights and biases for each hidden neuron\n",
    "- Generate one output neuron with h weights and 1 bias\n",
    "- Perform full forward pass and print final output\n",
    "\n",
    "## Expected Input/Output Example\n",
    "\n",
    "**Input (from user):**\n",
    "- Enter number of inputs: 3\n",
    "- Enter number of hidden neurons: 2\n",
    "- Enter activation (sigmoid/relu): relu\n",
    "\n",
    "**Expected Output:**\n",
    "- Inputs: [0.76, -0.43, 0.55]\n",
    "- Hidden layer weights: [[-0.4, 0.2, 0.1], [0.3, 0.6, -0.7]]\n",
    "- Hidden biases: [0.1, -0.2]\n",
    "- Hidden outputs (ReLU): [0.03, 0.0]\n",
    "- Output layer weights: [0.6, -0.5]\n",
    "- Bias: 0.1\n",
    "- Final Output: 0.118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee89cb5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import the necessary libraries including random and math for generating random parameters and implementing activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c76501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"- random: for generating random values\")\n",
    "print(\"- math: for mathematical operations including exponential function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb90b2f",
   "metadata": {},
   "source": [
    "## 2. Define Activation Functions\n",
    "\n",
    "Implement both ReLU and Sigmoid activation functions to give users flexibility in choosing the activation function.\n",
    "\n",
    "**ReLU (Rectified Linear Unit):** `ReLU(z) = max(0, z)`  \n",
    "**Sigmoid:** `sigmoid(z) = 1 / (1 + e^(-z))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) activation function.\n",
    "    \n",
    "    Args:\n",
    "        z (float): Input value\n",
    "        \n",
    "    Returns:\n",
    "        float: max(0, z)\n",
    "    \"\"\"\n",
    "    return max(0, z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    Args:\n",
    "        z (float): Input value\n",
    "        \n",
    "    Returns:\n",
    "        float: 1 / (1 + e^(-z))\n",
    "    \"\"\"\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def apply_activation(z, activation_type):\n",
    "    \"\"\"\n",
    "    Apply the specified activation function.\n",
    "    \n",
    "    Args:\n",
    "        z (float): Input value\n",
    "        activation_type (str): 'relu' or 'sigmoid'\n",
    "        \n",
    "    Returns:\n",
    "        float: Activated output\n",
    "    \"\"\"\n",
    "    if activation_type.lower() == 'relu':\n",
    "        return relu(z)\n",
    "    elif activation_type.lower() == 'sigmoid':\n",
    "        return sigmoid(z)\n",
    "    else:\n",
    "        raise ValueError(\"Activation must be 'relu' or 'sigmoid'\")\n",
    "\n",
    "# Test activation functions\n",
    "print(\"Testing activation functions:\")\n",
    "test_values = [-2, -1, 0, 1, 2]\n",
    "\n",
    "print(\"\\nReLU activation:\")\n",
    "for val in test_values:\n",
    "    result = relu(val)\n",
    "    print(f\"ReLU({val:2}) = {result:.3f}\")\n",
    "\n",
    "print(\"\\nSigmoid activation:\")\n",
    "for val in test_values:\n",
    "    result = sigmoid(val)\n",
    "    print(f\"Sigmoid({val:2}) = {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68202b",
   "metadata": {},
   "source": [
    "## 3. Random Parameter Generation Functions\n",
    "\n",
    "Create functions to generate random inputs, weights, and biases for the configurable neural network based on user specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30570943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_inputs(n):\n",
    "    \"\"\"Generate n random input values between -1 and 1.\"\"\"\n",
    "    return [random.uniform(-1, 1) for _ in range(n)]\n",
    "\n",
    "def generate_random_weights(rows, cols):\n",
    "    \"\"\"Generate random weight matrix between -1 and 1.\"\"\"\n",
    "    return [[random.uniform(-1, 1) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def generate_random_biases(size):\n",
    "    \"\"\"Generate random bias values between -1 and 1.\"\"\"\n",
    "    return [random.uniform(-1, 1) for _ in range(size)]\n",
    "\n",
    "# Test the random generation functions\n",
    "print(\"Testing random parameter generation functions:\")\n",
    "\n",
    "# Test with sample sizes\n",
    "n_inputs = 3\n",
    "n_hidden = 2\n",
    "\n",
    "sample_inputs = generate_random_inputs(n_inputs)\n",
    "sample_weights = generate_random_weights(n_hidden, n_inputs)\n",
    "sample_biases = generate_random_biases(n_hidden)\n",
    "\n",
    "print(f\"\\nSample with {n_inputs} inputs and {n_hidden} hidden neurons:\")\n",
    "print(f\"Random inputs ({n_inputs}): {[round(x, 3) for x in sample_inputs]}\")\n",
    "print(f\"Random weights ({n_hidden}x{n_inputs}): {[[round(w, 3) for w in row] for row in sample_weights]}\")\n",
    "print(f\"Random biases ({n_hidden}): {[round(b, 3) for b in sample_biases]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138719",
   "metadata": {},
   "source": [
    "## 4. Neural Network Layer Functions\n",
    "\n",
    "Implement forward pass functions for both hidden and output layers with configurable activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5273fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_hidden_layer(inputs, weights, biases, activation_type):\n",
    "    \"\"\"\n",
    "    Perform forward pass through hidden layer with configurable activation.\n",
    "    \n",
    "    Args:\n",
    "        inputs (list): Input values\n",
    "        weights (list): Weight matrix for hidden layer\n",
    "        biases (list): Bias values for hidden layer\n",
    "        activation_type (str): 'relu' or 'sigmoid'\n",
    "        \n",
    "    Returns:\n",
    "        list: Output values from hidden layer after activation\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for i, neuron_weights in enumerate(weights):\n",
    "        # Calculate weighted sum for each neuron\n",
    "        z = sum(x * w for x, w in zip(inputs, neuron_weights)) + biases[i]\n",
    "        # Apply specified activation function\n",
    "        output = apply_activation(z, activation_type)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def forward_pass_output_layer(hidden_outputs, weights, bias, activation_type='sigmoid'):\n",
    "    \"\"\"\n",
    "    Perform forward pass through output layer.\n",
    "    \n",
    "    Args:\n",
    "        hidden_outputs (list): Output values from hidden layer\n",
    "        weights (list): Weight values for output layer\n",
    "        bias (float): Bias value for output layer\n",
    "        activation_type (str): Activation function for output (default: sigmoid)\n",
    "        \n",
    "    Returns:\n",
    "        float: Final output after activation\n",
    "    \"\"\"\n",
    "    # Calculate weighted sum\n",
    "    z = sum(h * w for h, w in zip(hidden_outputs, weights)) + bias\n",
    "    # Apply activation function (usually sigmoid for output)\n",
    "    return apply_activation(z, activation_type)\n",
    "\n",
    "# Test the layer functions\n",
    "print(\"Testing neural network layer functions:\")\n",
    "\n",
    "# Sample test data\n",
    "test_inputs = [0.5, -0.3, 0.8]\n",
    "test_hidden_weights = [[0.2, -0.4, 0.1], [-0.3, 0.6, 0.2]]\n",
    "test_hidden_biases = [0.1, -0.2]\n",
    "\n",
    "print(f\"\\nTest inputs: {test_inputs}\")\n",
    "print(f\"Hidden weights: {test_hidden_weights}\")\n",
    "print(f\"Hidden biases: {test_hidden_biases}\")\n",
    "\n",
    "# Test with ReLU\n",
    "relu_outputs = forward_pass_hidden_layer(test_inputs, test_hidden_weights, test_hidden_biases, 'relu')\n",
    "print(f\"\\nHidden outputs (ReLU): {[round(x, 3) for x in relu_outputs]}\")\n",
    "\n",
    "# Test with Sigmoid\n",
    "sigmoid_outputs = forward_pass_hidden_layer(test_inputs, test_hidden_weights, test_hidden_biases, 'sigmoid')\n",
    "print(f\"Hidden outputs (Sigmoid): {[round(x, 3) for x in sigmoid_outputs]}\")\n",
    "\n",
    "# Test output layer\n",
    "test_output_weights = [0.5, -0.3]\n",
    "test_output_bias = 0.2\n",
    "final_output = forward_pass_output_layer(relu_outputs, test_output_weights, test_output_bias)\n",
    "print(f\"\\nFinal output: {round(final_output, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd71ba",
   "metadata": {},
   "source": [
    "## 5. User Input Functions\n",
    "\n",
    "Create functions to get user specifications for the neural network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_configuration():\n",
    "    \"\"\"\n",
    "    Get neural network configuration from user input.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (n_inputs, n_hidden, activation_type)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Configurable Neural Network Setup\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Get number of inputs\n",
    "        n_inputs = int(input(\"Enter number of inputs: \"))\n",
    "        if n_inputs <= 0:\n",
    "            raise ValueError(\"Number of inputs must be positive\")\n",
    "        \n",
    "        # Get number of hidden neurons\n",
    "        n_hidden = int(input(\"Enter number of hidden neurons: \"))\n",
    "        if n_hidden <= 0:\n",
    "            raise ValueError(\"Number of hidden neurons must be positive\")\n",
    "        \n",
    "        # Get activation type\n",
    "        activation_type = input(\"Enter activation (sigmoid/relu): \").strip().lower()\n",
    "        if activation_type not in ['sigmoid', 'relu']:\n",
    "            raise ValueError(\"Activation must be 'sigmoid' or 'relu'\")\n",
    "        \n",
    "        return n_inputs, n_hidden, activation_type\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_manual_configuration(n_inputs=3, n_hidden=2, activation_type='relu'):\n",
    "    \"\"\"\n",
    "    Get neural network configuration manually (for testing/demo purposes).\n",
    "    \n",
    "    Args:\n",
    "        n_inputs (int): Number of inputs (default: 3)\n",
    "        n_hidden (int): Number of hidden neurons (default: 2)\n",
    "        activation_type (str): Activation function (default: 'relu')\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (n_inputs, n_hidden, activation_type)\n",
    "    \"\"\"\n",
    "    print(\"Manual Configuration (for demo):\")\n",
    "    print(f\"Number of inputs: {n_inputs}\")\n",
    "    print(f\"Number of hidden neurons: {n_hidden}\")\n",
    "    print(f\"Activation function: {activation_type}\")\n",
    "    \n",
    "    return n_inputs, n_hidden, activation_type\n",
    "\n",
    "# Test manual configuration\n",
    "print(\"Testing manual configuration function:\")\n",
    "test_config = get_manual_configuration()\n",
    "print(f\"Configuration returned: {test_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ceedb",
   "metadata": {},
   "source": [
    "## 6. Complete Configurable Neural Network\n",
    "\n",
    "Combine all components to create a complete configurable neural network that adapts to user specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurable_neural_network(n_inputs, n_hidden, activation_type):\n",
    "    \"\"\"\n",
    "    Create and run a configurable neural network based on user specifications.\n",
    "    \n",
    "    Args:\n",
    "        n_inputs (int): Number of input values\n",
    "        n_hidden (int): Number of hidden neurons\n",
    "        activation_type (str): 'relu' or 'sigmoid'\n",
    "        \n",
    "    Returns:\n",
    "        float: Final output of the neural network\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONFIGURABLE NEURAL NETWORK\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Configuration: {n_inputs} inputs, {n_hidden} hidden neurons, {activation_type} activation\")\n",
    "    print()\n",
    "    \n",
    "    # Generate random inputs\n",
    "    inputs = generate_random_inputs(n_inputs)\n",
    "    print(f\"Inputs: {[round(x, 2) for x in inputs]}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate hidden layer parameters\n",
    "    hidden_weights = generate_random_weights(n_hidden, n_inputs)\n",
    "    hidden_biases = generate_random_biases(n_hidden)\n",
    "    \n",
    "    print(f\"Hidden layer weights: {[[round(w, 1) for w in neuron] for neuron in hidden_weights]}\")\n",
    "    print(f\"Hidden biases: {[round(b, 1) for b in hidden_biases]}\")\n",
    "    \n",
    "    # Forward pass through hidden layer\n",
    "    hidden_outputs = forward_pass_hidden_layer(inputs, hidden_weights, hidden_biases, activation_type)\n",
    "    activation_name = activation_type.upper()\n",
    "    print(f\"Hidden outputs ({activation_name}): {[round(h, 3) for h in hidden_outputs]}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate output layer parameters\n",
    "    output_weights = generate_random_weights(1, n_hidden)[0]  # Single output neuron\n",
    "    output_bias = generate_random_biases(1)[0]\n",
    "    \n",
    "    print(f\"Output layer weights: {[round(w, 1) for w in output_weights]}\")\n",
    "    print(f\"Bias: {round(output_bias, 1)}\")\n",
    "    \n",
    "    # Forward pass through output layer (using sigmoid for final output)\n",
    "    final_output = forward_pass_output_layer(hidden_outputs, output_weights, output_bias, 'sigmoid')\n",
    "    \n",
    "    print(f\"Final Output: {round(final_output, 3)}\")\n",
    "    print()\n",
    "    \n",
    "    # Show detailed calculations\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DETAILED CALCULATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Hidden layer calculations\n",
    "    print(f\"Hidden Layer ({activation_name} activation):\")\n",
    "    for i, (neuron_weights, bias) in enumerate(zip(hidden_weights, hidden_biases)):\n",
    "        z = sum(x * w for x, w in zip(inputs, neuron_weights)) + bias\n",
    "        output = apply_activation(z, activation_type)\n",
    "        \n",
    "        calculation_str = \" + \".join([f\"({x:.2f}×{w:.1f})\" for x, w in zip(inputs, neuron_weights)])\n",
    "        print(f\"  Neuron {i+1}: z = {calculation_str} + {bias:.1f} = {z:.3f}\")\n",
    "        print(f\"  Neuron {i+1}: {activation_name}({z:.3f}) = {output:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    # Output layer calculations\n",
    "    print(\"Output Layer (Sigmoid activation):\")\n",
    "    z_output = sum(h * w for h, w in zip(hidden_outputs, output_weights)) + output_bias\n",
    "    calculation_str = \" + \".join([f\"({h:.3f}×{w:.1f})\" for h, w in zip(hidden_outputs, output_weights)])\n",
    "    print(f\"  z = {calculation_str} + {output_bias:.1f} = {z_output:.3f}\")\n",
    "    print(f\"  final_output = sigmoid({z_output:.3f}) = {final_output:.3f}\")\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Function is ready to use!\n",
    "print(\"Configurable neural network function is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382701f4",
   "metadata": {},
   "source": [
    "## 7. Demonstration with Expected Configuration\n",
    "\n",
    "Run the neural network with the expected configuration from the problem statement (3 inputs, 2 hidden neurons, ReLU activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration with expected configuration from problem statement\n",
    "print(\"DEMONSTRATION WITH EXPECTED CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Expected configuration\n",
    "n_inputs = 3\n",
    "n_hidden = 2\n",
    "activation_type = 'relu'\n",
    "\n",
    "print(\"Expected Input:\")\n",
    "print(f\"Enter number of inputs: {n_inputs}\")\n",
    "print(f\"Enter number of hidden neurons: {n_hidden}\")\n",
    "print(f\"Enter activation (sigmoid/relu): {activation_type}\")\n",
    "print()\n",
    "\n",
    "# Run the configurable neural network\n",
    "result = configurable_neural_network(n_inputs, n_hidden, activation_type)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL RESULT: {result:.3f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26fa94",
   "metadata": {},
   "source": [
    "## 8. Additional Test Configurations\n",
    "\n",
    "Test the configurable neural network with different configurations to demonstrate its flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different configurations\n",
    "test_configurations = [\n",
    "    {\"name\": \"Config 1: Small network with Sigmoid\", \"inputs\": 2, \"hidden\": 1, \"activation\": \"sigmoid\"},\n",
    "    {\"name\": \"Config 2: Medium network with ReLU\", \"inputs\": 4, \"hidden\": 3, \"activation\": \"relu\"},\n",
    "    {\"name\": \"Config 3: Large network with Sigmoid\", \"inputs\": 5, \"hidden\": 4, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "print(\"TESTING DIFFERENT CONFIGURATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, config in enumerate(test_configurations, 1):\n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = configurable_neural_network(\n",
    "        config['inputs'], \n",
    "        config['hidden'], \n",
    "        config['activation']\n",
    "    )\n",
    "    \n",
    "    print(f\"Final result for {config['name']}: {result:.3f}\")\n",
    "    \n",
    "    if i < len(test_configurations):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"- ReLU activation can produce zero outputs for negative weighted sums\")\n",
    "print(\"- Sigmoid activation always produces outputs between 0 and 1\")\n",
    "print(\"- More hidden neurons provide more complex transformations\")\n",
    "print(\"- Network output depends on random initialization\")\n",
    "print(\"- The configuration flexibility allows for various network architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59e620",
   "metadata": {},
   "source": [
    "## 9. Interactive Configuration (Optional)\n",
    "\n",
    "Use this cell to run the interactive version where you can specify your own neural network configuration.\n",
    "\n",
    "**Note:** Uncomment and run the cell below if you want to input custom configuration interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b552be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below to run interactive configuration\n",
    "# print(\"INTERACTIVE CONFIGURATION\")\n",
    "# print(\"=\"*50)\n",
    "# n_inputs, n_hidden, activation_type = get_user_configuration()\n",
    "# if n_inputs is not None:\n",
    "#     result = configurable_neural_network(n_inputs, n_hidden, activation_type)\n",
    "#     print(f\"\\nYour custom neural network result: {result:.3f}\")\n",
    "\n",
    "# Alternative: Manual configuration for testing\n",
    "print(\"MANUAL CUSTOM CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(\"You can modify the values below and run this cell:\")\n",
    "\n",
    "# Modify these values as needed\n",
    "custom_inputs = 4\n",
    "custom_hidden = 3\n",
    "custom_activation = 'sigmoid'\n",
    "\n",
    "print(f\"Custom configuration: {custom_inputs} inputs, {custom_hidden} hidden neurons, {custom_activation} activation\")\n",
    "print()\n",
    "\n",
    "result = configurable_neural_network(custom_inputs, custom_hidden, custom_activation)\n",
    "\n",
    "print(f\"\\nCustom configuration result: {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847a4b2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully implements a **configurable neural network** that adapts to user specifications as requested in the assignment.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "✅ **User Configuration**: Accepts number of inputs, hidden neurons, and activation type  \n",
    "✅ **Flexible Architecture**: Adapts network structure based on user input  \n",
    "✅ **Multiple Activations**: Supports both ReLU and Sigmoid activation functions  \n",
    "✅ **Random Parameter Generation**: Creates random weights, biases, and inputs  \n",
    "✅ **Complete Forward Pass**: Implements full neural network computation  \n",
    "✅ **Detailed Output**: Shows all intermediate values and calculations  \n",
    "\n",
    "### Network Architecture:\n",
    "\n",
    "- **Input Layer**: n user-specified input values (randomly generated)\n",
    "- **Hidden Layer**: h user-specified neurons with configurable activation\n",
    "- **Output Layer**: 1 neuron with sigmoid activation (final output)\n",
    "\n",
    "### Activation Functions:\n",
    "\n",
    "1. **ReLU**: `f(z) = max(0, z)` - Can output zero for negative inputs\n",
    "2. **Sigmoid**: `f(z) = 1 / (1 + e^(-z))` - Always outputs between 0 and 1\n",
    "\n",
    "### Mathematical Process:\n",
    "\n",
    "1. **Hidden Layer**: `z = inputs·weights + bias` → `output = activation(z)`\n",
    "2. **Output Layer**: `z = hidden_outputs·weights + bias` → `final = sigmoid(z)`\n",
    "\n",
    "### Verification:\n",
    "\n",
    "- ✅ Expected configuration: 3 inputs, 2 hidden neurons, ReLU activation\n",
    "- ✅ Produces structured output matching problem statement format\n",
    "- ✅ Handles various network sizes and activation functions\n",
    "- ✅ Shows detailed step-by-step calculations\n",
    "\n",
    "The implementation demonstrates a flexible neural network framework that can be configured for different architectures and activation functions, making it suitable for various machine learning tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
